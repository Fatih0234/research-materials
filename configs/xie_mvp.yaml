# TrustBench MVP Config: Xie et al. (2024) Minimal Replication
# Implements one-shot Trust Game with baseline + llm_partner framing
# Target: GPT-5 Nano, Claude-3 Haiku, Llama-3.3-70b
# Personas: 20 (reduced from Xie's 53 for MVP cost control)

experiment_id: "xie_mvp_2026"

# OpenRouter API configuration
openrouter:
  api_key_env_var: "OPENROUTER_API_KEY"
  base_url: "https://openrouter.ai/api/v1"

models:
  - model_id: "openai/gpt-5-nano"
    label: "GPT-5 Nano"
    paper_equivalent: "gpt-4"

  - model_id: "anthropic/claude-3-haiku"
    label: "Claude 3 Haiku"
    paper_equivalent: null

  - model_id: "<PUT THE EXACT ID FROM /models LIST>"
    label: "Llama 70B Instruct"
    paper_equivalent: "llama2-70b"


prompts:
  template_dir: "specs/02_prompts/trust_game"
  trustor_template: "v001_trustor.md"
  trustee_template: "v001_trustee.md"

# Persona configuration
personas:
  source_path: "data/personas/xie_53.jsonl"  # Imported 53 personas from Xie et al. artifacts
  n_sample: 20  # Reduced from 53 for MVP (cost control)
  sampling_strategy: "sequential"  # Use first 20 personas
  seed: null  # No random sampling for reproducibility

# Trust Game parameters
game:
  variant: "trust_game"  # One-shot Trust Game (MVP)
  endowment: 10  # Trustor receives $10 (Xie et al. standard)
  multiplier: 3  # Amount sent is tripled (Xie et al. standard)
  rounds: 1  # One-shot game (repeated games deferred to extensions)
  roles:
    - "trustor"
    - "trustee"

# Treatments to test
treatments:
  partner_framing:
    - "baseline"  # No partner framing (Xie et al. baseline)
    - "llm_partner"  # Partner is LLM (Xie et al. treatment)
    # "human_partner" omitted for MVP (can be added later)

# Sampling parameters (aligned with Xie et al.)
sampling:
  temperature: 1.0  # High diversity for persona simulation (Xie et al. used 1.0)
  top_p: null  # Not specified in Xie et al., leave unset
  max_tokens: null  # Unlimited (models need flexibility for BDI reasoning)
  seed: null  # Not used in Xie et al.
  n_retries: 3  # Retry API failures
  retry_delay_seconds: 1.0  # Exponential backoff


# Output configuration
output:
  episodes_dir: "results/episodes"
  aggregates_dir: "results/aggregates"
  run_label: "xie_mvp"  # Human-readable label for this run

# Human baseline comparison (Cox 2004)
human_baseline:
  source: "Cox 2004"
  mean_sent: 6.0  # Mean amount sent by humans in Trust Game
  n: null  # Sample size not specified in extraction

# Expected outputs:
# - results/episodes/<run_id>_xie_mvp.jsonl (120 episodes: 3 models × 2 framings × 20 personas)
# - results/aggregates/<run_id>_xie_mvp.json (6 conditions: 3 models × 2 framings)
#
# Total LLM API calls: 120 (manageable for MVP testing)
