## 2 SELF-CONSISTENCY OVER DIVERSE REASONING PATHS

A salient aspect of humanity is that people think differently. It is natural to suppose that in tasks requiring deliberate thinking, there are likely several ways to attack the problem. We propose that such a process can be simulated in language models via sampling from the language model's decoder. For instance, as shown in Figure 1, a model can generate several plausible responses to a math question that all arrive at the same correct answer (Outputs 1 and 3). Since language models are not perfect reasoners, the model might also produce an incorrect reasoning path or make a mistake in one of the reasoning steps (e.g., in Output 2), but such solutions are less likely to arrive at the same answer. That is, we hypothesize that correct reasoning processes, even if they are diverse, tend to have greater agreement in their final answer than incorrect processes.

We leverage this intuition by proposing the following self-consistency method. First, a language model is prompted with a set of manually written chain-of-thought exemplars (Wei et al., 2022). Next,

Table 1: Accuracy comparison of different answer aggregation strategies on PaLM-540B.

|                                | GSM8K      | MultiArith   | AQuA       | SVAMP      | CSQA       | ARC-c      |
|--------------------------------|------------|--------------|------------|------------|------------|------------|
| Greedy decode                  | 56.5       | 94.7         | 35.8       | 79.0       | 79.0       | 85.2       |
| Weighted avg (unnormalized)    | 56.3 ± 0.0 | 90.5 ± 0.0   | 35.8 ± 0.0 | 73.0 ± 0.0 | 74.8 ± 0.0 | 82.3 ± 0.0 |
| Weighted avg (normalized)      | 22.1 ± 0.0 | 59.7 ± 0.0   | 15.7 ± 0.0 | 40.5 ± 0.0 | 52.1 ± 0.0 | 51.7 ± 0.0 |
| Weighted sum (unnormalized)    | 59.9 ± 0.0 | 92.2 ± 0.0   | 38.2 ± 0.0 | 76.2 ± 0.0 | 76.2 ± 0.0 | 83.5 ± 0.0 |
| Weighted sum (normalized)      | 74.1 ± 0.0 | 99.3 ± 0.0   | 48.0 ± 0.0 | 86.8 ± 0.0 | 80.7 ± 0.0 | 88.7 ± 0.0 |
| Unweighted sum (majority vote) | 74.4 ± 0.1 | 99.3 ± 0.0   | 48.3 ± 0.5 | 86.6 ± 0.1 | 80.7 ± 0.1 | 88.7 ± 0.1 |

we sample a set of candidate outputs from the language model's decoder, generating a diverse set of candidate reasoning paths. Self-consistency is compatible with most existing sampling algorithms, including temperature sampling (Ackley et al., 1985; Ficler &amp; Goldberg, 2017), topk sampling (Fan et al., 2018; Holtzman et al., 2018; Radford et al., 2019), and nucleus sampling (Holtzman et al., 2020). Finally, we aggregate the answers by marginalizing out the sampled reasoning paths and choosing the answer that is the most consistent among the generated answers.

In more detail, assume the generated answers a i are from a fixed answer set, a i ∈ A , where i = 1 , . . . , m indexes the m candidate outputs sampled from the decoder. Given a prompt and a question, self-consistency introduces an additional latent variable r i , which is a sequence of tokens representing the reasoning path in the i -th output, then couples the generation of ( r i , a i ) where r i → a i , i.e., generating a reasoning path r i is optional and only used to reach the final answer a i . As an example, consider Output 3 from Figure 1: the first few sentences ' She eats 3 for breakfast ... So she has 9 eggs * $2 = $18. ' constitutes r i , while the answer 18 from the last sentence, ' The answer is $18 ', is parsed as a i . 1 After sampling multiple ( r i , a i ) from the model's decoder, self-consistency applies a marginalization over r i by taking a majority vote over a i , i.e., arg max a ∑ m i =1 ✶ ( a i = a ) , or as we defined as the most 'consistent' answer among the final answer set.

In Table 1, we show the test accuracy over a set of reasoning tasks by using different answer aggregation strategies. In addition to majority vote, one can also weight each ( r i , a i ) by P ( r i , a i | prompt , question ) when aggregating the answers. Note to compute P ( r i , a i | prompt , question ) , we can either take the unnormalized probability of the model generating ( r i , a i ) given ( prompt , question ) , or we can normalize the conditional probability by the output length (Brown et al., 2020), i.e.,

<!-- formula-not-decoded -->

where log P ( t k | prompt , question , t 1 , . . . , t k -1 ) is the log probability of generating the k -th token t k in ( r i , a i ) conditioned on the previous tokens, and K is the total number of tokens in ( r i , a i ) . In Table 1, we show that taking the 'unweighted sum', i.e., taking a majority vote directly over a i yields a very similar accuracy as aggregating using the 'normalized weighted sum'. We took a closer look at the model's output probabilities and found this is because for each ( r i , a i ) , the normalized conditional probabilities P ( r i , a i | prompt , question ) are quite close to each other, i.e., the language model regards those generations as 'similarly likely'. 2 Additionally, when aggregating the answers, the results in Table 1 show that the 'normalized' weighted sum (i.e., Equation 1) yields a much higher accuracy compared to its unnormalized counterpart. For completeness, in Table 1 we also report the results by taking a 'weighted average', i.e., each a gets a score of its weighted sum divided by ∑ m i =1 ( a i = a ) , which results in a much worse performance.

✶ Self-consistency explores an interesting space between open-ended text generation and optimal text generation with a fixed answer. Reasoning tasks typically have fixed answers, which is why researchers have generally considered greedy decoding approaches (Radford et al., 2019; Wei et al., 2022; Chowdhery et al., 2022). However, we have found that even when the desired answer is fixed, introducing diversity in the reasoning processes can be highly beneficial; therefore we leverage

1 The parser is task dependent. For arithmetic reasoning, we parse the first numerical part as the final answer after the model generates 'The answer is '. For commonsense reasoning, we parse the full string answer as the final answer after the model generates 'The answer is '. Most generated outputs have a consistent format of '{Reasoning paths}. The answer is X.' if we prompt the language model in this format.

2 This also means that the language model is not well calibrated and thus cannot distinguish well between correct solutions and wrong solutions, which also explains why additional re-rankers were trained to better judge the quality of the solutions in previous work (Cobbe et al., 2021; Thoppilan et al., 2022).

sampling, as commonly used for open-ended text generation (Radford et al., 2019; Brown et al., 2020; Thoppilan et al., 2022), to achieve this goal. One should note that self-consistency can be applied only to problems where the final answer is from a fixed answer set, but in principle this approach can be extended to open-text generation problems if a good metric of consistency can be defined between multiple generations, e.g., whether two answers agree or contradict each other.
