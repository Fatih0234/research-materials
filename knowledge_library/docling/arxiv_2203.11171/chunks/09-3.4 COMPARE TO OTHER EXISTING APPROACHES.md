## 3.4 COMPARE TO OTHER EXISTING APPROACHES

We conduct a set of additional studies and show that self-consistency significantly outperforms existing methods including sample-and-rank, beam search, and ensemble-based approaches.

Comparison to Sample-and-Rank One commonly used approach to improve generation quality is sample-and-rank, where multiple sequences are sampled from the decoder and then ranked according to each sequence's log probability (Adiwardana et al., 2020). We compare self-consistency with sample-and-rank on GPT-3 code-davinci-001 , by sampling the same number of sequences from the decoder as self-consistency and taking the final answer from the top-ranked sequence. The results are shown in Figure 3. While sample-and-rank does improve the accuracy with additionally sampled sequences and ranking, the gain is much smaller compared to self-consistency.

Figure 3: Self-consistency significantly outperforms sample-and-rank with the same # of samples.

<!-- image -->

Comparison to Beam Search In Table 6, we compare self-consistency with beam search decoding on the UL2-20B model. For a fair comparison we report the accuracy under the same number of beams and reasoning paths. On both tasks self-consistency outperforms beam search significantly. Note self-consistency can also adopt beam search to decode each reasoning path (results are shown as 'Self-consistency using beam search'), but its performance is worse compared to self-consistency with sampling. The reason is that beam search yields a lower diversity in the outputs (Li &amp; Jurafsky, 2016), while in self-consistency the diversity of the reasoning paths is the key to a better performance.

Table 6: Compare self-consistency with beam search decoding on the UL2-20B model.

|            | Beam size / Self-consistency paths   | 1          | 5          | 10         | 20         | 40         |
|------------|--------------------------------------|------------|------------|------------|------------|------------|
|            | Beam search decoding (top beam)      | 23.6       | 19.3       | 16.1       | 15.0       | 10.2       |
| AQuA       | Self-consistency using beam search   | 23.6       | 19.8 ± 0.3 | 21.2 ± 0.7 | 24.6 ± 0.4 | 24.2 ± 0.5 |
| AQuA       | Self-consistency using sampling      | 19.7 ± 2.5 | 24.9 ± 2.6 | 25.3 ± 1.8 | 26.7 ± 1.0 | 26.9 ± 0.5 |
| MultiArith | Beam search decoding (top beam)      | 10.7       | 12.0       | 11.3       | 11.0       | 10.5       |
| MultiArith | Self-consistency using beam search   | 10.7       | 11.8 ± 0.0 | 11.4 ± 0.1 | 12.3 ± 0.1 | 10.8 ± 0.1 |
| MultiArith | Self-consistency using sampling      | 9.5 ± 1.2  | 11.3 ± 1.2 | 12.3 ± 0.8 | 13.7 ± 0.9 | 14.7 ± 0.3 |

Comparison to Ensemble-based Approaches We further compare self-consistency to ensemblebased methods for few-shot learning. In particular, we consider ensembling by: (1) prompt order permutation: we randomly permute the exemplars in the prompt 40 times to mitigate model's sensitivity to prompt order (Zhao et al., 2021; Lu et al., 2021); and (2) multiple sets of prompts (Gao et al., 2021): we manually write 3 different sets of prompts. We took majority vote of the answers from greedy decoding in both approaches as an ensemble. Table 7 shows that compared to self-consistency, existing ensemble-based approaches achieve a much smaller gain. 8 In addition, note that self-consistency is different from a typical model-ensemble approach, where multiple models are trained and their outputs are aggregated. Self-consistency acts more like a 'self-ensemble' on top of a single language model. We additionally show the results of ensembling multiple models in Appendix A.1.3 where the model-ensembles perform much worse compared to self-consistency.

Table 7: Self-consistency outperforms prompt-order and multi-prompt ensembles on LaMDA-137B.

|                                     | GSM8K      | MultiArith   | SVAMP      | ARC-e      | ARC-c      |
|-------------------------------------|------------|--------------|------------|------------|------------|
| CoT (Wei et al., 2022)              | 17.1       | 51.8         | 38.9       | 75.3       | 55.1       |
| Ensemble (3 sets of prompts)        | 18.6 ± 0.5 | 57.1 ± 0.7   | 42.1 ± 0.6 | 76.6 ± 0.1 | 57.0 ± 0.2 |
| Ensemble (40 prompt permutations)   | 19.2 ± 0.1 | 60.9 ± 0.2   | 42.7 ± 0.1 | 76.9 ± 0.1 | 57.0 ± 0.1 |
| Self-Consistency (40 sampled paths) | 27.7 ± 0.2 | 75.7 ± 0.3   | 53.3 ± 0.2 | 79.3 ± 0.3 | 59.8 ± 0.2 |

8 Self-consistency is compatible with both ensemble approaches and we show the results in Appendix A.1.4.
