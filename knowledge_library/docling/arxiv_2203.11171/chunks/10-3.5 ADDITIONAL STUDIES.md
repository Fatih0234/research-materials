## 3.5 ADDITIONAL STUDIES

We conducted a number of additional experiments to analyze different aspects of the self-consistency method, including its robustness to sampling strategies and parameters, and how it works with imperfect prompts and non-natural-language reasoning paths.

Self-Consistency is Robust to Sampling Strategies and Scaling We show self-consistency is robust to sampling strategies and parameters, by varying T in temperature sampling (Ackley et al., 1985; Ficler &amp; Goldberg, 2017), k in topk sampling (Fan et al., 2018; Holtzman et al., 2018; Radford et al., 2019), and p in nucleus sampling (Holtzman et al., 2020), over PaLM-540B in Figure 4 (left). Figure 4 (right) shows that self-consistency robustly improves performance across all scales for the LaMDA-137B model series. The gain is relatively lower for smaller models due to certain abilities (e.g., arithmetic) only emerge when the model reaches a sufficient scale (Brown et al., 2020).

Figure 4: GSM8K accuracy. (Left) Self-consistency is robust to various sampling strategies and parameters. (Right) Self-consistency improves performance across language model scales.

<!-- image -->

<!-- image -->

Self-Consistency Improves Robustness to Imperfect Prompts For few-shot learning with manually constructed prompts, human annotators sometimes make minor mistakes when creating the prompts. We further study if self-consistency can help improve a language model's robustness to imperfect prompts. 9 We show the results in Table 8: while imperfect prompts decrease accuracy with greedy decoding (17.1 â†’ 14.9), self-consistency can fill in the gaps and robustly improve the results.

Additionally, we found that the consistency (in terms of % of decodes agreeing with the final aggregated answer) is highly correlated with accuracy (Figure 5, over GSM8K). This suggests that one can use self-consistency to provide an uncertainty estimate of the model in its generated solutions. In other words, one can use low consistency as an indicator that the model has low confidence; i.e., self-consistency confers some ability for the model to 'know when it doesn't know'.

| LaMDA-137B   | Prompt with correct chain-of-thought                                 |   17.1 14.9 |
|--------------|----------------------------------------------------------------------|-------------|
| LaMDA-137B   | Prompt with imperfect chain-of-thought + Self-consistency (40 paths) |        23.4 |
| LaMDA-137B   | Prompt with equations                                                |         5   |
| LaMDA-137B   | + Self-consistency (40 paths)                                        |         6.5 |
| PaLM-540B    | Zero-shot CoT (Kojima et al., 2022)                                  |        43   |
| PaLM-540B    | + Self-consistency (40 paths)                                        |        69.2 |

Figure 5: The consistency is correlated with model's accuracy.

<!-- image -->

Self-Consistency Works for Non-Natural-Language Reasoning Paths and Zero-shot CoT We also tested the generality of the self-consistency concept to alternative forms of intermediate reasoning like equations (e.g., from ' There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. ' to ' 3 + 2 = 5 '). The results are shown in Table 8 ('Prompt with equations'): self-consistency still improves accuracy by generating intermediate equations; however, compared to generating natural language reasoning paths, the gain is smaller since the equations are much shorter and less opportunity remains for generating diversity in the decoding process. In addition, we tested self-consistency with zero-shot chain-of-thought (Kojima et al., 2022) and show that self-consistency works for zero-shot CoT as well and improves the results significantly (+26.2%) in Table 8.

9 We use the same prompts as before, but swap all the numbers in the reasoning paths with random numbers except the final answer, e.g., from ' There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. ' to ' There are 7 cars in the parking lot already. 6 more arrive. Now there are 7 + 6 = 5 cars. '.
