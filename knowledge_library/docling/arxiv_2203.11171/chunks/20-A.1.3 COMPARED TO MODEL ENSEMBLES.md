## A.1.3 COMPARED TO MODEL ENSEMBLES

Additionally, we provide results of directly ensembling the outputs from multiple language models . The results are shown in Table 10, by greedily decoding sequences from 3 language models and

Figure 8: Self-consistency (blue) significantly improves accuracy across various arithmetic and commonsense reasoning tasks, over PaLM-540B. Sampling a higher number of diverse reasoning paths consistently helps reasoning accuracy.

<!-- image -->

Table 9: GSM8K accuracy over PaLM-540B. The results show robustness of self-consistency with respect to different prompts in the input.

|                        | Prompt set 1 (used in the main text)   | Prompt set 2   | Prompt set 3   |
|------------------------|----------------------------------------|----------------|----------------|
| CoT (Wei et al., 2022) | 56.5                                   | 54.6           | 54.0           |
| Self-consistency       | 74.4 (+17.9)                           | 72.1 (+17.5)   | 70.4 (+16.4)   |

taking the majority vote (averaged over 10 runs). Note this is a typical ensemble approach (averaging over the predictions over multiple models) and it achieves a performance significantly worse than self-consistency (self-consistency over PaLM-540B gets an accuracy of 74.4%), as lower-capacity models drag down the performance of higher-capacity models. In addition, this approach is limited in two ways: 1) It requires multiple models for an ensemble which might not always be available, while self-consistency only requires one single model to 'self-ensemble'; 2) If one of the models is much weaker, it can actually hurt the final performance.

Table 10: Comparison of GSM8K accuracy over multiple-model ensembles.

| Method             | Method                                                  | GSM8K accuracy   |
|--------------------|---------------------------------------------------------|------------------|
| Single model       | PaLM-540B, greedy / self-consistency                    | 56.5 / 74.4      |
|                    | LaMDA-137B + PaLM-540B                                  | 36.9 ± 0.5       |
| Ensemble of models | PaLM-540B + GPT-3 (code-davinci-001, 175B)              | 36.6 ± 0.4       |
| Ensemble of models | LaMDA-137B + GPT-3 (code-davinci-001, 175B)             | 16.0 ± 0.8       |
| Ensemble of models | LaMDA-137B + PaLM-540B + GPT-3 (code-davinci-001, 175B) | 33.3 ± 0.7       |
