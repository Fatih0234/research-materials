## 1 Introduction

Most economic research takes one of two forms: (a) 'What would homo economicus do?' and b) 'What did homo sapiens actually do?' The (a)-type research takes a maintained model of humans, homo economicus , and subjects it to various economic scenarios, endowed with different resources, preferences, information, etc., and then deducing behavior; this behavior can then be compared to the behavior of actual humans in (b)-type research.

In this paper, I argue that newly developed large language models (LLM)-because of how they are trained and designed-can be thought of as implicit computational models of humans-a homo silicus . These models can be used the same way economists use homo economicus : they can be given endowments, put in scenarios, and then their behavior can be explored-though in the case of homo silicus , through computational simulation, not a mathematical deduction. 1 This is possible because LLMs can now respond realistically to a wide range of textual inputs, giving responses similar to what we might expect from a human. It is essential to note that this is a new possibility-that LLMs of slightly older vintage are unsuited for these tasks, as I will show.

I consider the reasons the reasons why AI experiments might be helpful in understanding actual humans. The core of the argument is that LLMs-by nature of their training and design-are (1) computational models of humans and (2) likely possess a great deal of latent social information. For (1), the creators of LLMs have designed them to respond in ways similar to how a human would react to prompts-including prompts that are economic scenarios. The design imperative to be 'realistic'' is why they can be thought of as computational models of humans. For (2), these models likely capture latent social information such as economic laws, decision-making heuristics, and common social preferences because the LLMs are trained on a corpus that contains a great deal of written text where people reason about and discuss economic matters: What to buy, how to bargain, how to shop, how to negotiate a job offer, how to make a job offer, how many hours to work, what to do when prices increase, and so on.

Like all models, any particular homo silicus is wrong, but that judgment is separate from a decision about usefulness. To be clear, each homo silicus is a flawed model and can often give responses far away from what is rational or even sensical. But ultimately, what will matter in practice is whether these AI experiments are practically valuable for generating insights. As such, the majority of the paper focuses on GPT-3 experiments.

Each experiment is motivated by a classic experiment in the behavioral economics literature. I use Charness and Rabin (2002), Kahneman et al. (1986), and Samuelson and

1 Lucas (1980) writes, 'One of the functions of theoretical economics is to provide fully articulated, artificial economic systems that can serve as laboratories in which policies that would be prohibitively expensive to experiment with in actual economies can be tested out at much lower cost.'

Zeckhauser (1988). I selected these experiments because they are simple to describe and implement. They also have clear qualitative results that can be compared to the AI experimental outcomes.

I use the simple unilateral dictator games from Charness and Rabin (2002). I show that endowing the AI with various social preferences affects play. Instructing the AI agent that it only cares about equity will cause it to choose the equitable outcomes; telling the agent it cares about efficiency will cause the selection of the pay-off maximizing outcomes; telling the agent is self-interested will cause the selection of allocations that maximize narrow self-interest. Interestingly, without any endowment, the AI will choose efficient outcomes. However, only the most capable GPT-3 model-text-davinci-003-will change its choices in the dictator game. More primitive or less capable models cannot do this-these typically just select the same choice.

I next present experiments motivated by Kahneman et al. (1986), which reports survey responses to economic scenarios. In the paper, there is an example where subjects imagine a hardware store raising the price of snow shovels following a snowstorm. They simply stated whether doing this was fair or unfair. Illustrating the benefit of GPT-3 agents, unlike Kahneman et al. (1986), I also vary the amount by which the store increases the price, the political leanings of the respondent, and how the price change is framed. I show that large gouging is viewed more negatively; the largest price increases earn approbation even from AI libertarians. Endowed political views matter, with predictable effects-AI agents of the right are more sanguine about gouging generally. Framing does not seem to matter too much.

Continuing with the theme of framing, I present the AI agents with a decision-making scenario introduced by Samuelson and Zeckhauser (1988). In the paper, the respondent has to allocate a federal budget between highway safety and car safety. The original paper showed humans are subject to a status quo bias, preferring budget options when presented as the status quo. I replicate this result by first endowing AI agents with baseline views about the relative importance of car or highway safety. I then put those agents through different scenarios, with each possible allocation taking a turn as the status quo. I find that GPT-3 text-davinci-003 is subject to status quo bias.

Finally, I explore a hiring scenario motivated by my paper, Horton (2023), which shows in a field experiment that employers facing a minimum wage will substitute towards higher-wage workers. I create a scenario where an employer is trying to hire a worker as a dishwasher and faces a collection of applicants that differ in their experience and wage ask, which are chosen randomly. The AI agent makes an experience/wage trade-off. I then impose a minimum wage that forces applicants bidding below that minimum to bid up. As in my paper, the minimum wage raises wages by causing a shift in the hiring of more experienced applicants.

Ultimately, we care about the behavior of actual humans and so results from AI ex-

periments will still require empirical confirmation. 2 As such, what is the value of these experiments? The most obvious use is to pilot experiments in silico first to gain insights. They could cheaply and easily explore the parameter space, test whether behaviors seem sensitive to the precise wording of various questions, and generate data that will 'look like' the actual data. The advantages in terms of speed and cost are so enormous-the experiments in this paper were run in minutes for a trivial amount of money-could make this relatively easy, especially as software tools develop. 3 As insights are gained, they could guide actual empirical work-or interesting effects could be captured in more traditional theory models. This use of simulation as an engine of discovery is similar to what many economists do when building a 'toy model'-a tool not meant to be reality but rather a tool to help us think.

In terms of contribution, the most closely related paper is Aher, Arriaga and Kalai (2022), which also convincingly demonstrates that GPT-3 can reproduce several experimental results in psychology and linguistics and offer acceptance in behavior in the ultimatum game. The paper also makes a similar argument about the potential usefulness of LLMs for social science. However, the argument is that they can be used when experiments are not feasible or ethical. The relative contribution of this paper-beyond extending and adding more economic experiments-is drawing the connection to the common research paradigm of economics and the role a foundational model/assumption like rationality plays in research. LLM experimentation is more akin to the practice of economic theory, despite superficially looking like empirical research.
