## 2 Background and conceptual issues

Large language models are machine learning models trained on very large datasets of text. The goal is to be able to generate human-like text or perform natural language processing tasks. Large language models have achieved impressive results on various natural language processing tasks, such as translation, summarization, and text generation. One of the most well-known large language models is GPT-3 (Generative Pre-trained Transformer 3), developed by OpenAI. All examples in this paper are based on GPT-3.

The above is a non-technical description of LLMs, which raises the question of whether a deeper understanding is needed. I would argue not. To do economics-even behavioral economics-we do not have to study neurons and parts of the brain. What matters is that LLMs are systems created for some understandable purpose with an explicit goal of optimization. As Simon (1996) notes, 'sciences of the artificial' can usefully abstract away from the

2 My concern is not with using LLM as a participant in games or the productive process (Westby and Riedl, 2022), of using experiments to study LLMs per se (Alberti, Lee and Collins, 2019), though I think both are fascinating. I am proposing using LLMs as an indirect way to study humans.

3 There is an analogy to protein-folding, where it is possible to find proteins via simulation and then find them in the real world (Kuhlman, Dantas, Ireton, Varani, Stoddard and Baker, 2003).

micro-details of construction so long as the created object is viewed as trying to maximize something subject to the constraints of the environment.
