## 2.1 The 'Garbage in, Garbage out' critique

One critique of using LLMs for social science is that because LLMs are trained on a corpus too large to be carefully curated, they are subject to a 'garbage in, garbage out' problem (Bender, Gebru, McMillan-Major and Shmitchell, 2021). Of course, every recycling plant is a testament to the notion that garbage in does not imply garbage out. But even if the corpus is carefully curated, the worry is that homo silicus is informed not by 'humans in general' but the highly selected pool of 'humans creating public writing, and then selected again what they choose to say.' And economists have historically taken a dim view of the economic content of mere statements rather than behaviors. This would seem to be a damning critique of a model literally trained on statements.

One potential response mirrors the Friedman (1953) argument that the realism of homo economicus or our assumptions more generally do not matter, and we should evaluate this approach on whether it can generate useful results. The proof of the pudding is in the eating, and 'eating' for our purposes is whether they can help us expand the frontiers of human knowledge more quickly. Suppose the primary use of homo silicus experiments is to simulate experiments before trying them in the real world, and this method is adopted. In that case, these debates are somewhat moot. But I think it is helpful to consider a non-Friedman response to the critiques.

For the 'stated versus revealed' preference critique, this idea is only superficially persuasive. The training corpus is not millions of lines of people lying about their reservation values in a bargaining scenario. Much of it is text is about people reasoning how to approach various economic questions, including 'stage whispers' about their true intentions, explaining how they would deal with a situation. 4

The 'garbage in, garbage' out critique rests partly on the notion that the responses of LLMs are a sort of weighted average. This is not correct. They are more like random number generators than estimators. If you trained an LLM on millions of people reporting random draws from U [0 , 1], it would not respond with â‰ˆ 0 . 5 but rather be more or less equally like to return any number in [0 , 1]. But there is a stochastic version of the garbage in; garbage out critique. Suppose 'true' social science was random numbers drawn from [0 , 1], but you used a different corpus included in the corpus of random numbers a prefix of either 'bad:x' (and x was drawn from N (0 , 1)) or 'good:x' with x drawn from U [0 , 1]. The unconditioned response

4 My dad runs a construction company and has to negotiate constantly. He said one of his useful negotiating skills is the ability to read upside down because many people will write their reservation value on a piece of paper they have in front of them (often underlined). And by me putting this text in a paper on the public Internet, this tiny piece of human reasoning about economic life is now available for LLMs to learn.

would indeed be 'bad'-a mixture distribution of the uniform and unit normal distributions. But simply prompting the model with 'good:' would fix the issue so long as the model is good at what it was designed to do: generate candidate solutions to an optimization problem like a good Bayesian.

Of course, there is no 'good:' prefix to use, but this approach of prompting to get conditional distributions of responses might be 'good enough' for the research question. Argyle, Busby, Fulda, Gubler, Rytting and Wingate (2022) makes this point in their perfectly titled paper 'Out of one, many'-there is not a single LLM but rather a model capable of being conditioned to take on different personas that respond realistically. And even if not perfect, the demands of representativeness in the social sciences have always depended on the research question. If the research question is 'How do US Presidents incorporate CIA intelligence estimates into decision-making?' you will need an extraordinary sample; if your research question is 'Do humans have physical mass?' anyone will do. Most social science questions are somewhere in between.

One advantage economists have in using LLMs is they tend to pose questions that place few demands on the sample. We do not think of demand curves sloping downward as a 'Western, Rich, Industrialized, and Democratic' phenomenon but rather as a result of rational goalseeking that nearly all humans engage in. The willingness of economists to use undergraduates at elite four-year universities for laboratory experiments is partially a convenience-but also consistent with a disciplinary point of view we share with psychologists that it is not likely to matter much. More generally, much of social science is concerned not with the precise measure of some level but with the direction of causal effects (Horton, Rand and Zeckhauser, 2011).
