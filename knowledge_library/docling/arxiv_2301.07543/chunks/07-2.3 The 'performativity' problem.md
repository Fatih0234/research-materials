## 2.3 The 'performativity' problem

With billions of parameters and a massive training corpus, one might think LLMs are simply repeating back to us something they have already 'read' somewhere in their training corpus. In full disclosure, this was my prior. This view is not correct. It is also inconsistent with the worry that they hallucinate and make up new 'facts' (which they do regularly).

Even if they do not cite specific experimental results back to us, there is potentially a 'performativity' problem in the sense that AI agents might behave following our theories and empirical results because they have read about them (MacKenzie, 2007): LLMs have read and memorized our textbooks and papers and will parrot results back to us if asked concretely. But the fact it does not 'know' these theories is useful to us because it will not try to apply them. For example, it is clear GPT3 knows π and will recite it if asked for the answer in textbook language, yet it also does not know π or how to apply it in even simple real settings. Like students who have crammed for an exam, these models 'know' things, but often do not apply that knowledge consistently to scenarios. This is useful because it makes this 'performativity' critique less important. But even if it is a concern, we should avoid a textbook framing of questions. The increased capability of the LLMs likely expands the possible experiments, but it also makes performativity a greater concern.

It is also possible to to determine if the LLM knows the results. For example, I asked GPT3 text-davinci-003 a series of questions about the results from the Charness and Rabin (2002) experiment (fractions that chose 'left') in the various scenarios. I asked: 'In Charness and Rabin (2002), which fraction of respondents chose 'Left' in the Berk29, Berk 26, Berk23 and Barc2 scenarios?' and it returned 'In the Berk29, Berk 26, Berk23, and Barc2 scenarios, approximately 59%, 57%, 43%, and 16% of respondents chose 'Left,' respectively.' The actual answers are 31%, 78%,100%, and 52%.
