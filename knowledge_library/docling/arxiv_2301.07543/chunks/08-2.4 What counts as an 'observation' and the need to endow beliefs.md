## 2.4 What counts as an 'observation' and the need to endow beliefs

With a particular LLM, there is a single model, so it would seem that N = 1 no matter what. However, it has no fixed persona-it can be induced to play different agents via prompts. For example, in the Kahneman et al. (1986) example, I will have the agent answer the question 'as' a libertarian, a socialist, a moderate, and so on. This agent 'programming' is not unlike the experimental economics practice of giving an experimental subject a card that says their marginal cost of producing a widget is 15 tokens. Although I do some of this in this paper, Argyle et al. (2022) 'endow' LLM agents with demographic characteristics and then get responses in various scenarios that match what is seen empirically.

The responses can be stochastic, depending on the 'temperature' given to the model as

a parameter. And, of course, different models can lead to different responses. Unlike the one homo economicus that is rational, there are many homo silci .

LLMs are not 'fine-tuned' to any particular language application-changes in prompts obtain different behavior. However, it is possible to fine-tune models for particular applications by giving new examples or providing feedback (sometimes called RLHF, or 'Reinforcement Learning from Human Feedback.') This is potentially useful, as one could imagine creating agents with extensive experience or skills and using them as subjects instead. For example, instead of a blank slate homo silicus , one could create a trader with extensive knowledge of financial markets and conventions. Or if there are certain behavioral biases that we think are commonplace but disciplined out by market interactions (e.g., how List (2011) shows the endowment effect is mostly eliminated among professional traders), those kinds of agents could be created.
