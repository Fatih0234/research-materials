## 3.1 A social preferences experiment: Charness and Rabin (2002)

In Charness and Rabin (2002), experimental subjects had to choose between two allocations that involved some trade-off between efficiency and equity. Although there are several experiments in the paper, I focus on the unilateral dictator game. All the dictator games are structured as so:

Left: Person B gets $ 600 and Person A gets $ 400 or Right: Person B gets $ 300 and Person A gets $ 700.

If you are Person B and deciding, you have to give up $ 100, so Person A can get an extra $ 400. As a short-hand, we can write this as

<!-- formula-not-decoded -->

Person B has to choose between 'left' or 'right.'

I had a GPT-3 AI consider each scenario without any endowment of views. For each scenario, I also endowed the AI agent with a point of view-namely, they only care about equity, total payoff, and personal payoff. For this task, I also used different GPT3 models. In addition to the most capable text-davinci-003, I also use the less capable LLMs: text-ada-001, text-babbage-001, and text-currie-001.

Figure 1 plots the results. Within each pane of the figure, the y-axis is the scenario. The x-axis is 'left' or 'right.' I report the fractions playing 'left.' In the first column from the left, I report the results from the original experiment. E.g., in Berk29, 31% of respondents chose (400,400) while 68% chose (750, 400). Recall that the respondent is Person B, so a considerably higher fraction was willing to create a more unequal but more efficient outcome (i.e., Person A gets an extra 350).

and so

Figure 1: Charness and Rabin (2002) Simple Tests choices by model type and endowed 'personality'

<!-- image -->

Notes: This shows the faction of AI subjects choosing each option, by framing.

Across scenarios, note that there is variation in answers- except the extremely spiteful Berk23 scenario (in which you forgo 200 to ensure Person A does not get 800), there is variation in fractions choosing left and right. This 'natural' human variation in preferences does not exist in LLMs unless they are endowed with differences.

The rest of the figures' columns correspond to different endowments given to GPT-3. One endowment is no additional instruction. For the rest, I endow them-by pre-pending to the prompt-messages of

- Inequity aversion: 'You only care about fairness between players.'
- Efficient: 'You only care about the total payoff of both players'
- Self-interested: 'You only care about your own payoff'

The different rows in the figure correspond to other models. In the top row, the model is GPT-3 text-davinci-003, the most advanced model used in the experiment.

Starting from the far right, the self-interested AI consistently chooses the payoff that maximizes its payoff by picking 'left.' The only exception is the Berk29 scenario, where payoffs are equal from B's perspective, but playing 'Right' gives a much larger payoff to A.

For the efficiency-minded AI in the second column from the left, it always chooses the option that maximizes the total payoff.

For the inequity-averse AI in the third column from the left, it chooses the option that minimizes the discrepancy between the two players except for the Berk23 scenario, which wastes 1,000 (800, 200). Even an inequity-averse AI seems to have a limit.

In the second column from the right, the AI was given no endowment at all-though it was instructed it is Person B. Interestingly, the AI not given preferences acts like the social planner maximizing payoffs.

In the bottom row of the figure, I pool together all the less advanced GPT-3 models because they do not seemingly change their answers despite the endowments (except the unendowed agent, which chooses all 'Right' in one model). Notwithstanding that one exception, they always choose 'Left' which is the selfish option. However, it is unclear if this default is meaningful or reflects the order in which results are presented.

One could imagine defining agent 'types' as bit vectors for whether they play 'left' e.g., the fairness player would be represented by v f = (1 , 0 , 1 , 0 , 0 , 1). Then we could find the mixture of âˆ‘ k w k v k that most closely approximates the actual experimental play, then uses that distribution for other games. If we minimize the mean square error, we get about 15% fair, 32% efficient, and 52% selfish. We could then use this population for other games and compare how it does relative to reality.
