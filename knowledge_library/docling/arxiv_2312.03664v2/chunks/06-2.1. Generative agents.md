## 2.1. Generative agents

Simulated agent behavior should be coherent with common sense, guided by social norms, and individually contextualized according to a personal history of past events as well as ongoing perception of the current situation.

March and Olsen (2011) posit that humans generally act as though they choose their actions by answering three key questions:

1. What kind of situation is this?
2. What kind of person am I?
3. What does a person such as I do in a situation such as this?

Our hypothesis is that since modern LLMs have been trained on massive amounts of human culture they are thus capable of giving satisfactory (i.e. reasonably realistic) answers to these questions when provided with the historical context of a particular agent. The idea is that, if the outputs of LLMs conditioned to simulate specific human sub-populations reflect the beliefs and attitudes of those subpopulations as argued in work such as Argyle et al. (2023) then this approach to implementing generative agents should yield agents that can reasonably be said to model humans with some level of fidelity. Safdari et al. (2023) have also found out that personality measurements in the outputs of some LLMs under specific prompting configurations are reliable and valid, therefore generative agents could be used to model humans with diverse psychological profiles. In some cases answering the key questions might require com- mon sense reasoning and / or planning, which LLMs do show capacity for (Huang et al., 2022; Song et al., 2023; Wei et al., 2022; Zhao et al., 2023), and show similar biases in behavioral economics experiments as humans (Aher et al., 2023; Brand et al., 2023; Horton, 2023). The ability of LLMs to learn 'in-context' and zero-shot Brown et al. (2020); Bubeck et al. (2023); Dong et al. (2022); OpenAI (2023) reinforces the hypothesis further-the agent might be able to ascertain what is expected of them in the current situation from a demonstration or an example.

For an LLM to be able to answer the key questions, it must be provided with a record of an agent's historical experience. However, simply listing every event that happened in an agent's life would overwhelm the LLM (it would not fit in the context window). Therefore we follow the approach of Park et al. (2023) and use an associative memory to keep the record of agents experience. Concordia makes it easy to design generative agents in a modular fashion. Our approach was inspired by Park et al. (2023), but designed to be more flexible and modular.

Concordia agents dynamically construct the text that conditions the LLM call they use to select their course of action on each timestep. The context-generation process is factorized into a set of components . Components serve as intermediaries between long-term memories of experience and the relatively compact conditioning text used to generate action. Intuitively, the set of components used in an agent comprise its 'society of mind' (Minsky, 1988), where each component focuses on a certain aspect of the agent or its circumstances which are relevant to generating its current choice of action. For example, if we are building agents for economic simulation, we will add components that describe the agents possessions and financial circumstances. If we want to model the agent's physiological state, we add components that describe the agent's level of thirst and hunger, health and stress levels. Together the components produce the context of action -text which conditions the query to the LLM, asking 'what should this agent do next?'.

A Concordia agent has both a long-term memory and a working memory. Let the long-term

Figure 3 | Illustration of generative agency sampling process defined by eq. 1 and eq. 2.

<!-- image -->

memory be a set of strings m that records everything remembered or currently experienced by the agent. The working memory is z = { ğ‘§ ğ‘– } ğ‘– is composed of the states of individual components (Figure 2). A component ğ‘– has a state ğ‘§ ğ‘– , which is statement in natural language-e.g. 'Alice is at work'. The components update their states by querying the memory (which contains the incoming observations) and using LLM for summarising and reasoning. Components can also condition their update on the current state of other components. For example, the planning component can update its state if an incoming observation invalidates the current plan, conditioned on the state of the 'goal' component. Components can also have internal logic programmed using classic programming, for example a hunger component can check how many calories an agent consumed and how recently it consumed them, and update its state based on the result.

We use the same associative memory architecture as in Park et al. (2023) 3 . We feed the in-

3 The idea of simulating a group of generative agents has been explored in a variety of ways in recent work. Our work is focused on on agent-based modeling for science and for evaluation of digital technologies. Another recent line of work has focused instead on the idea of using groups of generative agents to simulate organizations that solve

coming observations immediately into the agents memory, to make them available when components update 4 .

When creating a generative agent in Concordia, the user creates the components that are relevant for their simulations. They decide on the initial state and the update function. The components are then supplied to the agents constructor.

Formally, the agent is defined as a two step sampling process, using a LLM ğ‘ (see Figure 3 for illustration). In the action step, the agent samples its activity ğ‘ğ‘¡ , given the state of components z ğ‘¡ = { ğ‘§ ğ‘– ğ‘¡ } ğ‘– :

<!-- formula-not-decoded -->

Here ğ‘“ ğ‘ is a formatting function, which creates out of the states of components the context used to sample the action to take. The most simple form of ğ‘“ ğ‘ is a concatenation operator over z ğ‘¡ = { ğ‘§ ğ‘– ğ‘¡ } ğ‘– . We do not explicitly condition on the memory m or observation ğ‘œ , since we can sub-

problems like software companies and to thereby try to build a general-purpose problem solving system (Hong et al., 2023; Li et al., 2023b).

4 For convenience, we also allow the components to subscribe to the observation stream explicitly.

sume them into components. First, we can immediately add o ğ‘¡ to the memory m ğ‘¡ = m ğ‘¡ -1 âˆª o ğ‘¡ . Unlike RL, we do not assume that the agent responds with an action to every observation. The agent can get several observations before it acts, therefore o ğ‘¡ is a set of strings. Then we can set z 0 to be the component that incorporates the latest observations and relevant memories into its state. This allows us to exclusively use the vehicle of components to define the agent.

In the second step the agent samples its state z , given the agents memory m ğ‘¡ up to the present time:

<!-- formula-not-decoded -->

Here, ğ‘“ ğ‘– is a formatting function that turns the memory stream and the current state of the components into the query for the component update. We explicitly condition on the memory stream m , since a component may make specific queries into the agent's memory to update its state. Here eq.2 updates components after every action, but generally , it is up to the agent to decide at what cadence to update each of its components. It is reasonable to update some components less frequently for efficiency or longer term consistency.

Notice how eq.1 and eq.2 are not fundamentally different. What makes the difference between an agent output and a component is that the output of the former is interpreted by the GM as an action in the environment. In eq.1 we also don't explicitly condition on the memory to point out the architectural decision, where components mediate between a long-term memory and the agents working memory. Otherwise, we can think of an agent as a special kind of component and of components as sub-agents.
