## 2.2. Generative environments

RL research was fuelled by the availability of complex games, where the agents can be tested, trained and evaluated (Bellemare et al., 2013; Jaderberg et al., 2019; Vinyals et al., 2019). Here we take an inspiration from table top role playing games like Dungeons and Dragons (Gygax and Cook, 1989). In these games players collabora- tively generate a story, while using rules, dice, pen and paper to ground it-for example, players have to keep their health points above zero to avoid death.

The GM is responsible for all aspects of the simulated world not directly controlled by the agents. The GM mediates between the state of the world and agents' actions. The state of the world is contained in GM's memory and the values of grounded variables (e.g. money, possessions, votes, etc.). To achieve this the GM has to repeatedly answer the following questions:

1. What is the state of the world?
2. Given the state of the world, what event is the outcome of the players activity?
3. What observation do players make of the event?
4. What effect does the event have on grounded variables?

The GM is implemented in a similar fashion to a generative agent. Like agents, the GM has an associative memory similar to Park et al. (2023)'s proposal. Like agents, the GM is implemented using components. However, instead of contextualizing action selection, the components of the GM describe the state of the world-for example location and status of players, state of grounded variables (money, important items) and so on-so that GM can decide the event that happens as the outcome of players' actions. The outcome is described in the event statement (e.g. ' Alice went to the grocery store and met Bob in the cereal aisle'), which is then added to the GM associative memory. After the event has been decided the GM elaborates on its consequences. For example, the event could have changed the value of one of the grounded variables or it could have had an effect on a non-acting player. Figure 1 illustrates this process.

The GM generates an event statement 洧 洧노 in response to each agent action:

<!-- formula-not-decoded -->

Here we explicitly condition on the action attempted by the agent, although it could be subsumed into the components (like observation in

eq.1). This is to highlight that the GM generates an event statement 洧 洧노 in response to every action of any agent, while the agent might take in several observations before it acts (or none at all). After adding the event statement 洧 洧노 to its memory the GM can update its components using the same eq. 2 as the agent. It can then emit observations o 洧녰 洧노 for player 洧녰 using the following equation:

<!-- formula-not-decoded -->

In case the GM judges that a player did not observe the event, no observation is emitted. Notice that the components can have their internal logic written using any existing modelling tools (ODE, graphical models, finite state machines, etc.) and therefore can bring known models of certain physical, chemical or financial phenomena into the simulation.
