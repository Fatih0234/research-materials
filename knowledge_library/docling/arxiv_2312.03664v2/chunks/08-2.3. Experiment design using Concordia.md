## 2.3. Experiment design using Concordia

An experiment is a specific configuration of the agents and the GM, which models a certain kind of social interaction. For example, an experiment that models a small business would have a grounded variable that accounts for money and goods to be exchanged between agents. An experiment modeling local elections in a small town would have grounded variables accounting for votes and voting procedures. An experiment modeling resource governance by a local community, e.g. a lobster fishery, may have grounded variables reflecting the state of the resource as well as financial and political variables.

The experimenter would then control some (independent) variables affecting either the GM or the agents and observe the effect of their intervention on outcome variables. Outcomes of interest may be psychological and per-agent, e.g. responses to questionnaires, or global variables pertaining to the simulation as a whole such as the amount of trade or the average price of goods.

The basic principle of model validation is one of similarity between tested and untested samples. A model typically makes a family of related predictions, and perhaps a rigorous experiment tests only one of them. Nevertheless, if the untested predictions are sufficiently similar to the tested prediction then one might also gain some confidence in the untested predictions. The key question here is how similar is similar enough.

We can articulate some concrete recommendations for best practices in generative agent-based modeling:

1. Measure generalization -Direct measurement of model predictions on truly new test data that could not have influenced either the model's concrete parameters or its abstract specification is the gold standard. For instance, when a model makes predictions about how humans will behave in certain situation then there is no better form of evidence than actually measuring how real people behave when facing the modeled situation. If the prediction concerns the effect of an intervention, then one would need to run the experiment in real life (or find a natural experiment that has not already contaminated the model's training data). However, it is important to remember that direct evidence of generalization trumps other forms of evidence.
2. Evaluate algorithmic fidelity -a validity concept developed recently for research on human behavior using data sampled using generative AI (Argyle et al., 2023). Algorithmic fidelity describes the extent to which a model may be conditioned using sociodemographic backstories to simulate specific human groups (or stereotypes of them, see unsolved issues below). Note however that it's unlikely that algorithmic fidelity would be uniform over diverse research topics or parts of human lived experience. Any particular LLM will be better at simulating some people over other people (Atari et al., 2023), and will work better for some applications than others. Argyle et al. (2023) conclude from this that algorithmic fidelity must be measured anew for each research question. A finding of sufficient algorithmic fidelity to address one research question does not imply the same will be true for others (see also Amirova et al. (2023); Santurkar et al. (2023)).
3. Model comparison -It is a lot easier to

- support the claim that one model is better (i.e. more trustworthy) than another model than to support the claim that either model is trustworthy on an absolute scale without reference to the other.
4. Robustness -It will be important to try to develop standardized sensitivity analysis / robustness-checking protocols. For instance, it's known that LLMs are often quite sensitive to the precise wording used in text prompts. Best practices for GABMs should involve sampling from a distribution of 'details' and ways of asking questions to show that the factors not thought to be mechanistically related to the outcome are indeed as irrelevant as expected. Keep in mind that no amount of sensitivity analysis can substitute for a test of generalization.
5. A useful slogan to keep in mind is that one should try to make the minimal number of maximally general modeling choices . This is a kind of parsimony principle for generative agent-based modeling. Obeying it does not guarantee a model will generalize; nevertheless failure to follow it does often doom generalization since models that are more complex are usually also more brittle, and models that are more brittle generally fail to generalize.

While generalization data is the gold standard, it is often difficult, unethical, or simply impossible to obtain. Therefore the hierarchy of evidence for validating GABMs also includes lower rungs corresponding to weaker forms of evidence. These include:

1. Consistency with prior theory -i.e. checking coherence with predictions of other theoretical traditions. For instance, evidence for the validity of a GABM modeling consumer behavior could be obtained by showing that prices in the model move in ways predicted by classic microeconomic theories of downward-sloping price-quantity demand curves. It is possible to directly evaluate counterfactuals and ceteris paribus stipulations in many kinds of model. As a result, it is often simple to test a model's consistency
2. with a causal theory in a very direct way 5 .
2. Low similarity between validating observations and desired application . How low is too low? Some populations are just very hard to reach by researchers, but some of these populations are very much online. For example individuals with low generalized trust do not pick up the phone to pollsters and do not sign up for experiments. Nevertheless there are millions of such people, and they do use the internet. It's likely that an LLM trained on large amounts of data from the internet would absorb some level of understanding of such groups. In such cases where it is difficult to recruit real participants, adopting a more flexible approach to validating GABMs representing such populations may be the best that can be done.

Several unsolved issues impacting validity in ways specific to ABMs that incorporate generative AI like Concordia are as follows. For now it is unclear how to resolve them.

1. Train-test contamination -this is especially an issue with regard to academic papers. For instance, it's not valid to simply ask an LLM to play Prisoner's Dilemma. LLMs have 'read' countless papers on the topic and that experience surely affects how they respond. However, many researchers are of the opinion that such an experiment may be conducted in a valid way if the interpretation of the situation as Prisoner's Dilemma is somewhat hidden. So instead of describing a situation with prisoners you make up a different story to justify the same incentives. This issue was also discussed in Aher et al. (2023), especially appendix F, see also Ullman (2023).
2. LLMs likely represent stereotypes of human groups (Weidinger et al., 2021). Therefore we may inadvertently study stereotypes of people not their real lived experience. This problem may be exacerbated for minority groups.
3. What happens in the limit of detail? Beyond groupwise algorithmic fidelity it's pos-

5 Non-generative ABMs based on multi-agent reinforcement learning have frequently relied on this kind of evidence (e.g. Johanson et al. (2022); Perolat et al. (2017)).

sible to measure individual-fidelity. How can you validate a model meant to represent a specific individual?
