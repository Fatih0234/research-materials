## 3.3. Concordia agents do not make decisions by optimizing

The cake is a lie.

Portal (Valve, 2007)

We may divide this interpretation into two parts. Really we are making the same point twice, but for two different audiences. First we frame this idea using the retrospective decision-making terminology familiar to Reinforcement Learning (RL) researchers (Section 3.3.1). Second we articulate a very similar point in the language of prospective decision making familiar in game theory, economics, and other theoretical social sciences (Section 3.3.2).

A generative agent acts by asking its LLM ques-

tions of the form 'what does a person such as I do in a situation such as this?'. Notice that this formulation is not consequentialist. The 'reason' for the agent's specific decision is its similarity to the LLM's (and GA's memory) representations of what an agent such as the one in question would do. In recent years considerable effort has gone in to predicting the properties of powerful consequentialist AI decision-maker agents (e.g. Bostrom (2014); Roff (2020)). However, Concordia agents may behave quite differently from consequentialist agents. So much of that theory may not be applicable 7 . It has only recently become possible to explore the kind of agency exhibited by Concordia agents, since doing so relies critically on the LLM powering the agent being powerful enough to approximately understand common-sense reasoning and common social conventions and norms, a milestone which was only recently achieved. To paraphrase March and Olsen (2011), decisions can be justified either via the 'logic of consequence' or via the 'logic of appropriateness'. Much of AI focused previously on the former (at least implicitly), while now using generative agents we begin to consider the latter.
