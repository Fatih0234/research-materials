## 4.5. AI assistants with transparent auditing and credit assignment

Concordia agents can also be used as assistants or synthetic workers. The component system provides a modular and transparent way for the agent designer to define the agents' policy. Some generic components for perception, action, and tool use could be standardised and re-used, while some application and context specific components designed or adjusted by the end-user themselves. The fact the the policy is specified through natural language, rather than a reward or utility, is a feature that would make such agents more versatile and easier to define. For example, a digital secretary can be easily instructed with a phrase "help Priya manage her social calendar, but don't change the work schedule", which would be much harder to specify with a quantitative reward. Concordia agents can potentially lead to development of AI agents capable of intricate social cognition, which would make them safe and dynamically aligned with the current cultural norm.

Moreover, the Component system facilitates transparency in agent operations since the 'chain of thought' leading up to any decision of a Concordia agent could be stored and made available for auditing. Each episode creates a complete trace of component states z 洧노 and the resulting actions 洧녩洧노 . For every action, a human auditor can asses whether it is reasonable under z 洧노 or not. If it is not, than the credit goes to the LLM 洧녷 , which has to be updated. This can mean adding the ( z 洧노 , 洧녩 洧노 ) pair into a dataset that can be later used for fine-tuning or RLHF. If, however, the 洧녩洧노 is deemed reasonable, given z 洧노 , then the credit goes to the components and their specification. The auditor can then manipulate the components to find the source of undesired behaviour and use it to improve the agent.

Scheurer et al. (2023) describe an interesting case where a generative agent modeling an em- ployee of a financial trading firm proves willing to engage in illegal trading based on insider information and strategically deceive others to hide this activity . In real life such outcomes could perhaps be mitigated by designing thought process transparency and capacity for thought auditing after the fact into any generative agent models that would actually be deployed. At least the transparency of the thought process may help assigning responsibility for an ethical lapse to a particular LLM call, perhaps one causing the agent to fail to retrieve its instruction not to engage in illegal activity from memory at the moment when it could prevent the decision to do so. Being able to pinpoint which LLM call in a chain of thought is the problematic one does not remove the longstanding question of neural network interpretability within the specific LLM call (e.g. Adadi and Berrada (2018)). But it does make the issue much easier to mitigate. Since a Concordia-style generative agent has a Python program laying out its chain of thought, that means that as long as the individual LLM call where the unethical behavior originated can be isolated, which should be easy in an audit, then a variety of mitigations are possible. For instance, the agent could potentially be fixed by designing more safeguards into its chain of thought such as generating multiple plans and critiquing them from the perspective of morality, legality, etc (Ag칲era y Arcas, 2022; Bai et al., 2022; Weidinger et al., 2023).

The fact that the internal processing of a Concordia agent is largely conducted in natural language raises new opportunities to develop participatory design protocols where stakeholders can directly modify agents without the intermediaries who are usually needed to translate their ideas into code (Birhane et al., 2022). A generative agent 'reasons' in natural language, and its chain of thought can be steered in natural language. It should be possible to extend participation in the design of such agents to a much wider group of stakeholders.
