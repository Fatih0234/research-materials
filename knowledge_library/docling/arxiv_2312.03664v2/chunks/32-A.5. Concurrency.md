## A.5. Concurrency

The performance bottleneck of the library is waiting on the LLM API calls. To improve the wall time efficiency, we use concurrency during update calls to components. In this way, while one of the components is waiting for the LLM inference, other components can keep updating. This means that the sequence at which the components are updated is not guaranteed . If you would like to update the components sequentially, you can use concordia/generic\_components/sequential.py wrapper, which wraps a set of components into one and updates them sequentially.
