## 4.1 Setup

The efficiency experiment depends on both the size of the model and the tokenizer's vocabulary size. We used Llama-3.1-8B-Instruct with the Llamacpp inference engine as backend for Outlines, Guidance, and Llamacpp. As XGrammar doesn't support Llamacpp as backend , we add an additional experiment with the Hugging Face Transformers inference engine for XGrammar. All experiments are conducted on a single NVIDIA A100-SXM4-80GB GPU with AMD EPYC 7543 (12 cores) CPU. The batch size is set to 1 for all experiments. Additional details about setup are provided in the Appendix E. We also provide a snippet of how we call each engine in the Appendix G.

Addressing coverage bias. The efficiency metrics are meaningful only for instances that a grammar engine can process. Different engines exhibit varying levels of schema

3 The $schema keyword, defined in the JSON Schema specification, allows any schema to selfidentify which version of JSON Schema it is written for.

coverage, with some engines handling a wider range of schemas than others. Engines with lower coverage often process simpler, shorter schemas, which naturally compile and generate faster. As a result, averaging efficiency metrics across covered instances can introduce bias favoring engines with lower coverage. For a more detailed discussion on coverage, see Section 5. To ensure fairness, we calculate efficiency metrics on the intersection of covered instances across all engines.

Table 2: Efficiency metrics for different engines with LlamaCpp as the inference engine. GCT : Grammar Compilation Time, TTFT : Time to First Token, TPOT : Time Per Output Token. Bold values indicate the smallest in each column for GCT, TTFT, and TPOT. All values are median of the samples. Results for the GitHub Hard and Washington Post datasets are provided in Appendix E.

| Dataset       | Framework   | GCT (s)   |   TTFT (s) |   TPOT (ms) |
|---------------|-------------|-----------|------------|-------------|
| GlaiveAI      | LM only     | NA        |       0.1  |       15.4  |
|               | Guidance    | 0.00      |       0.24 |        6.37 |
|               | Llamacpp    | 0.05      |       0.2  |       29.98 |
|               | Outlines    | 3.48      |       3.65 |       30.33 |
| GitHub Easy   | LM only     | NA        |       0.1  |       15.83 |
|               | Guidance    | 0.00      |       0.34 |        7.44 |
|               | Llamacpp    | 0.05      |       0.18 |       27.22 |
|               | Outlines    | 3.71      |       3.97 |       39.78 |
| Snowplow      | LM only     | NA        |       0.11 |       16.23 |
|               | Guidance    | 0.00      |       0.28 |        6.55 |
|               | Llamacpp    | 0.05      |       0.2  |       28.9  |
|               | Outlines    | 3.91      |       4.14 |       42.66 |
| GitHub Medium | LM only     | NA        |       0.2  |       16.68 |
|               | Guidance    | 0.01      |       0.54 |        7.57 |
|               | Llamacpp    | 0.06      |       0.3  |       29.08 |
|               | Outlines    | 8.05      |       8.38 |       46.57 |
| Kubernetes    | LM only     | NA        |       0.16 |       15.32 |
|               | Guidance    | 0.01      |       0.45 |        9.47 |
|               | Llamacpp    | 0.05      |       0.28 |       28.04 |
|               | Outlines    | 5.29      |       5.55 |       46.1  |

Grammar compilation time. There are notable differences in grammar compilation times between the engines. Both Guidance and Llamacpp dynamically compute their constraints during token generation, leading to minimal grammar compilation time. In the middle, XGrammar does include a non-trivial compilation step, but they are able to largely mitigate its impact by running it concurrently with prompt pre-filling. Finally Outlines, which converts JSON schemas into regular-expression based constraints, has significantly higher compilation time.

Time per output token. While Outlines and Llamacpp demonstrate substantially lower throughput than the LM-only approach, Guidance achieves even higher efficiency, which it accomplishes by fast-forwarding 4 certain generation steps with its guidance acceleration [GuidanceAI, 2024b]. Comparing Guidance and XGrammar with the HF Transformers backend shows that Guidance has a significantly better TPOT.
