## 6.1 Setup

Kurt [2024b]; Tam et al. [2024] have introduced a series of tasks to investigate potential quality concerns in constrained decoding, which we leverage and extend in this benchmark. Specifically, we adopt the three reasoning tasks from these studies to evaluate the impact of constrained decoding on task accuracy, as detailed in Table 7. The simple output structure of these tasks was designed to isolate the effects of constrained decoding on reasoning, as outlined by Tam et al. [2024].

For our experiments, we use the Llama-3.1-8B-Instruct model to measure task performance. We follow the original setup and prompt specifications from Kurt [2024b], with full details provided in Appendix F.

Table 7: Task Descriptions and Structures

| Task            | Example                                                              | Structure                         | Metric                      |
|-----------------|----------------------------------------------------------------------|-----------------------------------|-----------------------------|
| Last Letter     | Input: Ian Peter Bernard Stephen Output: nrdn                        | CoT reasoning + answer in a - z   | Case- sensitive exact match |
| Shuffle Objects | Input: Sequence of exchanges among individuals + choices Output: A-E | CoT reasoning + answer in A - E   | Exact match                 |
| GSM8K           | Input: Basic caculation problems Output: Number, e.g. 8              | CoT reasoning + answer as integer | Number exact match          |

We implement the following constraints for the first three tasks: (1) Last Letter the output needs to be a concatenation of letters from a-z; (2) Shuffle Objects the output needs to be a single letter from A-E enclosed in parentheses; (3) GSM8K the output is an valid integer or float number. The outputs for all three tasks are structured as JSON objects with two fields: "reasoning" and "answer" , formatted as {"reasoning": &lt;reasoning about the answer&gt;, "answer": &lt;final answer&gt;} .
