## 0.00 0.50 1.00** 1.00**

Figure 6: JSON Schema test suite coverage by category. Each cell represents the proportion of passing tests for a given category-framework pair, with darker shades indicating higher coverage. A single asterisk (*) marks frameworks tied for the highest (non-zero) coverage, while a double asterisk (**) marks the framework with the single highest coverage in the category.

- 1.0

We provide code snippets that show the use of the JSON Schema Test Suite to assess the test coverage of each constrained decoding framework. For each framework, we implemented a 'test harness' according to the base classes showed in listing 1.

Listing 2 shows the criteria for a test case to pass, which depends on all tests in the case to pass (listing 3). We show the definition of TestCase and Test in listing 4.

Concrete implementations of the test harness for each framework are reported in listings 5, 6, 7, and 8.

```
class Compiler : def __ init __ (self, model _ id: str): """ Builds a Compiler, taking a huggingface model _ id to provide configuration information about the model and/or tokenizer. """ def compile(self, schema: str) -> Masker: """ Compiles a schema into a masker used to validate a stream of tokens according to the schema. Raises an exception if the framework cannot compile the schema. """ class Masker : def advance(self, token: int): """ Advances the masker by one token. Raises an exception if the token is not allowed by the mask. """ def assert _ done(self): """ Asserts that the masker is either in a terminal state or will accept an EOS token, after which it will be in a terminal state. Raises an exception if otherwise. """
```

Listing 1: Abstract test harness

```
def do _ test _ case(test _ case: TestCase, compiler: Compiler, tokenizer: Tokenizer) -> bool: ↪ → try : masker = compiler.compile(json.dumps(test _ case.schema)) except : if all( not test.valid for test in test _ case.tests): # Pass: compile error on a case with only invalid test data return True else : # Fail: datum ↪ → return False for test in test _ case.tests: passed = do _ test(test, tokenizer, masker.copy()) if not passed: # Fail: a test failed return False # Pass: all tests passed
```

Listing 3: Running a test

```
compile error but schema has at least one valid test return True Listing 2: Running a test case def do _ test(test: Test, tokenizer: Tokenizer, masker: Masker) -> bool: tokens = tokenizer(json.dumps(test.data), add _ special _ tokens= False )["input _ ids"] ↪ → try : for token in tokens: masker.advance(token) masker.assert _ done() except : if test.valid: # Fail: valid data was rejected return False else : # Pass: invalid data was rejected return True else : if test.valid: # Pass: valid data was accepted return True else : # Fail: invalid data was accepted return False
```

```
from pydantic import BaseModel from typing import Any, Union class TestCase (BaseModel): schema: Union[bool, dict] tests: list[Test] class Test (BaseModel): data: Any valid: bool Listing 4: TestCase specification import outlines import outlines _ core class OutlinesCompiler (Compiler): def __ init __ (self, model _ id: str): self.tokenizer = outlines.models.transformers(model _ id).tokenizer def compile(self, schema: str) -> "OutlinesMasker": regex = build _ regex _ from _ schema(schema) guide = outlines.fsm.guide.RegexGuide.from _ regex( regex, self.tokenizer ) return OutlinesMasker(guide, eos _ token _ id=self.tokenizer.eos _ token _ id) ↪ → class OutlinesMasker (Masker): def __ init __ (self, guide, eos _ token _ id= None ): self.guide = guide self.state = self.guide.initial _ state self.eos _ token _ id = eos _ token _ id def advance(self, token: int): assert token in self.guide.get _ next _ instruction(self.state).tokens ↪ → self.state = self.guide.get _ next _ state(self.state, token) def assert _ done(self): if not self.guide.is _ final _ state(self.state): assert self.eos _ token _ id in self.guide.get _ next _ instruction(self.state).tokens ↪ → self.advance(self.eos _ token _ id) assert self.guide.is _ final _ state(self.state)
```

Listing 5: Concrete test harness for Outlines

```
import guidance import llguidance class GuidanceCompiler (Compiler): def __ init __ (self, model _ id: str): self.gtokenizer = guidance.models.transformers.TransformersTokenizer(model _ id, None ) ↪ → ↪ → self.lltokenizer = llguidance.LLTokenizer(llguidance.TokenizerWrapper(self.gtokenizer)) ↪ → def compile(self, schema: str) -> GuidanceMasker: grammar = guidance.json(schema=schema) llinterpreter = llguidance.LLInterpreter( tokenizer=self.lltokenizer, llguidance _ json=json.dumps(grammar.ll _ serialize()), enable _ backtrack= False , enable _ ff _ tokens= False , ) return GuidanceMasker(llinterpreter, self.gtokenizer.eos _ token _ id) ↪ → class GuidanceMasker (Masker): def __ init __ (self, llinterpreter, eos _ token _ id): self.llinterpreter = llinterpreter self.eos _ token _ id = eos _ token _ id def advance(self, token: int): bytemask, _ = self.llinterpreter.compute _ mask() assert bytemask[token] > 0 self.llinterpreter.commit _ token(token) def assert _ done(self): if self.llinterpreter.stop _ reason() == "NotStopped": bytemask, _ = self.llinterpreter.compute _ mask() if bytemask is not None : assert bytemask[self.eos _ token _ id] > 0 self.llinterpreter.commit _ token(self.eos _ token _ id) bytemask, _ = self.llinterpreter.compute _ mask() assert bytemask is None assert self.llinterpreter.stop _ reason() in {"NoExtension", "EndOfSentence"} ↪ →
```

Listing 6: Concrete test harness for Guidance

```
import xgrammar as xgr from transformers import AutoConfig, AutoTokenizer class XGrammarCompiler (Compiler): def __ init __ (self, model _ id: str): tokenizer = AutoTokenizer.from _ pretrained(model _ id) config = AutoConfig.from _ pretrained(model _ id) self.eos _ token _ id = tokenizer.eos _ token _ id self.tokenizer _ info = xgr.TokenizerInfo.from _ huggingface( tokenizer, vocab _ size=config.vocab _ size ) self.compiler = xgr.GrammarCompiler( tokenizer _ info=self.tokenizer _ info, ) def compile(self, schema: str) -> "XGrammarMasker": compiled _ grammar = self.compiler.compile _ json _ schema(schema, strict _ mode= False ) ↪ → xgr _ matcher = xgr.GrammarMatcher(compiled _ grammar) return XGrammarMasker(xgr _ matcher, self.eos _ token _ id) class XGrammarMasker (Masker): def __ init __ (self, xgr _ matcher, eos _ token _ id): self.matcher = xgr _ matcher self.eos _ token _ id = eos _ token _ id def advance(self, token: int): assert self.matcher.accept _ token(token) def assert _ done(self): if not self.matcher.is _ terminated(): self.advance(self.eos _ token _ id) assert self.matcher.is _ terminated() Listing 7: Concrete test harness for xGrammar from llama _ cpp import LlamaGrammar import xgrammar as xgr class LlamacppCompiler (XGrammarCompiler): def compile(self, schema) -> XGrammarMasker: grammar _ bnf = LlamaGrammar.from _ json _ schema(schema). _ grammar compiled _ grammar = self.compiler.compile _ grammar(grammar _ bnf) xgr _ matcher = xgr.GrammarMatcher(compiled _ grammar) return XGrammarMasker(xgr _ matcher, self.eos _ token _ id)
```

Listing 8: Concrete test harness for Llamacpp, inheriting from the XGrammar harness for all functionality after using llamacpp to convert the schema to GGML BNF.
