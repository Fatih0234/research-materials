## 1. Introduction

Despite the impressive capabilities of Large Language Models (LLMs) and the implementation of safety alignment techniques [6], [7], LLMs remain inherently vulnerable to adversarial jailbreak attacks, where attackers attempt to bypass safety mechanisms and induce harmful outputs [8], [9].

Red-teaming has emerged as a proactive safety mechanism by exposing LLMs to adversarial prompts, uncovering

∗ Corresponding author.

Youzhi Zhang ∗

Centre for Artificial Intelligence and Robotics Hong Kong Institute of Science &amp; Innovation Chinese Academy of Sciences Hong Kong SAR, China youzhi.zhang@cair-cas.org.hk

<!-- image -->

(c) Multi-Turn with Pre-Scheduled Queries (d) Ours: Multi-Turn, Learning-Based

Figure 1. Jailbreak Attack Strategies. (a) Use a single adversarial query to bypass the target LLM's safety mechanisms, e.g., GCG [1]. (b) Iteratively refine adversarial prompts via attacker-LLM interactions, e.g., PAIR [2]. (c) Multi-turn attacks like ActorAttack [3], Crescendo [4], and 'Speak Out of Turn' [5] follow predefined patterns to decompose queries, with the attacking LLM frozen. (d) Siren (Ours) : A learning-based approach where the attacking LLM is post-trained based on the target LLM's responses.

vulnerabilities [10]-[12], and inspiring the development of corresponding defenses [13]-[16]. Prompt-based jailbreak attacks are predominantly single-turn, where attackers aim to bypass safety mechanisms within a single prompt. Methods include GCG [1], PAIR [2], and ReNeLLM [17]. However, single-turn attacks are limited in their ability to address the complexity and adaptability required for multi-turn adversarial strategies, which better reflect real-world scenarios.

Multi-turn jailbreak attacks represent a sophisticated threat, wherein malicious intent is distributed across several interactions. This approach gradually steers an ostensibly benign dialogue towards generating harmful or unintended outputs. Empirical studies involving human red-teamers have exposed significant vulnerabilities in contemporary LLM defenses, indicating their susceptibility to dynamic, multi-turn attack strategies [18]. Despite these findings, current automated multi-turn attack methodologies [3], [4] predominantly depend on static or predefined interaction patterns. The primary challenge in developing human-like red-teaming methods lies not only in decomposing queries but also in enabling the attacking LLM to dynamically adapt its attack queries during each interaction turn.

To address this challenge, we propose Siren , a learningbased multi-turn attack framework for simulating realworld human jailbreak behaviors. Unlike prior methods, Siren conceptualizes adversarial query generation as a dynamic learning task, as illustrated in Figure 1. Specifically: (1) It automates the attack process using a scalable, learning-driven methodology, thereby obviating the need for manual multi-turn red-teaming efforts. (2) Employing a MiniMax-driven sample curation strategy with turn-level LLM feedback, Siren constructs a high-quality training dataset. (3) Leveraging Supervised Fine-Tuning (SFT) [19], Siren trains the attacking LLM to implement sophisticated multi-turn strategies based on decomposed malicious intents. (4) Through the application of Direct Preference Optimization (DPO) [20], Siren refines query generation by aligning the attacking model's outputs with preferred adversarial examples. This enables dynamic adaptation to subtle variations in target model responses and query formulations, significantly enhancing overall attack efficacy.

Evaluation on the HarmfulBehaviors benchmark [1] demonstrates Siren's robust effectiveness across diverse target LLMs. Specifically, Siren achieved a 90% attack success rate (ASR) utilizing LLaMA-3-8B against Gemini-1.5-Pro and a 70% ASR employing Mistral-7B against GPT-4o, markedly outperforming single-turn baselines. Furthermore, a Siren-augmented 7B-parameter attacker model matched the performance of a multi-turn GPT-4o-based baseline, while requiring fewer interaction turns and exhibited superior semantic alignment in its generated queries.

Our findings highlight significant variations in LLM vulnerability. Among local models, LLaMA-3-8B demonstrates stronger defenses (ASR ∼ 30%) than Mistral-7B and Qwen2.5-7B (ASR ≥ 70%). For API-based models, Claude3.5 exhibits the highest resistance, while Gemini-1.5-Pro and GPT-4o are more susceptible. Furthermore, an important observation is that untrained, frozen LLM-based attackers often prove ineffective as query generators. For example, PAIR utilizing LLaMA-3 and ActorAttack employing GPT-4o both failed because the attacker LLMs refused to generate harmful prompts. This limitation arises from the robust safety mechanisms present in modern LLMs, suggesting such approaches are becoming progressively less practical. Additionally, we found that Siren could combine different attack strategies, even though its training data only contained individual strategies and no examples of combined strategy. We summarize our contributions as follows:

· Learning-Based Multi-Turn Attack Framework : We introduce Siren, a novel framework designed to simulate dynamic multi-turn jailbreak scenarios by enabling adaptive adversarial query generation.

· Advanced Learning and Optimization Strategy : Siren employs turn-level feedback, critically for its MiniMax-driven training sample construction, and employs SFT and DPO to enhance adversarial query generation.

· Extensive Experimental Validation : We conducted comprehensive experiments on three query decomposition scenarios, evaluating three base attacker LLMs against six target LLMs. Our results demonstrate that Siren's learningbased multi-turn attack strategies consistently achieve high effectiveness in generating successful adversarial queries.
