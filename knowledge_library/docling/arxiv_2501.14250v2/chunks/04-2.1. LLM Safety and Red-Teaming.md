## 2.1. LLM Safety and Red-Teaming

In spite of the impressive capabilities of LLMs, they possess inherent vulnerabilities that can be exploited to generate unsafe or otherwise undesirable content, potentially leading to significant harmful consequences. Such undesirable outputs often contravene established usage policies. For instance, OpenAI's usage policy [21] enumerates thirteen forbidden categories of harmful outcomes. These encompass, but are not limited to, the promotion of illegal activities, hate speech, malware generation, and incitement to physical harm. Addressing the propensity of LLMs to produce outputs falling into these prohibited categories remains a critical challenge. Consequently, safety alignment methods [6], [7], [22] are extensively developed and employed to ensure that LLM behaviors and outputs remain consistent with human values and ethical standards.

Jailbreaking refers to a class of adversarial attacks designed to exploit vulnerabilities in LLMs, thereby circumventing their inherent safety mechanisms and eliciting undesirable outputs [10]-[12]. To mitigate the risks posed by such adversarial attacks, red-teaming has emerged as a crucial proactive safety methodology [23]-[26]. This process entails systematically exposing LLMs to carefully crafted adversarial prompts with the objective of uncovering latent vulnerabilities. The insights derived from these activities are pivotal, as they directly inform the iterative refinement of existing alignment methods. Moreover, red-teaming is instrumental in fostering the development of robust defenses [13]-[16] against novel and evolving adversarial strategies.
