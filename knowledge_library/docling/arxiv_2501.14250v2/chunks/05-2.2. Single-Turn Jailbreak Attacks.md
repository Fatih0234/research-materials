## 2.2. Single-Turn Jailbreak Attacks

GCG [1] adopts a discrete optimization strategy, integrating greedy and gradient-based techniques to craft adversarial suffixes. However, the efficacy of GCG can be diminished by perplexity-based detection mechanisms [27]. AutoDAN [28] employs a hierarchical genetic algorithm for generating semantically coherent jailbreak prompts, thereby enhancing attack stealth and scalability relative to manual approaches. While AutoDAN exhibits robust performance against perplexity-based defenses, its considerable computational complexity poses limitations for real-time attack scenarios. Other notable approaches include ReNeLLM [17], which constructs nested prompts through techniques such as scenario nesting and prompt rewriting, and In-Context Attack (ICA) [29], which uses harmful in-context examples.

Certain single-turn jailbreaking methods employ iterative strategies to optimize their adversarial prompts. For instance, PAIR [2] utilizes an attacking LLM to repeatedly query the target LLM, progressively refining the candidate jailbreak prompt with each interaction. Similarly, MART [24] incorporates an iterative refinement process; notably, it also extends its application by training target LLMs with red-teaming prompts to enhance their robustness. DAP [30] also leverages an automated red-teaming approach, characterized by its integration of techniques such as malicious content obfuscation and memory reframing. However, as LLMs become increasingly sophisticated and their safety alignments more robust, a pertinent challenge arises: the auxiliary LLMs employed within these frameworks may increasingly refuse to generate or refine malicious inputs, potentially diminishing their utility as effective attackers.
