## 3.3. Overview of the Siren Framework

The Siren framework, depicted in Figure 2, orchestrates multi-turn jailbreak attacks through a three-stage process.

Stage 1: MiniMax-Driven Adversarial Data Curation via Turn-Level Feedback. This foundational stage is dedicated to the systematic curation of a high-quality training dataset, employing a MiniMax-driven strategy to generate and select effective training instances. For each turn i within a simulated adversarial interaction, a data generation agent utilizes the specified adversarial goal x goal and the historical interaction context-comprising queryresponse pairs ( x (1) , r (1) , . . . , x ( i -1) , r ( i -1) ) as input. The output is the attack query x ( i ) . In this data generation phase, an advanced LLM (e.g., GPT-4o) serves as a representative target model. The crucial turn-level responses elicited from

Figure 2. Overview of the Siren Framework. (1) Training Set Construction : Turn-level feedback from a sophisticated LLM (e.g., GPT-4o) is used to generate training samples. At each turn i , the adversarial goal x goal and the historical interaction context ( x (1) , r (1) , . . . , x ( i -1) , r ( i -1) ) are the inputs, and the output of the attacking LLM is the attack query x ( i ) . (2) Post-Training : The attacking LLM is fine-tuned through SFT and DPO, with negative outputs generated by a relatively weak LLM (e.g., Baichuan-13B). (3) Interaction Phase : The attacking LLM interacts with the target LLM over multiple turns, dynamically generating queries with a maximum of 4 turns, typically up to 3 turns. Through iterations over time, new interaction records can be further utilized to fine-tune the attacking LLM for red-teaming purposes. The training set is constructed using the DAN FobiddenQuestions dataset, and the interactions are applied to the HarmfulBehaviors benchmark, ensuring no dataset overlap through a deduplication process.

<!-- image -->

this target model provide the feedback necessary to guide the MiniMax selection process. This ensures that the curated training samples are those deemed most potent or informative for subsequently training the attacking LLM. A detailed exposition of this methodology is presented in Section 3.4.

Stage 2: Attacker LLM Post-Training and Refinement. Following the dataset construction in Stage 1, the attacking LLM undergoes a two-phase post-training regimen involving SFT and DPO. Initially, SFT is employed to imbue the attacker LLM with foundational query generation strategies requisite for multi-turn attacks. Subsequently, DPO [20] is applied to further refine the model's outputs through preference-based learning. The preference pairs for DPO are constructed by contrasting desired outputs with negative examples. These negative instances are generated using a less capable LLM, such as Baichuan-13B [31]. This model's limited instruction-following capabilities often yield outputs like non-English text, improper formatting, or ineffective attack queries. Such outputs serve as the negative samples.

Stage 3: Dynamic Adversarial Interaction with Target LLM. This concluding stage involves the operational deployment of the fine-tuned attacking LLM against the designated target LLM in a multi-turn adversarial dialogue. During the inference phase, at each interaction turn i , the attacking LLM forms an adversarial query x ( i ) , conditioned on the overarching attack objective and the accumulated interaction history. This query is then presented to the target LLM. The ensuing response r ( i ) from the target LLM is meticulously recorded and integrated into the interaction history. This updated history is then utilized by the attacking LLM to inform the generation of the query x ( i +1) for the subsequent turn, facilitating dynamic adaptation of the attack strategy to the target's real-time behaviors. Interactions are typically conducted for a maximum of three turns; however, a conditional fourth turn may be initiated if the adversarial objective is not achieved within the initial responses.
