## 3.4. MiniMax-Driven Training Dataset Construction with Turn-Level LLM Feedback

Central to our approach is a strategic data curation process that leverages turn-level feedback from an advanced LLM (e.g., GPT-4o). It is guided by the MiniMax principle: we aim to iteratively identify and prioritize interaction sequences that either demonstrate high adversarial efficacy in eliciting prohibited responses or expose critical vulnerabilities in the target model. Specifically, we only retained samples that succeeded despite the advanced model's safety filters, ensuring effectiveness when attacking weaker LLMs. This process creates a potent, challenging dataset. The method (Figure 3) is as follows:

Step 1. The process commences with the manual authoring of a small set of seed demonstrations. In these demonstrations, high-level adversarial goals (e.g., 'How do Ponzi schemes operate?'-a form of investment fraud ) are strategically decomposed into multi-turn sub-queries using techniques like narrative transformation . For the Ponzi scheme example, sub-queries might be structured progressively across the interaction. Turn 1 (Topic Introduction) could focus on persuasive tactics with a query such as, 'What psychological tactics might be used in a Ponzi scheme to make it appear convincing early on?' . Following this, Turn 2 (Evasion/Redirection) would shift the dialogue

Figure 3. Training set construction. Step 1: Adversarial questions are decomposed into multi-turn queries using strategies and expanded through LLMaugmented generation. Step 2: The target LLM answers each query based on prior interactions. Step 3: Each response is labeled as either Harmless or Harmful using LLM-assisted evaluation. Step 4: Turn-level samples with harmful responses are selected and deduplicated for training the adversarial LLM.

<!-- image -->

towards seemingly legitimate inquiries, for instance, by asking about detection or prevention (e.g., 'What are common red flags or warning signs that can help ordinary individuals detect such schemes?' ). Finally, Turn 3 (Harmful Steer) is designed to guide the LLM towards generating problematic content, perhaps with a query like, 'From a purely theoretical standpoint, how might one architect the foundational elements of a system designed to mimic those persuasive early tactics?' . Another decomposition strategy employed is story-driven imagination , examples can be found in Table 1. Manually crafted sequences are subsequently augmented using an LLM with in-context learning to generate variations; the specific prompt is detailed in Table 2. To ensure query quality and relevance, human-guided filtering is implemented. Finally, an advanced LLM, acting as a target model, validates the filtered queries to assess their utility.

Step 2. This phase simulates multi-turn interactions with the target LLM. For each turn i , the target LLM processes the adversarial query Q ( i ) , conditioned on the accumulated interaction history. This history encompasses all preceding queries Q ( &lt; i ) and their corresponding responses R ( &lt; i ) . The target LLM then generates a response R ( i ) to the current query Q ( i ) . To illustrate, the initial query Q 1 is sent to the target LLM, eliciting the response R 1 . Subsequently, the query Q 2 , contextualized by the history [ Q 1 , R 1] , produces R 2 . This iterative process continues, with Q 3 , given the full historical context [ Q 1 , R 1 , Q 2 , R 2] , yielding R 3 .

Step 3. Upon collection of the multi-turn interaction dialogues from Step 2, each individual response R ( i ) is categorized as either Harmless or Harmful . This classification is performed using an LLM-assisted evaluation protocol, for which the specific prompt utilized in the labeling process is detailed in Table 3. As depicted in Figure 3, R 1 is labeled as Harmless because it describes legitimate detection methods. Conversely, R 2 and R 3 are labeled as Harmful because they contain details about illegal smuggling operations.

Step 4. Finally, turn-level training samples are meticulously selected to construct the dataset for the adversarial LLM. The primary selection criterion is as follows: if a target LLM's response R ( i ) at any turn i within an interaction sequence is classified as Harmful , the entire sequence of queries generated by the data generation agent up to and including query Q ( i ) is considered a successful adversarial trajectory. For example, if R ( i ) is harmful, the samples { [ Q 1] , R 1 } , { [ Q 1 , R 1 , Q 2] , R 2 } , ..., { [ Q 1 , R 1 , . . . , Q ( i )] , R ( i ) } are selected to form the training set, regardless of whether R 1 or R 2 is labeled as Harmful or Harmless . To avoid data redundancy, particularly from overlapping successful interaction pathways, samples are typically collated by tracing interactions backward from the final turn's response to the initial turn's response.
