## 5.3. Ablation Study

To rigorously evaluate the contributions of individual components within the Siren framework, we conducted a comprehensive ablation study. For these experiments, Mistral-7B was employed as the default attacking LLM, and variants of Siren were trained under differing configurations. The evaluation was performed on the initial 50 samples extracted from the HarmfulBehaviors benchmark.

The impact of Siren's post-training phase, which encompasses both SFT and DPO, is detailed in Table 10 across three distinct query decomposition scenarios ( Decop 1 , Decop 2 , and Combined ). Our findings indicate that the omission of this post-training stage (denoted 'w/o Posttraining') results in a substantial degradation of attack efficacy. This performance decline is particularly acute when targeting API-based LLMs such as GPT-4o and Claude3.5. For instance, under the Decop 2 scenario, the ASR

against Claude-3.5 diminished drastically from 28% to 2% when post-training was excluded. Furthermore, this study underscores the synergistic benefit of combining SFT and DPO, which collectively yield the highest ASR. In contrast, the application of SFT or DPO in isolation leads to discernibly moderated performance. Illustratively, in the Combined scenario, the complete Siren framework achieved an 86% ASR against both Mistral-7B and Qwen2.5-7B. This stands in stark contrast to the ASRs observed with only SFT ('w/ SFT'), which were 68% for Mistral-7B and 58% for Qwen2.5-7B, and those with only DPO ('w/ DPO'), recorded at 52% for Mistral-7B and 48% for Qwen2.5-7B.
