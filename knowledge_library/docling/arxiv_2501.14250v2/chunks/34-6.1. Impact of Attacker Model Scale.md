## 6.1. Impact of Attacker Model Scale

The scalability of attacker LLMs is a critical consideration for their practical application, directly influencing both attack efficacy and the requisite computational resources. To explore this dimension, we analyzed the performance and resource consumption of our Qwen2.5 series models, which were post-trained using both SFT and DPO. These models, ranging from 0.5B to 7B parameters, were evaluated as attackers employing the 'combined decomposition' strategy against LLaMA-3. The findings, including ASR and GPU memory usage (when models are loaded in BF16 precision), are presented in Table 13.

Our analysis reveals nuanced trends related to model scale. For instance, the KW-ASR metric shows strong performance for the Qwen-1.5B model (46.0%) and peaks at 49.3% with the Qwen-3B model, while the larger Qwen7B model achieves a slightly lower KW-ASR of 48.8%.

TABLE 13. ASR AND COMPUTATIONAL COST ANALYSIS.

| Metric           | Qwen-0.5B   | Qwen-1.5B   | Qwen-3B   | Qwen-7B   |
|------------------|-------------|-------------|-----------|-----------|
| KW-ASR           | 43.5%       | 46.0%       | 49.3%     | 48.8%     |
| GPT-ASR          | 26.5%       | 28.1%       | 29.7%     | 30.0%     |
| GPU Memory (MiB) | 1354        | 3494        | 6514      | 15060     |

Conversely, the GPT-ASR metric demonstrates a more consistent, albeit modest, positive correlation with increasing model size, rising from 26.5% for the Qwen-0.5B model to its highest observed value of 30.0% for the Qwen-7B variant. Concurrently, the computational cost, specifically GPU memory footprint, escalates substantially with model size. As indicated in Table 13, memory requirements increase from 1354 MiB for the Qwen-0.5B model to 15060 MiB for the Qwen-7B model. This highlights an important trade-off: while larger models, such as Qwen-7B, may offer marginal improvements or optimal performance on specific ASR metrics (like GPT-ASR in this case), they impose significantly greater computational demands. The Qwen-3B model, for this particular attack scenario and target, appears to present a compelling balance, achieving the highest KWASR with a more moderate memory overhead compared to its 7B counterpart. These observations underscore the necessity of carefully selecting an attacker LLM's scale based on the specific adversarial objectives, the nature of the target system, and the available computational budget.
