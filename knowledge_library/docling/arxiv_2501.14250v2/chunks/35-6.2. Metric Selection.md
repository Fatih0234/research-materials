## 6.2. Metric Selection

The accurate assessment of attack success is crucial for understanding LLM vulnerabilities, yet its reliability depends critically on the chosen adjudicator. We conducted a human evaluation on a subset of generated responses, comparing these ground-truth labels against judgments from two potential automated adjudicators: GPT-4o and LLaMAGuard. Our comparison revealed that GPT-4o's judgments achieve a high degree of alignment with human annotators. Conversely, and consistent with prior work [3], we found that LLaMA-Guard frequently misclassifies unsafe outputs as safe. This high rate of false negatives significantly compromises the evaluation's integrity by substantially underestimating true attack efficacy. As Table 14 illustrates, this leads to a stark disparity in the measured ASR. For instance, when targeting LLaMA-3 with the same set of attack outputs, the ASR adjudicated by GPT-4o is 29.8%, whereas LLaMA-Guard reports a mere 3.7%.

TABLE 14. ASR EVALUATED BY DIFFERENT ADJUDICATORS

| Adjudicator   | LLaMA-3   | Mistral   | Qwen   | GPT-4o   | Claude   | Gemini   |
|---------------|-----------|-----------|--------|----------|----------|----------|
| GPT           | 29.8%     | 89.8%     | 81.9%  | 70%      | 24%      | 88%      |
| LLaMAGuard    | 3.7%      | 18.7%     | 19.2%  | 4%       | 2%       | 2%       |
