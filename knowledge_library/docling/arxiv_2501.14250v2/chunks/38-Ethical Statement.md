## Ethical Statement

Our research aims to simulate real-world human jailbreak attacks to identify vulnerabilities in LLMs and support the AI community in addressing jailbreak attacks. The ultimate goal of this work is to advance the safety and robustness of LLMs in real-world scenarios. To prevent misuse of our method, we have taken the following precautions:

- Controlled Public Access to Code and Data: (1) All experiments were conducted in a controlled environment. (2) Only a limited portion of attack interaction samples will be publicly available. (3) While some trained adapters are publicly accessible, access to more sensitive or the complete set of adapters requires an email request.
- Responsible Presentation of Content: (1) We present only partial content of attack queries and responses, using placeholders such as '. . . ' to ensure the methodology is understandable while not showing all the words of attack queries or responses. (2) A content warning is included in the abstract, following standard practice in similar works.

These measures demonstrate our commitment to conducting responsible research while contributing to the development of a red-team simulator for future defenses against advanced jailbreak attacks. We believe the positive contributions of our work outweigh its potential negative impacts.
