## 1 Introduction

The increasing deployment of autonomous agents based on Large Language Models (LLMs) [Wang et al. , 2024] in realworld applications necessitates an examination of their collective impact on machine-machine interactions and human culture [Brinkmann et al. , 2023]. Whilst individual LLM capabilities are frequently assessed, understanding their collective behaviours and societal consequences remains crucial and underexplored.

The development of social capabilities in these agents may lead to dual-use skills usable for both pro-social and antisocial purposes, termed differential capabilities . [Dafoe et al. , 2020]. This duality raises questions about the balance between cooperation and conflict in autonomous agent interactions. Furthermore, situations such as social dilemmas pose inherent risks, as competent agents acting rationally can lead to suboptimal collective outcomes [Pan et al. , 2023 07 232023 07 29]. If agents succeed through aggressive behaviours, competitive pressures could potentially drive systems towards suboptimal equilibria [Anwar et al. , 2024].

Prior assessments of LLMs have evaluated their capacity to engage in various multiplayer games [Mao et al. , 2023; Yocum et al. , 2023; Park et al. , 2023; Gong et al. , 2023; Zhang et al. , 2024; Wu et al. , 2023] and the emergent behaviours of systems of LLM agents has been explored. Conventionally, however, LLMs are prompted to output a single action in response to a given game state or trajectory. Recent analyses have revealed that LLMs struggle when tasked with making decisions at this level of granularity [Fan et al. , 2024]. In such scenarios, they fail to identify basic patterns, such as an opponent mirroring their own moves. This limitation likely stems from the fact that LLMs are not specifically trained for data science tasks, or to handle inputs of this format.

In response, in contrast to prior work, our we prompt LLMs to create fixed strategies in natural language, which are subsequently implemented as algorithms in Python. This method enables the LLMs to craft their approach at a higher level of abstraction. For example, with our approach, we observe that many LLM strategies utilise pattern recognition and successfully implement code to detect simple patterns up to a fixed length. A key advantage of creating strategies to encode as algorithms, rather than outputting individual actions, is that it facilitates behaviour checking in advance. This approach allows users to inspect the strategy, test for safety and robustness, and explore the potential implications prior to deployment.

Our research employs the iterated Prisoner's Dilemma (IPD) [Axelrod, 1980; Crandall, 2014; Beaufils et al. , 1996] to evaluate the balance between pro-social and anti-social behaviours exhibited by state-of-the-art LLM agents. We utilise evolutionary game theory [Axelrod, 1986; Mahmoud et al. , 2010; Nowak et al. , 2004; Nowak, 2006] to investigate whether systems of frontier LLM agents are predisposed to exhibit cooperation or conflict under competitive pressures. The choice of IPD provides a robust mathematical framework for analysing the strategic behaviour and cooperative biases of LLM agents. Moreover, as game theory represents a highlevel abstraction of various social phenomena, with applications spanning economics, politics, sociology, and psychology, insights gained from LLM performance in these scenarios may have far-reaching implications across multiple disciplines.

Our contributions are as follows: we quantify the relative success of pro-social and anti-social LLM agent behaviours in a social dilemma; we assess the relative likelihoods of systems of LLM agents converging to anti-social or pro-social equilibria; and, we release our code 1 as an evaluation suite for model developers to assess the emergent behaviour of their products.

In addition to the above, we provide supplementary analysis: we verify that the LLM strategies exhibit the requested behaviours; we benchmark the LLM strategy performance against human written strategies using the same setup as prior work [Beaufils et al. , 1996]; and, we investigate the impact of noisy actions, which represent execution mistakes.
