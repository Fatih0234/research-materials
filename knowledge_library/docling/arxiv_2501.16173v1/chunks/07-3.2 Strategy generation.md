## 3.2 Strategy generation

Weemploy LLMs to create natural language strategies, which are subsequently coded into algorithms that output either cooperate or defect, given the game history. When prompted to create a strategy, the LLMs are provided with specific behaviours to exhibit, which we term their attitude , from the following set:

<!-- formula-not-decoded -->

Recognising that different prompting techniques can yield varying performance [Madaan et al. , 2023; Moghaddam and Honey, 2023; Shinn et al. , 2023; Fernando et al. , 2023; Khot et al. , 2023; Wei et al. , 2022 28 November 9 December;

Table 2: Prompt styles

| Name    | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
|---------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Default | The LLM is provided with information about the game and prompted to create a strategy exhibit- ing the desired attitude in natural language.                                                                                                                                                                                                                                                                                                                              |
| Refine  | The LLM is initially prompted with the Default prompt above. We then use Self-Refine [Madaan et al. , 2023] to ask the LLM to provide and in- corporate self-feedback as follows: (i) the LLM is prompted to provide a list of critiques of the strategy, before ii) tasking the LLM with rewrit- ing the strategy taking into account the critique.                                                                                                                      |
| Prose   | The Prose prompt samples a scenario description with the same dynamics of Prisoner's Dilemma from a set of four, such as a diplomatic nego- tiation around trade protocols, while avoiding the use of game theoretic language. The LLM is provided with the scenario and prompted to create a high-level strategy. The LLM is sub- sequently provided with information about the game, and tasked with converting the high-level strategy into one suitable for the game. |

Wang et al. , 2023], we experiment with different techniques. Our approach aims not to be definitive, but rather to explore a range of prompting styles to illustrate a range of possible results and understand output variability. We experiment with three different prompt styles, as described in Table 2.

Weselect the LLMs ChatGPT-4o and Claude 3.5 Sonnet as they are popular frontier models. For each LLM and prompt style, we create 25 strategies for the three attitudes. We then use ChatGPT-4o to convert the natural language strategies for all LLMs into Python functions. Since we are not assessing the coding abilities of the LLMs, we use the same model to code the algorithms to maintain consistency.

This fixed set of algorithms is assessed for operational safety, as executing arbitrary code is generally unsafe. This is why we create a fixed set for the following experiments, rather than generating new strategies on the fly. Where an algorithm has a bug, we manually fix this if the model's intention is clear. Otherwise, we delete the strategy and generate a new one. Full details of the prompts and generated strategies created can be found in our GitHub at https: //github.com/willis-richard/evollm.

Qualitatively, we observe that the strategies produced by all three prompts for both models are game theoretic in nature. Even with the Prose prompt, which obfuscates the task in an attempt to elicit different reasoning process, the models appear to recognise that the structure of the situation means it is appropriate to apply game theoretic strategies from their knowledge base. This suggests generative agents will reason about scenarios by recognising that game theory inspired strategies in real world scenarios.

We observe differences in the strategies generated by different LLMs. Claude 3.5 Sonnet frequently compares its running total payoff to that of its opponent as part of its decisionmaking process, whereas ChatGPT-4o does not. When craft- ing the set of obfuscated prose prompts, we initially had a scenario dealing with scientific collaboration between academic researchers with the option to either hide or share findings with a colleague. Claude 3.5 Sonnet would frequently refuse to write an aggressive strategy in such a situation. Consequently, we modified the scenario to describe a similar situation, but using commercial engineering rather than academic science, resolving the issue.
