## 4.1 Validation

To quantify whether the strategies faithfully exhibit their assigned attitudes, we conduct an IPD tournament (Section 3.1) involving all 75 strategies (25 of each attitude). We then compute the average number of cooperations over all rounds in all matches for strategies of each attitude against strategies of another attitude. The results for the default prompt without noise are shown in Table 3. We show the normalised propensity to cooperate: the proportion of actions in a tournament that the strategies cooperate.

For both LLMs, the neutral and cooperative attitudes exhibit similar behaviour, mutually cooperating in almost all rounds when paired against themselves. Qualitatively, we observe that both neutral and cooperative strategies tend to initiate cooperation in the first round and then broadly follow a Tit-For-Tat approach, which sustains cooperation. The aggressive strategies, however, behave markedly differently, typically initiating with defection. For ChatGPT-4o (Table 3a), aggressive strategies consistently cooperate the least

Figure 2: Performance compared to human-written algorithms

<!-- image -->

across all match-ups, demonstrating their aggression. For Claude 3.5 Sonnet (Table 3a), aggressive strategies similarly defect the most when paired with cooperative or neutral strategies. Interestingly, they exhibit the highest cooperation rate when paired against other aggressive strategies, suggesting a greater willingness to attempt mutual cooperation when encountering an aggressive opponent. For instance, some strategies detect if multiple consecutive rounds of mutual defection have occurred, and will attempt cooperation afterwards.

Overall, the strategies demonstrate reactivity to their opponents' play, modifying their actions in response. The most pronounced exploitation we observe is from the ChatGPT-4o aggressive strategies, which cooperate 12% less than their opponents on average when paired with neutral strategies.

To identify which attitudes the LLMs are better at generating strategies that are robust to a range of behaviours, and which struggle, we enter them into an IPD tournament against human-written algorithms. Unlike the previous all-play-all tournaments using individual strategies, this analysis employs

| Prompt   |                | Aggressive     | Cooperative    | Neutral   |
|----------|----------------|----------------|----------------|-----------|
| Default  | Aggressive     | 1.81           | 2.09           | 2.26      |
|          | Cooperative    | 1.55           | 3.00           | 2.99      |
|          | Neutral        | 1.55           | 2.99           | 2.99      |
| Refine   | Aggressive     | 2.20           | 2.57           | 2.63      |
|          | Cooperative    | 2.53           | 2.99           | 2.99      |
|          | Neutral        | 2.55           | 2.97           | 2.97      |
| Prose    | Aggressive     | 1.65           | 2.29           | 2.35      |
|          | Cooperative    | 2.08           | 2.82           | 2.89      |
|          | Neutral        | 2.12           | 2.89           | 2.93      |
|          | (a) ChatGPT-4o | (a) ChatGPT-4o | (a) ChatGPT-4o |           |
| Prompt   |                | Aggressive     | Cooperative    | Neutral   |
| Default  | Aggressive     | 1.56           | 1.42           | 1.44      |
|          | Cooperative    | 1.41           | 2.99           | 2.98      |
|          | Neutral        | 1.47           | 2.98           | 2.98      |
| Refine   | Aggressive     | 1.87           | 2.18           | 2.04      |
|          | Cooperative    | 2.10           | 2.86           | 2.67      |
|          | Neutral        | 2.01           | 2.69           | 2.50      |
| Prose    | Aggressive     | 1.64           | 2.24           | 2.19      |
|          | Cooperative    | 2.02           | 2.64           | 2.65      |
|          | Neutral        | 2.00           | 2.64           | 2.63      |

(b) Claude 3.5 Sonnet

Table 4: Normalised head-to-head payoffs

the attitude-agents (Section 3.3). For both LLMs, the three attitude-agents are entered into the tournament as described by Beaufils [Beaufils et al. , 1996], competing against 11 standard human-written algorithms. These include Tit-For-Tat, which initially cooperates and then mirrors its opponent's previous action, and Random, which arbitrarily chooses between cooperation and defection in each round. We repeat the tournament 200 times using different seeds.

Figure 2 illustrates the performance of the three attitudeagents in the Beaufils tournament. Each plot displays the median of the tournament scores (the mean round payoff in a single tournament) for each strategy, and a violin depicting the distribution of tournament scores across all repetitions. For both LLMs, the neutral and cooperative strategies perform well, whilst the aggressive strategies perform poorly. This does not necessarily indicate that aggressive strategies are inherently flawed; they may perform better against a different set of opponents. However, it suggests that the LLMs are more adept at crafting cooperative approaches.

The three attitude-agents typically exhibit a larger spread of payoffs in comparison to many of the human-written algorithms. This increased variability stems from the fact that each attitude-agent samples a strategy from its corresponding attitude-strategy set before each match, introducing an additional layer of variation absent in the fixed human-written algorithms. Within each attitude-strategy set, the performance of individual algorithms can vary considerably, with some proving significantly more effective than others.
