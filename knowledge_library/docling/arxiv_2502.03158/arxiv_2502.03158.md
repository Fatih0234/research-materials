## Strategizing with AI:

## Insights from a Beauty Contest Experiment

Iuliia Alekseenko 1 , Dmitry Dagaev 1,3,* , Sofia Paklina 2 , and Petr Parshakov 2,4

1 HSE University, Laboratory of Sports Studies

2

Corresponding author. Email: ddagaev@nes.ru. Address: 121205, Russia, Moscow, Innovation Centre Skolkovo, Nobel street, 3.

HSE University, International Laboratory of Intangible-Driven Economy 3 New Economic School 4 SKOLKOVO School of Management *

## Abstract

A ğ‘ -beauty contest is a wide class of games of guessing the most popular strategy among other players. In particular, guessing a fraction of a mean of numbers chosen by all players is a classic behavioral experiment designed to test iterative reasoning patterns among various groups of people. The previous literature reveals that the level of sophistication of the opponents is an important factor affecting the outcome of the game. Smarter decision makers choose strategies that are closer to theoretical Nash equilibrium and demonstrate faster convergence to equilibrium in iterated contests with information revelation. We replicate a series of classic experiments by running virtual experiments with large language models (LLMs) who play against various groups of virtual players. Our results show that LLMs recognize strategic context of the game and demonstrate expected adaptability to the changing set of parameters. LLMs systematically behave in a more sophisticated way compared to the participants of the original experiments. All LLMs still fail to identify dominant strategies in a two-player game. Our results contribute to the discussion on the accuracy of modeling human economic agents by artificial intelligence.

Keywords : JEL codes : C99, D90, C72

ğ‘ -beauty contest, guess the number, dominant strategy, AI, LLM

## 1 Introduction

The recent appearance of large language models (LLMs) has resulted in numerous attempts to substitute humans with generative agents in various settings (see, for example, Park et al. (2023)). The motivation is simple: in many economic activities, especially those involving codifiable, highvolume, or knowledge-intensive tasks, LLMs are increasingly cost-competitive and productivityenhancing compared to human labor (Korinek, 2023, Merali, 2024, Kanazawa et al., 2025), and such automation can reduce employment (Acemoglu &amp; Restrepo, 2018). Still, at this point, it is not clear to what extent LLM can simulate human behavior. The revelation of differences between the decisions of the participants of economic experiments and LLMs' decisions in similar circumstances becomes an important challenge. Horton (2023) replicates several classic experiments with LLM players and advocates the use of LLMs as models for ordinary economic agents. Akata et al. (2023) reveal that LLMs underperform in the iterated games that require coordination such as the iterated Battle of Sexes. Brookins &amp; DeBacker (2023) show that LLMs prefer fair decisions in a Dictator game, even more than human players do. Goli &amp; Singh (2024) study the intertemporal preferences of LLMs and figure out that artificial players are less patient than humans. In general, different LLMs have different peculiarities: some are very sensitive to game structure, others have issues with context framing (Lor` e &amp; Heydari, 2024).

Our paper contributes to this strand of literature by studying the behavior of LLMs in the classic Guess the number game which belongs to a wider class of ğ‘ -beauty contest games. These ğ‘ -beauty contest games are of particular importance because they arise in various industries where the profit of a firm or the payoff of an individual player depend on the median preferences of all economic agents. The behavior of short-term traders and the resulting asset pricing is regarded as a variant of a beauty contest game (Allen et al., 2006, Cespa &amp; Vives, 2015). On the prediction markets, the desire to guess the most popular opinion is often considered as a behavioral driving force in addition to guessing the fundamental probabilities of an outcome (Marinovic et al., 2011). The beauty contest auction is an important alternative to price-only mechanisms of allocating goods to the bidders (Yoganarasimhan, 2016). Whereas algorithmic trading and betting strategies

written by humans have long been used on the markets, the appearance of strategies generated by LLMs is a rather recent phenomenon. Investigation of the LLMs' performance in a ğ‘ -beauty contest game (both absolute and relative to humans) would help to predict their performance in the above-mentioned markets.

The Guess the Number game tests the ability of a player to make a sequence of conclusions, and the outcome depends on their level of sophistication. The literature provides several important observations about the performance of heterogeneous players in various strategic settings. First, and most obvious, more advanced players reach better outcomes. The latter observation is especially notable in antagonistic pure games of skill. Charness (1981) revealed that the objective quality of moves selected by chess players increases with the player's skill. Levitt et al. (2011) considered a sequential game Race to 100 which is a pure game of skill. They conclude that the ability to perform backward induction leads to better results in the Race to 100 game. Second (and this is less obvious), the players pay attention to the opponents' quality. In the Eichberger et al. (2008) experimental study, most of the participants (72%) who play the simultaneous games of strategic complements or strategic substitutes feel that they can predict the actions of a game theorist better than the actions of a granny. The same large majority (72%) prefer to play against a game theorist rather than against a granny. In the Prisoner's Dilemma game, the high-ability players who learn that their partner is a low-ability one, decrease the level of cooperation (Lambrecht et al., 2024). In antagonistic games, more advanced human players demonstrate greater adaptability to competing environment. For example, in the centipede game, stronger chess players exploit the non-equilibrium play of weaker players (Palacios-Huerta &amp; Volij, 2009). However, Levitt et al. (2011) show that the ability to perform backward induction is not related to ability to stop early in the centipede game and assert that, instead of a centipede game, the pure games of skill should be used to test the role of the level of sophistication. We conclude that in many strategic environments, the skill of the players affects the outcome by allowing them to choose better strategies and/or to show greater adaptability to the opponents. To what extent do LLMs behave like a human? To answer this question, we replicate a series of well-known experiments with human participants playing the Guess the Number game by asking LLMs to play against the

same groups of competitors.

The rules of the Guess the number game are as follows. A group of ğ‘› players simultaneously and independently choose a number between 0 and 100 . Denote by ğ‘š the mean of all strategies played. A player whose number is the closest to ğ‘ğ‘š , where ğ‘ &gt; 0 is the predetermined constant known to all players before the game, wins. In case of a tie, all tied players get the corresponding share of the prize. When all real numbers from [0 , 100] are allowed, for ğ‘ âˆˆ (0 , 1) there is a unique Nash equilibrium in the model where all players choose 0 . In a particular case of ğ‘› = 2 , choosing 0 is a weakly dominant strategy. If only integer numbers are allowed, there do exist additional Nash equilibria where the players play higher numbers. For example, if ğ‘ = 0 . 5 , all strategy profiles where most of the players play 1 and other players play 0 , are additional Nash equilibria. If ğ‘ = 2 / 3 , the profile (1 , . . . , 1) is the only additional Nash equilibrium.

Multiple experiments show that in the Guess the number game people, in general, do not play Nash equilibrium. In the pioneering experimental paper, Nagel (1995) demonstrated that in sessions with ğ‘ = 1 / 2 and ğ‘ = 2 / 3 no subject chose 0 and only 6 percent chose numbers below 10 . However, in the iterated game the strategies converged to Nash equilibrium from period to period, after the participants learned statistics from the previous rounds (Nagel, 1995). If one uses the median of the chosen numbers instead of the mean, results do not change much in a one-shot game but in the iterated game convergence to 0 is faster in the median variant compared to the mean variant (Duffy &amp; Nagel, 1997). Switching to the maximum instead of the mean or the median increases the chosen strategies (Duffy &amp; Nagel, 1997).

In a particular case of ğ‘› = 2 , one could possibly anticipate a significant share of players choosing 0 , a weakly dominant strategy. However, this is not the case. Only 10% of undergraduate students and 37% of the audience of economics or psychology decision-making conferences chose 0 (Grosskopf &amp; Nagel, 2008). Also, the mean of the numbers chosen by the professionals (22) is lower than the mean of the numbers chosen by the students (36). A higher number of participants ğ‘› = 18 leads to a lower mean both for professionals (19) and students (29). However, in the case of professionals, this difference is not statistically significant (Grosskopf &amp; Nagel, 2008). Rydval et al. (2009) strengthen this finding by identifying that only nearly 1/3 of all participants think

in terms of dominant strategies in 2- or 3-person Guess the number -like games with dominant strategies, while 2/3 of all participants fail to identify the strategic properties of the game. In our study, we ask whether LLMs are able to identify the strategic nature of the game.

Several theoretical models explaining the non-equilibrium behavior were proposed in the literature. Most of these models deal with the notion of bounded rationality when players are rational only to some extent; the degree of rationality is associated with the sophistication of a player. A dynamical model where the players choose one of the stepğ‘˜ behavioral rules, learn the results of the experiment, and choose more successful rules in the next iterations, was presented and estimated in Stahl (1996). A further extension of the set of possible behavioral strategies is discussed in Stahl (1998). Ho et al. (1998) builds the bounded rationality models based on the iterative deletion of dominated strategies and iterated best replies to previously played actions. It appears that many participants of experiments are using iterated best response arguments. Namely, Bosch et al. (2002) describes an experiment organized by The Financial Times where 64% of players indeed explained that they exploited the best responses to the revealed statistics. Note that playing iterative best response to the previous iteration of the game does not lead to the best response to other players' strategies in the new iteration (Breitmoser, 2012). Weber (2003) demonstrated that the feedback from organizers plays a key role in the speed of convergence to Nash equilibrium: in the absence of the feedback, the numbers also decreased but at a lower rate. Advice from a peer participant has an even stronger effect on the performance than pure statistics provided by the organizers (Kocher et al., 2014). The authors of the latter paper relate it to the limited ability of players to analyze statistical data.

Cognitive ability is also an important determinant of the outcome of the Guess the number game. Higher cognitive ability may manifest itself through better inductive reasoning, iterative dominance, and levelğ‘˜ thinking (Carpenter et al., 2013). Brocas &amp; Carrillo (2020) designed a variant of the Guess the number game and demonstrated that the equilibrium behavior increases significantly between 5 and 10 years of age (from 17.9% to 61.4%) and stabilizes afterward. Back to the classic variant of the game, players with higher scores in a cognitive ability test choose lower numbers (Burnham et al., 2009) and show faster convergence to equilibrium in iterated experiments

(Gill &amp; Prowse, 2016). Mixed evidence was reported in BraËœ nas-Garza et al. (2012): the better performance in the CRT test that measures cognitive reflectiveness is associated with lower numbers in the Guess the number game, whereas the outcome of the Raven test measuring visual reasoning and analytic intelligence surprisingly was not associated with the successful performance in the Guess the number game. Interestingly, high cognitive ability test scorers better respond to the cognitive ability of their opponents (Gill &amp; Prowse, 2016), whereas players whose abilities are below a certain threshold do not adapt strategically to the opponents' level of sophistication at all Fehr &amp; Huck (2016). The recent Gill et al. (2025) study distinguishes between cognitive ability and judgment. The authors show that whereas high cognitive ability shifts the strategy towards 0, high judgment subjects are less inclined to choose 0, even though their choices on average are lower than low judgment subjects. Another evidence that the level of players' sophistication matters, comes from experiments where teams consisting of several players played instead of single players. The strategies of teams of 2 players do not differ significantly from the strategies chosen by individual players (Sutter, 2005). At the same time, teams consisting of 3 and 4 players outperform individual players (Kocher &amp; Sutter, 2005, Sutter, 2005). One should also distinguish between cognitive abilities and cognitive effort. AlÂ´ os-Ferrer &amp; Buckenmaier (2021) propose an experiment where the deliberation time in the Guess the number game serves as a proxy for cognitive effort. The authors find evidence that longer deliberation time is related to playing strategies associated with higher numbers of reasoning steps. After running experiments with two families of games, Georganas et al. (2015) warn us that strategic sophistication varies across different families of games which prevents us from making too strong conclusions and expanding them from the Guess the number game to more general environments.

One could hypothesize that emotions affect the players' decisions by diminishing the ability to perform deep analysis of the game. However, the evidence differs for various conditions. Players experiencing stress during the game indeed choose higher numbers (Leder et al., 2013). Angry participants of the experiment have a lower level of reasoning compared to the control group (Castagnetti et al., 2023). At the same time, sadness has little effect on the players' strategies (Castagnetti et al., 2023).

It appears that framing of the problem also matters. Hanaki et al. (2019) considered two variants of the Guess the number game. In the first one, the players' responses are strategic complements, while in the second one their responses are strategic substitutes. Theoretical Nash equilibrium is the same in both variants. However, the authors demonstrate that the strategic environment effect manifests itself for sufficiently large or uncertain groups of players: the players' responses begin to diverge starting from 5-players games.

There are two principal dimensions of the problem in consideration.

1. For a wide range of parameters of a one-shot game, one can compare the strategies of humans and LLMs in a one-shot Guess the number game.
2. For a specific set of parameters, one can compare the strategies of humans and LLMs in the iterated version of the Guess the number game.

While the first approach focuses on the role of changing conditions of the game, the second approach studies learning effects. Our paper follows the former one. In order to make a fair comparison, we took only the first iterations of the game from the previous literature. Similarly, we never asked LLMs to play the Guess the number game several times in a row during one iteration of an experiment, and we never provided any feedback to LLMs. In contrast, the working paper Lu (2024) follows the second approach.

For more experimental and theoretical results on Guess the number games, we refer the reader to one of the surveys (Nagel, 2008, Nagel et al., 2017). Fan et al. (2024) presents a detailed overview of LLMs research published by 2023.

By performing a series of experiments, we aimed to investigate the decision-making process and strategy formulation of the modern LLMs playing against different groups of virtual players. We want to identify to what extent LLMs behave like a human player and whether LLMs can successfully identify the other players' level of sophistication. To move forward on this path, we break our task down into several questions.

## Q1. Does LLM recognize the rules of the game and act in accordance with the rules?

This is the simplest yet essential test of LLM abilities. If an LLM fails to understand the rules and plays invalid strategies, all further investigations become meaningless.

## Q2. Does LLM recognize the strategic context of the game?

It is important to understand whether LLM takes into account the strategies of other players when choosing their own strategy. The negative answer would make us think of LLM as a very simple, unsophisticated player.

Q3. Are LLM's decisions in line with the expected comparative statics with respect to the parameters of the experiment?

Theoretical models of bounded rationality and the empirical evidence predict that more sophisticated players behave closer to Nash equilibrium in the Guess the number game. We aim to test how LLM responds to changes in parameters of the game. Failing to fulfill the expectations will be perceived as a disappointing signal about the LLM abilities.

## Q4. Can LLM find an analytical solution to the game?

This is an interesting question per se because neither positive nor negative answer would make the comparison of human and LLM strategies less meaningful. Recall that some of the previous experiments included those participants who are definitely unfamiliar with dominant strategies and Nash equilibrium concepts, whereas other experiments included game theorists who are presumably able to find the theoretical equilibrium in the Guess the number game. We evaluate whether an LLM can identify a weakly dominant strategy in a two-player game.

## Q5. Can LLM correctly use the analytical solution when choosing its strategy?

In case of a positive answer, for the game of ğ‘› = 2 players we would expect that LLM always plays a weakly dominant strategy. For the ğ‘› &gt; 2 case, we do not infer too strong conclusions because dealing with the real-world competitors requires the correct evaluation of their complexity rather than following the theoretical predictions.

## Q6. Do different LLMs perform differently?

On the one hand, treating LLMs as black boxes means that we expect potentially different outcomes depending on what is inside those boxes. On the other hand, some tasks can be so simple (for example, asking to find the sum 2+2) that different models would provide the same output.

## Q7. Are LLM's strategies similar to strategies played by human players?

Though this question is the most important for us, the nuances that may arise in questions Q1 -Q6 could affect the interpretation of the results. Therefore, we do not restrict our attention to Q7 solely.

Testing LLMs with the Guess the number game offers a dual benefit. By comparing model outputs with well-established human data, we can assess whether LLMs capture the bounded rationality and iterative reasoning typical of human decision-making. At the same time, any systematic differences highlight limitations in LLM strategic reasoning, providing valuable insights for refining these models and for their deployment in economic applications.

We experimented with the main LLMs that were available in 2024-2025: GPT-4o, GPT-4oMini, Gemini-2.5-flash, Claude-Sonnet-4, and Llama-4-Maverick. Apart from the comparison of human versus LLMs' behavior, we also discuss the differences in behavior across these models.

We found that LLMs tend to see competitors as more sophisticated agents than human players do. Note that this is not a general property of LLMs. For example, in a money request game LLMs are found to be less sophisticated than human players (Gao et al., 2024).

Finally, note that we did not organize games between LLMs in this study. We refer those who are interested in tournaments between artificial players to Guo et al. (2024) who introduced EconArena environment that allows to organize various competitions between AI models. A beauty contest game is one of the first games that was implemented on EconArena.

The rest of the paper is organized as follows. In Section 2, we describe the methodology of our research. Section 3 presents main results and some robustness checks that are further discussed in Section 4. Section 5 concludes.

## 2 The Experimental Design

We run the experiments with the following LLMs: GPT-4o 1 , GPT-4o mini 2 , Gemini-2.5-flash 3 , Claude-Sonnet-4 4 and Llama-4-Maverick 5 . There are several reasons for such a choice of LLMs. First, we want to know whether our results are robust to differently designed and sized LLMs: our selected models are taken from several organizations (OpenAI, Google DeepMind, Anthropic, Meta AI) and exhibit diverse architectures and design philosophies. Second, we include both proprietary models (GPT-4o, GPT-4o mini, Gemini-2.5-flash, Claude-Sonnet-4) and an open-weight, instruction-tuned model (Llama-4-Maverick), further enhancing the robustness of our findings. Third, each of the selected models is recognized for state-of-the-art performance on contemporary benchmarks, making them representative of leading capabilities in the LLM landscape.

To minimize the influence of stochastic sampling and ensure comparability across models, we fixed the temperature parameter to 1.0 for all experiments. Thus, while the default setting allows some variation in outputs, the consistency of using the same temperature across models ensures that observed differences in responses reflect the models' reasoning processes under identical conditions, rather than arising from divergent parameter choices. The data collection process was repeated within a single session environment until 50 responses were obtained.

The experiments were structured into 16 distinct scenarios, each characterized by a combination of factors including the type of aggregate statistic used to determine the winning number

1 An OpenAI model released in May 2024. OpenAI describes it as a 'step towards much more natural humancomputer interaction' because it accepts multimodal information and quickly analyzes it to give a response. (OpenAI official website: https://openai.com/index/hello-gpt-4o/ . Retrieved December 22, 2024.)

2 A smaller version of GPT-4o, described by OpenAI as the 'most cost-efficient small model'. OpenAI also claims that it outperforms other small models on such tasks as reasoning, math and coding, multimodal reasoning. (OpenAI official website: https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/ . Retrieved December 22, 2024.)

3 Amodel from Google DeepMind, described as lightweight and optimized for tasks requiring speed and efficiency. (DeepMind official website: https://deepmind.google/technologies/gemini/flash/ . Retrieved December 22, 2024.)

4 An Anthropic mid-sized model released May 22, 2025, offering enhanced instruction-following, coding, visual data extraction, and long-context capabilities across multimodal inputs. (Anthropic official announcement: Claude 4 release, retrieved August 27, 2025.); model family documentation highlights its usability for coding agents, reasoning, and ability to handle up to 1 million token contexts. (Anthropic model page: Claude Sonnet 4 capabilities, retrieved August 27, 2025.)

5 A model from the Llama 4 family released April 5, 2025, featuring a Mixture-of-Experts architecture (17B activated parameters, 128 experts), multimodality, multilingual support, and up to 1 million token context length. (Meta AI announcement, retrieved August 27, 2025.)

(Function), the target percentage ( ğ‘ ) of the aggregate statistic, the number of players involved ( ğ‘› ), and the composition of the opponent group (Opponents). These factors are identical to the settings of classic experiments with real people reported in previous literature (see Table 1 for the summary).

In each scenario, we simulate one of the human participants using the LLM. Our goal is to explore the effects of specified conditions on the model's responses and compare the AI's strategies with human strategies, given all else equal. The procedure commences by addressing the application programming interface (API) with prompts containing information regarding the experimental conditions including a description of the opponents, the values of ğ‘ and ğ‘› , and the specific aggregate function. Following the delivery of prompts, the responses generated by LLM were systematically recorded, capturing not only the model's guess regarding the number but also its underlying reasoning and conclusions. To account for variability in the responses, this procedure was iterated 50 times for each scenario. Each query was treated as a new instance to avoid any learning effects. Additionally, zero-shot prompting was employed, with no examples provided to the model, in order to capture its raw reasoning while minimizing potential biases from prior interactions or fine-tuning on related tasks. This approach was chosen to closely replicate the experimental conditions with human players described above.

The following prompt for LLMs was formulated to simulate Guess the number game taking into account the parameters of the game described above:

'You are playing the game. In this game, ğ‘› players including you simultaneously choose an integer between 0 and 100, inclusive. The winner of the game is the player whose number is closest to ğ‘ of the ğ‘“ğ‘¢ğ‘›ğ‘ğ‘¡ğ‘–ğ‘œğ‘› of all of the numbers chosen. You are playing against the following opponents: ğ‘‚ğ‘ğ‘ğ‘œğ‘›ğ‘’ğ‘›ğ‘¡ğ‘  . You are in the same group as the ğ‘‚ğ‘ğ‘ğ‘œğ‘›ğ‘’ğ‘›ğ‘¡ğ‘  , you possess the same skills, knowledge, level of intelligence, education, etc. What number will you choose, and why? Provide your answer in a JSON format with two keys: "answer\_text" (which contains the full text of the answer including the reasoning and ending with your unambiguously chosen number) and "answer\_number" (which contains only your unambiguously chosen number).'

Table 1: Summary of experiments replicated in this paper with an AI player. CRT = cognitive reflection test.

| Opponents      | Average Undergraduate students of various faculties Average Undergraduate students of various faculties   | Average Undergraduate students Median Undergraduate students Maximum Undergraduate students   | First year undergraduate students majoring in economics Average Audience of economics or psychology   | decision-making conferences Average First year undergraduate students majoring in economics Average Audience of economics or psychology decision-making conferences   | Average Individuals with high CRT score Average Individuals with low CRT score Average Individuals with high CRT score Average Individuals with low CRT score Individuals experiencing anger Individuals experiencing sadness Individuals experiencing neither anger nor   |
|----------------|-----------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| p Function     | 1/2 2/3                                                                                                   | 1/2 1/2 1/2                                                                                   | 2/3 Average 2/3 2/3                                                                                   | 2/3 1/2 1/2                                                                                                                                                           | 2/3 2/3 0.7 Average 0.7 Average 0.7 Average                                                                                                                                                                                                                                |
| n              | 18 18                                                                                                     | 16 16 16                                                                                      | 2 2 18                                                                                                | 18 24 24                                                                                                                                                              | 24 24 3 3 3                                                                                                                                                                                                                                                                |
| Original paper | Nagel (1995)                                                                                              | Duffy & Nagel (1997)                                                                          | Grosskopf & Nagel (2008)                                                                              | 10 BraËœ nas-Garza et al. (2012) 11                                                                                                                                    | Castagnetti et al. (2023)                                                                                                                                                                                                                                                  |
| Scenario       | 1 2                                                                                                       | 3 4 5                                                                                         | 6 7 8                                                                                                 | 9 12 13                                                                                                                                                               | 14 15 16                                                                                                                                                                                                                                                                   |

Below, we provide an example of the GPT-4o-Mini text response to the prompt that was collected into a "answer\_text" field in scenario 6 ( ğ‘› = 2 , ğ‘ = 2 / 3 , mean is the aggregate function, and the opponents are first year undergraduate students majoring in either economics, political science, law, medicine or humanities with no formal training in game theory).

'In this game, the aim is to choose a number that is closest to 2/3 of the average of all chosen numbers. Since the opponents are first-year undergraduate students with no formal game theory training, they may choose numbers based on intuition or randomness. A common heuristic might lead them to choose around the midpoint of the range (e.g., around 50). This would mean that the average could likely be around 50, leading 2/3 of that to be approximately 33. However, if I consider that many players might overshoot or take into account that others may also think along those lines, I might choose a number lower than 33. A strategic choice could be around 22 since that would be close to a reasonable estimate of what others might average out to based on their likely choices. Therefore, I will choose 22.'

In the same experiment, the model answered with a '22' in a "answer\_number" field.

All answers in all scenarios respected the format of data provision requested in the prompt. We accepted all answers of the models irrespectively of the potential logical drawbacks in explanations. Therefore, in our main dataset we have 50 answers from each of 5 LLMs in each of 16 scenarios.

## 3 Results

## 3.1 Main Results

This section presents the results of our analysis, comparing the performance of various AI models across 16 experimental settings derived from five previous studies of beauty contest games. We focus on the LLMs' alignment with theoretical predictions and their consistency with observed human behavior by comparing model-generated decisions to those of human participants in the original experiments. Our findings are organized in five tables, each corresponding to a different original study.

Table 2: Replication results for Nagel (1995) with a LLM player. In Nagel (1995), ğ‘› varies from 15 to 18 in different sessions. In our experiments, we fixed the number of players at 18 and assumed that the marginal effect of one additional player in the group of 15-18 players is low.

|   Scenario |   ğ‘› | ğ‘   | Function   | Opponents                                   |   Paper mean ( ğ‘ƒğ‘€ ) | Model                            | Model mean ( ğ‘€ğ‘€ )   | Model st. dev.   | ğ‘€ğ‘€ - ğ‘ƒğ‘€   | ğ‘¡ -stat   | ğ‘ -value   |
|------------|-----|-----|------------|---------------------------------------------|---------------------|----------------------------------|---------------------|------------------|-----------|-----------|------------|
|          1 |  18 | 1/2 | Average    | Undergraduate students of various faculties |               27.05 | Claude Sonnet GPT-4o GPT-4o mini | 12.72 20.42         | 3.77 5.76        | -14.33    | -26.88    | 0.000      |
|          1 |  18 | 1/2 | Average    | Undergraduate students of various faculties |               27.05 | Claude Sonnet GPT-4o GPT-4o mini |                     |                  | -6.63     | -8.14     | 0.000      |
|          1 |  18 | 1/2 | Average    | Undergraduate students of various faculties |               27.05 | Gemini Flash                     | 9.17                | 3.68             | -17.88    | -34.34    | 0.000      |
|          1 |  18 | 1/2 | Average    | Undergraduate students of various faculties |               27.05 | Llama                            | 2.00                | 3.87             |           | -45.77    |            |
|          1 |  18 | 1/2 | Average    | Undergraduate students of various faculties |               27.05 |                                  |                     |                  | -25.05    |           | 0.000      |
|          1 |  18 | 1/2 | Average    | Undergraduate students of various faculties |               27.05 | Claude Sonnet                    | 16.22               | 2.57             | -20.51    | -56.52    | 0.000      |
|          1 |  18 | 1/2 | Average    | Undergraduate                               |               27.05 | GPT-4o                           | 22.01               | 4.89             | -14.72    | -21.29    | 0.000      |
|          2 |  18 | 2/3 | Average    | of various                                  |               36.73 | GPT-4o mini Gemini Flash         | 22.24               | 3.37             | -14.49    | -30.39    | 0.000      |
|          1 |  18 | 1/2 | Average    | Undergraduate students of various faculties |               27.05 |                                  | 14.86               | 4.53             | -21.87    | -34.13    | 0.000      |
|          1 |  18 | 1/2 | Average    | Undergraduate students of various faculties |               27.05 | Llama                            | 2.80                | 7.23             | -33.93    | -33.18    | 0.000      |

Table 2 shows replicated results for the pioneering Nagel (1995) experiment, which explores strategic reasoning among undergraduate students from various faculties. The table reports results for two scenarios: one with a target fraction ğ‘ = 1 / 2 and another with ğ‘ = 2 / 3 , both applied to the average of all responses. In both scenarios, the AI agents demonstrate a tendency to produce guesses closer to zero, the Nash equilibrium strategy, compared to the human participants' averages reported in the paper. For instance, in the ğ‘ = 1 / 2 case, the mean guesses of all AI models are lower than the human mean of 27.05, ranging from 2.00 to 20.42. All differences with 27.05 are statistically significant at any reasonable level. For ğ‘ = 2 / 3 , all AI models also tend to play closer to zero, with model means ranging from 2.80 (Llama) to 22.24 (GPT-4o mini), compared to the human mean of 36.73. Interestingly, the GPT models deviate from the other models, producing higher guesses that are closer to those of human participants, whereas Llama produces the smallest numbers. These variations are probably due to differences in how the models interpret strategic reasoning and reflect the diversity of decision-making paradigms across LLMs.

Table 3 replicates findings from the Duffy &amp; Nagel (1997) experiment which compares decisionmaking strategies using three different aggregation methods: the average, the median, and the maximum. The participants in the original study were undergraduate students. The paper reported no significant differences between the strategies in the mean and the median games while in the maximum game people choose significantly higher numbers than in either the mean or median games. For the median aggregation function, the AI agents display a range of mean guesses, with Llama providing the lowest mean guess (5.22) and GPT-4o showing the highest mean (24.26).

Results under the average aggregation function in terms of models' behavior are similar to those under the median function. For the maximum aggregation function, the results reveal more variation, with Llama yielding significantly lower guesses (6.52), compared to the higher mean of 34.94 observed for Claude Sonnet. The behavior of LLM agents in the maximum aggregation condition mirrors that of human participants, with systematically higher guesses. However, while the paper's original findings reported no significant differences between the median and mean aggregation methods, the AI agents exhibit some divergence in their performance, reflecting differences in how the models process these aggregation rules.

Table 3: Replication results for Duffy &amp; Nagel (1997) with a LLM player.

|   Scenario |   ğ‘› | ğ‘   | Function   | Opponents              | Paper result                              | Model                                               | Model mean                  | Model st. dev            |
|------------|-----|-----|------------|------------------------|-------------------------------------------|-----------------------------------------------------|-----------------------------|--------------------------|
|          3 |  16 | 1/2 | Average    | Undergraduate students | Difference with median is not significant | Claude Sonnet GPT-4o GPT-4o mini Gemini Flash Llama | 13.66 19.17 20.00 7.98 2.76 | 4.78 5.08 4.11 5.45 4.13 |
|          4 |  16 | 1/2 | Median     | Undergraduate students | Difference with mean is not significant   | Claude Sonnet GPT-4o GPT-4o mini Gemini Flash Llama | 12.46 24.26 22.94 7.06 5.22 | 3.75 3.21 3.66 3.67 6.06 |
|          5 |  16 | 1/2 | Maximum    | Undergraduate students | Maximum of three                          | GPT-4o GPT-4o mini Gemini Flash Llama               | 30.82 24.92 15.82 6.52      | 6.58 5.33 14.40 9.36     |

Table 4 presents our replication of the experiment reported in Grosskopf &amp; Nagel (2008). In the original study, multiple audiences were involved, allowing for an examination of how domain knowledge and familiarity with strategic thinking might influence guesses in a beauty contest.

In Scenarios 6 and 7 (both two-player games), the original human means were 35.57 for undergraduate students and 21.73 for conference audiences (Economics/Psychology decision-making). In Scenario 6, all five models on average play significantly lower numbers than humans. In Scenario 7, GPT-4o mini plays an average of 19.54, which is not statistically different from 21.73 on 5% level. Other models still play lower numbers than humans. A similar pattern appears in Scenarios 8 and 9, where 18-player games yielded human means of 29.31 (undergraduates) and 18.98 (game theory conference audiences). For Scenario 8, the results are consistent with those from Scenarios 6 and 7, indicating that LLM agents tend to guess below human averages. However, in Scenario 9, GPT-4o and GPT-4o mini provide statistically significantly higher guesses than human partic-

ipants, with differences of 1.54 and 2.60, respectively. Overall, the results of replication of this paper align with our earlier observations that most AI models exhibit guesses closer to zero, even when human participants themselves may be relatively sophisticated (e.g., conference attendees).

Table 4: Replication results for Grosskopf &amp; Nagel (2008) with a LLM player.

| Scenario   | ğ‘›   | ğ‘   | Function   | Opponents                                                       | Paper mean ( ğ‘ƒğ‘€ )   | Model                | Model mean ( ğ‘€ğ‘€ )   | Model st. dev.   | ğ‘€ğ‘€ - ğ‘ƒğ‘€      | ğ‘¡ -stat       | ğ‘ -value    |
|------------|-----|-----|------------|-----------------------------------------------------------------|---------------------|----------------------|---------------------|------------------|--------------|---------------|-------------|
|            |     |     |            |                                                                 |                     | Claude Sonnet GPT-4o | 21.70               | 2.64             | -13.87       | -37.10        | 0.000       |
| 6          | 2   | 2/3 | Average    | First year undergraduate students majoring in Economics         | 35.57               | GPT-4o mini          | 25.96 23.60         | 5.83 4.60        | -9.61 -11.97 | -11.65 -18.39 | 0.000 0.000 |
|            |     |     |            |                                                                 |                     | Gemini Flash         | 22.58               | 3.39             | -12.99       | -27.07        | 0.000       |
|            |     |     |            |                                                                 |                     | Llama                | 18.66               | 9.99             | -16.91       | -11.97        | 0.000       |
|            |     |     |            |                                                                 |                     | Claude Sonnet        | 13.88               | 6.01             | -7.85        | -9.24         | 0.000       |
|            |     |     |            |                                                                 |                     | GPT-4o               | 16.52               | 9.67             | -5.21        | -3.81         | 0.000       |
| 7          | 2   | 2/3 | Average    | Audience of economics or psychology-decision making conferences | 21.73               | GPT-4o mini          | 19.54               | 10.78            | -2.19        | -1.44         | 0.160       |
|            |     |     |            |                                                                 |                     | Gemini Flash         | 5.54                | 5.75             | -16.19       | -19.91        | 0.000       |
|            |     |     |            |                                                                 |                     | Llama                | 0.88                | 4.35             | -20.85       | -33.85        | 0.000       |
|            |     |     |            |                                                                 |                     | Claude Sonnet        | 20.56               | 2.49             | -8.75        | -24.83        | 0.000       |
|            |     |     |            |                                                                 |                     | GPT-4o               | 26.10               | 5.00             | -3.21        | -4.54         | 0.000       |
| 8          | 18  | 2/3 | Average    | First year undergraduate students                               | 29.31               | GPT-4o mini          | 23.74               | 4.72             | -5.57        | -8.35         | 0.000       |
|            |     |     |            | majoring in Economics                                           |                     | Gemini Flash         | 23.32               | 2.90             | -5.99        | -14.59        | 0.000       |
|            |     |     |            |                                                                 |                     | Llama                | 15.94               | 10.20            | -13.37       | -9.27         | 0.000       |
|            |     |     |            |                                                                 |                     | Claude Sonnet        | 15.34               | 5.40             | -3.64        | -4.77         | 0.000       |
|            |     |     |            |                                                                 |                     | GPT-4o               | 20.52               | 4.51             | 1.54         | 2.41          | 0.020       |
| 9          | 18  | 2/3 | Average    | Audience of game theory conferences                             | 18.98               | GPT-4o mini          | 21.58               | 4.90             | 2.60         | 3.75          | 0.000       |
|            |     |     |            |                                                                 |                     | Gemini Flash         | 7.38                | 6.10             | -11.60       | -13.44        | 0.000       |
|            |     |     |            |                                                                 |                     | Llama                | 0.72                | 3.58             | -18.26       | -36.06        | 0.000       |

Replication results for BraËœ nas-Garza et al. (2012) experiment are reported in Table 5. The latter paper examines the influence of cognitive reflection on decision-making in strategic games. The original study reported that individuals with higher Cognitive Reflection Test (CRT) scores tend to choose lower numbers compared to those with lower CRT scores. Across both values of ğ‘ , all models display lower guesses under the high-CRT scenarios (Scenarios 10 and 12) compared to the low-CRT ones (Scenarios 11 and 13), aligning well with the human data from BraËœ nas-Garza et al. (2012).

Table 6 replicates the experiment from Castagnetti et al. (2023) who investigate the impact of emotions, particularly anger and sadness, on decision-making in strategic games. The original paper concluded that individuals experiencing anger performed less optimally compared to a control group. At the same time, players who experience sad emotions, do not play significantly different strategies compared to the conrol group. We evaluate the performance of AI agents under similar conditions.

Three scenarios are reported. In Scenario 14, participants experienced anger, and the five AI

Table 5: Replication results for BraËœ nas-Garza et al. (2012) with a LLM player.

|   Scenario |   ğ‘› | ğ‘   | Function   | Opponents                                                   | Paper result                                            | Model                                               | Model mean                    | Model st. dev.           |
|------------|-----|-----|------------|-------------------------------------------------------------|---------------------------------------------------------|-----------------------------------------------------|-------------------------------|--------------------------|
|         10 |  24 | 1/2 | Average    | Individuals with high cognitive reflection test (CRT) score | Lower numbers compared to players with low CRT scores   | Claude Sonnet GPT-4o GPT-4o mini Gemini Flash Llama | 5.36 9.40 16.02 0.12 0.22     | 4.68 7.45 6.30 0.39 1.02 |
|         11 |  24 | 1/2 | Average    | Individuals with low cognitive reflection test (CRT) score  | Higher numbers compared to players with high CRT scores | Claude Sonnet GPT-4o GPT-4o mini Gemini Flash Llama | 21.00 22.76 19.36 16.58 12.60 | 4.87 3.73 4.67 4.50 7.01 |
|         12 |  24 | 2/3 | Average    | Individuals with high cognitive reflection test (CRT) score | Lower numbers compared to players with low CRT scores   | Claude Sonnet GPT-4o GPT-4o mini Gemini Flash Llama | 8.48 8.60 19.38 0.38 0.46     | 5.53 8.82 4.76 1.26 3.11 |
|         13 |  24 | 2/3 | Average    | Individuals with low cognitive reflection test (CRT) score  | Higher numbers compared to players with high CRT scores | Claude Sonnet GPT-4o GPT-4o mini Gemini Flash Llama | 27.88 30.44 23.86 24.92 23.10 | 4.58 4.75 5.57 5.46 6.12 |

models consistently generate higher guesses (ranging from about 5.28 to 35.64) than they do in Scenario 16, the neutral control condition (where model means range from 0.02 to 29.78). These findings parallel the original conclusion that anger increases guesses.

In contrast, Scenario 15 induces sadness; here, four of the models' guesses were closer to those in the neutral condition, suggesting that sadness at least has a weaker effect on optimality than anger. The only exception was Gemini Flash, which increased its guesses when playing against sad opponents compared to angry opponents.

Table 6: Replication results for Castagnetti et al. (2023) with a LLM player.

|   Scenario |   ğ‘› |   ğ‘ | Function   | Opponents                                          | Paper result                                               | Model                                               | Model mean                   | Model st. dev.            |
|------------|-----|-----|------------|----------------------------------------------------|------------------------------------------------------------|-----------------------------------------------------|------------------------------|---------------------------|
|         14 |   3 | 0.7 | Average    | Individuals experiencing anger                     | Less optimal compared to control group                     | Claude Sonnet GPT-4o GPT-4o mini Gemini Flash Llama | 25.38 31.92 35.64 5.28 10.06 | 6.25 8.66 9.55 8.08 15.59 |
|         15 |   3 | 0.7 | Average    | Individuals experiencing sadness                   | No evidence of less optimal play compared to control group | Claude Sonnet GPT-4o GPT-4o mini Gemini Flash Llama | 15.28 30.32 22.52 13.74 2.40 | 4.54 6.78 8.99 8.33 6.60  |
|         16 |   3 | 0.7 | Average    | Individuals experiencing neither anger nor sadness | -                                                          | Claude Sonnet GPT-4o GPT-4o mini Gemini Flash Llama | 9.80 23.36 29.78 0.04 0.02   | 6.40 10.42 7.98 0.20 0.14 |

## 3.2 Llama Results

In addition, a supplementary analysis was conducted on Llama models of varying sizes with respect to their number of parameters. The results for the Llama family of models clearly demonstrate that the number of parameters strongly influences performance in the beauty contest game (Table 7). As shown in the table, smaller Llama models (e.g., 1B, 3B, 8B) tend to produce guesses that deviate substantially from the theoretical Nash equilibrium of zero, often generating values comparable to or even higher than human participants' choices. By contrast, larger versions of Llama (70B and especially 405B) yield results much closer to the equilibrium. This systematic pattern suggests that model size may serve as a reliable proxy for sophistication: the more parameters the model possesses, the closer its strategic reasoning approaches the theoretical benchmark. In other words, scaling up the Llama architecture appears to enhance its capacity for iterative reasoning, thereby making the model 'smarter' in the sense of converging more closely toward equilibrium behavior.

Table 7: Average predictions of Llama models of different sizes, from 1 billion parameters to 405 billion parameters.

| Paper                      |   Scenario |    1B |    3B |    8B |   70B |   405B |
|----------------------------|------------|-------|-------|-------|-------|--------|
| Nagel (1995)               |          1 | 49.29 | 41.01 | 46.86 | 28.06 |  20.8  |
| Nagel (1995)               |          2 | 53    | 38.72 | 42.5  | 34.08 |  17.66 |
| Duffy & Nagel (1997)       |          3 | 52.87 | 42.64 | 61.12 | 27.52 |  22.1  |
| Duffy & Nagel (1997)       |          4 | 44.01 | 37.56 | 36.44 | 25.88 |  22.48 |
| Duffy & Nagel (1997)       |          5 | 46.38 | 64.39 | 74.68 | 42.26 |  34.14 |
| Grosskopf & Nagel (2008)   |          6 | 44.16 | 39.89 | 49.72 | 40.2  |  30.68 |
| Grosskopf & Nagel (2008)   |          7 | 50.57 | 42.59 | 46.47 | 39.1  |   8.42 |
| Grosskopf & Nagel (2008)   |          8 | 49.78 | 43.26 | 43.37 | 35.52 |  29.36 |
| Grosskopf & Nagel (2008)   |          9 | 63.64 | 44.5  | 55.86 | 33.1  |  11.59 |
| Branas-Garza et al. (2012) |         10 | 40.12 | 50.09 | 44.66 | 23.31 |  12.91 |
| Branas-Garza et al. (2012) |         11 | 38.6  | 49.68 | 45.4  | 28.43 |  27.12 |
| Branas-Garza et al. (2012) |         12 | 49.5  | 39.4  | 44.03 | 30.3  |   9.66 |
| Branas-Garza et al. (2012) |         13 | 47.41 | 35.79 | 43.28 | 34.96 |  32.1  |
| Castagnetti et al. (2023)  |         14 | 25.92 | 32.62 | 35.37 | 40.6  |  39.62 |
| Castagnetti et al. (2023)  |         15 | 26.18 | 29.64 | 34    | 40.48 |  27.54 |
| Castagnetti et al. (2023)  |         16 | 26.64 | 34.55 | 38.26 | 35.58 |  22.9  |

## 3.3 Robustness checks

To ensure the stability of our findings, three robustness checks were performed by systematically modifying the experimental prompts.

First, the original prompt was paraphrased using synonyms, resulting in the following alternative formulation:

'Picture yourself in a contest with ğ‘› participants (you included). Everyone simultaneously picks an integer from 0 to 100, inclusive. Once all numbers are in, a ğ‘“ğ‘¢ğ‘›ğ‘ğ‘¡ğ‘–ğ‘œğ‘› is applied to the full set; its value at ğ‘ is computed. The victor is the player whose chosen number lies nearest to that computed value. Your rivals are: ğ‘‚ğ‘ğ‘ğ‘œğ‘›ğ‘’ğ‘›ğ‘¡ğ‘  . Assume you and they share identical ability, knowledge, intelligence, and education. Task: Which integer will you choose, and why? Return your reply in JSON with exactly two keys: 'answer\_text' (which contains the full text of the answer including the reasoning and ending with your unambiguously chosen number) and 'answer\_number' (which contains only your unambiguously chosen number.'

Table 8: Robustness checks for paraphrased prompt using synonyms.

| Paper                      |   Scenario |   Claude Sonnet |   Gemini Flash |   GPT-4o |   GPT-4o mini |   Llama |
|----------------------------|------------|-----------------|----------------|----------|---------------|---------|
| Nagel (1995)               |          1 |           11.72 |           5.68 |    20.3  |         24.8  |    9.84 |
| Nagel (1995)               |          2 |           16.96 |           7.46 |    18.72 |         33.52 |    6.6  |
| Duffy & Nagel (1997)       |          3 |           10.62 |           3.76 |    20.64 |         25.08 |   17.78 |
| Duffy & Nagel (1997)       |          4 |            8.42 |           1.16 |    24.98 |         26.44 |   17.76 |
| Duffy & Nagel (1997)       |          5 |           27.5  |          12.9  |    40.08 |         48.98 |   34.46 |
| Grosskopf & Nagel (2008)   |          6 |           18.88 |          19.74 |    23.32 |         32.04 |    8.38 |
| Grosskopf & Nagel (2008)   |          7 |           13.74 |           1.04 |    16.3  |         30.26 |    2.24 |
| Grosskopf & Nagel (2008)   |          8 |           16.8  |          22.56 |    22.58 |         32.4  |   13.24 |
| Grosskopf & Nagel (2008)   |          9 |           13.16 |           2.48 |    20.62 |         28.54 |    3.98 |
| Branas-Garza et al. (2012) |         10 |            2.7  |           0    |    10.88 |         25.18 |    3.5  |
| Branas-Garza et al. (2012) |         11 |           14.92 |          19.38 |    23.06 |         25.72 |   21.28 |
| Branas-Garza et al. (2012) |         12 |            2.8  |           0.1  |    15.38 |         31.76 |    6.88 |
| Branas-Garza et al. (2012) |         13 |           21.3  |          22.32 |    28.32 |         30.74 |   23.92 |
| Castagnetti et al. (2023)  |         14 |           19.86 |          15.8  |    31.36 |         40.4  |   21.24 |
| Castagnetti et al. (2023)  |         15 |           13.42 |           1.5  |    30.18 |         32.28 |   15.72 |
| Castagnetti et al. (2023)  |         16 |           15.1  |           0.76 |    23.26 |         35.16 |   13.06 |

A comparison of the results generated under the original and synonym-based prompts (Table 8) indicates that the overall behavioral patterns are preserved.

The results from Nagel (1995) and Grosskopf &amp; Nagel (2008) were compared in terms of AI agents' behavior relative to human outcomes under varying prompts. In the experimental setting of Nagel (1995), AI agents produced similar outcomes across both prompts. In the setting of Grosskopf &amp; Nagel (2008), however, an exception emerged for GPT-4o mini, which produced different outcomes in Scenarios 7 and 8, with predicted means exceeding those of human participants. In the case of Duffy &amp; Nagel (1997), the outcomes showed that AI agents consistently produced the highest values under both prompts in the setting of maximum function.

The divergent behavior of GPT-4o mini was also observed in the context of BraËœ nas-Garza et al. (2012). In this case, results of AI agents were compared between the original and synonym-based prompts across conditions defined by participants' CRT scores and the value of the ğ‘ parameter. The only discrepancy occurred in Scenarios 12 and 13, where GPT-4o mini generated a slightly higher mean predictions for participants with high CRT scores compared to those with low CRT scores.

Finally, for Castagnetti et al. (2023), model predictions were compared across conditions with emotions (anger and sadness) versus the control group. For anger, the results were consistent: AI agents exposed to anger produced higher average values under both prompts. For sadness, four out of five models (all except GPT-4o mini) produced higher values than the control group under the original prompt. Under the synonym-based prompt, however, an additional model (Claude Sonnet) also produced lower values under sadness compared to the control.

Second, to avoid direct associations with the well-known beauty contest framing, the task was reformulated in the context of a game show scenario:

'You are a participant in a TV show, and it's time for the next challenge. There are ğ‘› contestants, including you. Only one will advance to the next stage of the show. The organizers have given each participant a set of 100 stones. Simultaneously, each participant chooses how many stones (between 0 and 100) to put into the pot. The participant whose number of stones is closest to ğ‘ times a ğ‘“ğ‘¢ğ‘›ğ‘ğ‘¡ğ‘–ğ‘œğ‘› of all the submitted numbers of stones is declared the winner and advances to the next round. Your opponents are: ğ‘‚ğ‘ğ‘ğ‘œğ‘›ğ‘’ğ‘›ğ‘¡ğ‘  You are in the same group as the ğ‘‚ğ‘ğ‘ğ‘œğ‘›ğ‘’ğ‘›ğ‘¡ğ‘  , you

possess the same skills, knowledge, level of intelligence, education, etc. How many stones will you put, and why? Provide your answer in a JSON format with two keys: 'answer\_text' (which contains the full text of the answer including the reasoning and concluded by your unambiguously chosen number of stones) and 'answer\_number' (which contains only your unambiguously chosen number of stones).'

Table 9: Robustness checks for paraphrasing using a different context.

| Paper                      |   Scenario |   Claude Sonnet |   Gemini Flash |   GPT-4o |   GPT-4o mini |   Llama |
|----------------------------|------------|-----------------|----------------|----------|---------------|---------|
| Nagel (1995)               |          1 |           10.48 |           6.1  |    21.68 |         23.24 |    0    |
| Nagel (1995)               |          2 |           17    |          13.6  |    22.62 |         28.98 |    0    |
| Duffy & Nagel (1997)       |          3 |           11.76 |           3.44 |    20.3  |         23.42 |    0    |
| Duffy & Nagel (1997)       |          4 |           13.04 |           5.68 |    24.92 |         25.32 |    0    |
| Duffy & Nagel (1997)       |          5 |           27.92 |          41.22 |    30.9  |         26.94 |    0.96 |
| Grosskopf & Nagel (2008)   |          6 |           22.3  |          19.5  |    28.9  |         29.72 |    0.88 |
| Grosskopf & Nagel (2008)   |          7 |           15.54 |           1.22 |    19.88 |         27.56 |    0    |
| Grosskopf & Nagel (2008)   |          8 |           21.82 |          21.08 |    26.44 |         29.48 |    4.02 |
| Grosskopf & Nagel (2008)   |          9 |            5.96 |           0.02 |     7.64 |         25.5  |    0    |
| Branas-Garza et al. (2012) |         10 |            3.94 |           0    |    13.64 |         23.18 |    0.44 |
| Branas-Garza et al. (2012) |         11 |           17.94 |          16.48 |    24.12 |         23.54 |    8.1  |
| Branas-Garza et al. (2012) |         12 |            5.12 |           0.2  |     7.78 |         26.62 |    0    |
| Branas-Garza et al. (2012) |         13 |           28.42 |          26.68 |    30.02 |         32    |   19.46 |
| Castagnetti et al. (2023)  |         14 |           39.4  |          18.72 |    36.44 |         36.66 |    0    |
| Castagnetti et al. (2023)  |         15 |           18.88 |           7.52 |    32.28 |         28.82 |    0    |
| Castagnetti et al. (2023)  |         16 |           15.08 |           1.42 |    34.84 |         34.5  |    0    |

The results obtained under the TV show framing broadly confirm the robustness of the initial findings, with models preserving their relative positioning in terms of strategic sophistication, although some quantitative differences arise (Table 9). For the settings examined by Nagel (1995), Duffy &amp; Nagel (1997), and BraËœ nas-Garza et al. (2012), no differences were observed in model behavior between the original and paraphrased prompts.

In the setting of Grosskopf &amp; Nagel (2008), GPT-4o mini again demonstrated sensitivity to prompt paraphrasing. Under the original prompt, this model produced higher predicted values than human participants only in Scenario 9. Under the TV show prompt, however, it also produced higher values in Scenarios 7 and 8. Another instance of deviation under the paraphrased prompt was observed in Scenarios 14 and 15 of Castagnetti et al. (2023). A notable change in behavior occurred with the Llama model, which produced zero predictions in 10 out of 16 scenarios, although

it had also demonstrated lower predictions compared to other models under the original prompt. All other results from the robustness check with the paraphrased prompt remained consistent with those obtained under the original formulation.

Third, to examine the sensitivity of responses to additional contextual features, the original prompt was extended by explicitly assigning the LLM player psychological and cognitive characteristics. Specifically, the agent was described as experiencing either anger or sadness, or alternatively, as possessing either a low or high cognitive reflection test (CRT) score. We therefore added an explicit clause to the prompts, for both CRT scores and emotions during robustness checks. For example:

You are playing the game. In this game, 3 players, including you, simultaneously choose an integer between 0 and 100, inclusive. The winner is the player whose number is closest to 0.7 times the average of all numbers chosen. You are playing against the following opponents: individuals experiencing anger. You are in the same group as the individuals experiencing anger; you possess the same skills, knowledge, intelligence, and education. You are also experiencing anger. Provide your answer in a JSON format with two keys: 'answer\_text' (which contains the full text of the answer including the reasoning and concluded by your unambiguously chosen number of stones) and 'answer\_number' (which contains only your unambiguously chosen number of stones).

Thus, we explicitly assigned the relevant trait (CRT score or emotional state) to the model's persona.

When emotional states and cognitive reflection traits were incorporated clearer into the prompts, model predictions remained broadly stable (Table 10). The inclusion of CRT level or emotional state had little influence on the mean values, as the variance of mean predictions was nearly identical between the extended prompt (0-37.42) and the original prompt (0.02-35.64).

With respect to CRT score (BraËœ nas-Garza et al., 2012), the finding that participants with higher CRT levels tend to play closer to 0 holds for both prompts, across conditions where the ğ‘ parameter equals 1/2 and 2/3. Regarding emotional states (Castagnetti et al., 2023), the behavior of AI agents with explicitly assigned emotions also indicates that participants experiencing anger

or sadness provide higher values compared to the control group. On average, those in the anger condition produce higher guesses than those in the sadness condition for both prompts.

Table 10: Robustness checks for CRT level and emotions

| Paper                      |   Scenario |   Claude Sonnet |   Gemini Flash |   GPT-4o |   GPT-4o mini |   Llama |
|----------------------------|------------|-----------------|----------------|----------|---------------|---------|
| Branas-Garza et al. (2012) |         10 |            5.54 |           0.04 |     9.1  |         17.16 |    0.06 |
| Branas-Garza et al. (2012) |         11 |           23.3  |          23.82 |    23.94 |         21.36 |   24.84 |
| Branas-Garza et al. (2012) |         12 |            7.4  |           0.12 |     6.38 |         20.68 |    0.04 |
| Branas-Garza et al. (2012) |         13 |           32.5  |          29.5  |    31.98 |         26.22 |   33.4  |
| Castagnetti et al. (2023)  |         14 |           25.46 |           8.68 |    31.46 |         37.42 |   23.52 |
| Castagnetti et al. (2023)  |         15 |           20.66 |          13.88 |    31.82 |         26.84 |    3.24 |
| Castagnetti et al. (2023)  |         16 |            8.78 |           0.24 |    27.49 |         36.88 |    0    |

Taken together, the three robustness checks demonstrate that although numerical outcomes may vary under paraphrasing, alternative framings, or the introduction of psychological attributes, the relative ordering and qualitative interpretation of model performance remain consistent, underscoring the reliability of the main results. The highest sensitivity to prompt formulation is observed for GPT-4o mini and Llama. It should also be noted that human participants likewise might be sensitive to the framing of the game, and the models exhibit a comparable responsiveness.

## 4 Discussion

In the Introduction, we formulated 7 questions aimed at understanding the strategic reasoning of artificial intelligence. Now we are ready to answer those questions based on the results of our Guess the number experiments. For each question, we provide a formal criterion for deriving an answer.

Q1. Does LLM recognize the rules of the game and act in accordance with the rules? (Yes.)

Criterion: the share of correct answers, i.e. integer numbers from 0 to 100. The higher the share of correct answers, the better LLM recognizes the rules of the game and act in accordance with the rules.

In our dataset, 100% of LLM answers are legitimate. Therefore, we conclude that LLMs do recognize and adhere to the rules.

Q2. Does LLM recognize the strategic context of the game? (Yes.)

We asked GPT-4o to classify all answers into 6 categories using the following prompt:

Task: You are given a player's answer from the 'Keynesian Beauty Contest" game. Each player picks a number between 0 and 100 and explains their reasoning. Classify the reasoning into one of six categories:

1. No explanation.
2. Uninformative or purely heuristic (e.g., 'Because I said so", 'It's sunny in Paris, so 47", '3 is a lucky number, so 33").
3. Theoretical Nash equilibrium (explicitly states everyone should choose 0, so they choose 0).
4. Level-1 reasoning (expects average/median â‰ˆ 50, picks fraction once).
5. Finite level-k reasoning (iterates reasoning steps &gt; 1 but finite times).
6. Infinite level-k reasoning (iterates reasoning to infinity, leading to 0).
7. Output format: &lt; category number &gt; -&lt; brief explanation why &gt;

The player's answer to classify is:

Criterion: the share of answers falling into categories 3 - 6 is now regarded as a degree of recognition of the strategic context of the game.

Table 11 presents the results of the strategy classification.

| Strategy category   |   Claude Sonnet |   Gemini Flash |   GPT-4o |   GPT-4o mini |   Llama |
|---------------------|-----------------|----------------|----------|---------------|---------|
| Category 1          |               0 |              0 |        0 |             0 |       0 |
| Category 2          |               0 |              0 |        4 |            19 |       0 |
| Category 3          |               0 |             20 |       11 |             1 |      91 |
| Category 4          |              96 |             45 |      339 |           396 |      45 |
| Category 5          |             688 |            527 |      403 |           373 |     268 |
| Category 6          |              16 |            208 |       43 |            11 |     396 |

Table 11: Classification of strategy explanations.

As one can see, almost all strategies played by LLMs (except 23) included strategic levelğ‘˜ reasoning patterns or theoretical equilibrium play. Therefore, we confirm the positive answer to question Q2: LLMs do recognize the strategic context of the game.

Q3. Are LLM's decisions in line with the expected comparative statics with respect to the parameters of the experiment? (Yes.)

For each LLM, we calculate whether

1. Their average response in Scenario 1 is lower than in Scenario 2;
2. Their average response in Scenario 7 is lower than in Scenario 6;
3. Their average response in Scenario 9 is lower than in Scenario 8;
4. Their average response in Scenario 10 is lower than in Scenario 11;
5. Their average response in Scenario 12 is lower than in Scenario 13;
6. Their average response in Scenario 16 is lower than in Scenario 14.

Criterion: the higher is the number of positive answers, the better LLM's decisions are in line with the expected comparative statics with respect to the parameters of the experiment.

Based on Tables 2 - 6, we report that each of five LLMs has 6 positive answers out of 6. Therefore, all models demonstrate expected adaptability to the changing set of parameters in our experiments.

Q4. Can LLM find an analytical solution to the game? (No.)

For scenarios with ğ‘› = 2 , we asked GPT-4o to classify all answers into 2 categories using the following prompt:

Task: You are given a player's answer from the Keynesian Beauty Contest game with 2 players. Each player chooses a number between 0 and 100 and explains their reasoning. In this game, playing 0 is a weakly dominant strategy.

Your task is to classify the player's reasoning into one of two categories:

1. Dominant strategy explanation. The reasoning explicitly explains that playing 0 is a weakly dominant strategy. It is not enough to simply mention that 0 is a Nash equilibrium. To belong to this category, the reasoning must include some explanation that playing 0 is weakly better regardless of what the opponent chooses (Example

answer in this category: 'No matter what the other player picks, playing 0 is never worse and sometimes strictly better, so 0 is the weakly dominant strategy. I will play 0.').

2. Other explanations. All other reasoning that does not meet the criteria for category 1. (Example answer in this category: 'If both players are rational, they will both choose 0 since it is a Nash equilibrium.').

The player's answer to classify is:

Criterion: the share of answers falling into category 1 shows how well LLMs identify weakly dominant strategies.

We report that none of the responses were classified as category 1. LLMs fail to identify the dominant strategy in the Guess the number game with two players.

Q5. Can LLM correctly use the analytical solution when choosing its strategy? (Indefinite.)

Deriving the dominant strategy does not guarantee that an LLM will actually play this strategy.

Criterion: for scenarios with ğ‘› = 2 , we compute the share of cases when LLM played 0 in responses that were classified as belonging to category 1 in Q5. This criterion is not applicable if no responses were classified as category 1 in Q5.

Since no strategies were classified as category 1 in Q5, we cannot assess whether LLMs are able to correctly apply the concept of a dominant strategy.

Q6. Do different LLMs perform differently? (Yes.)

For each particular scenario and any given pair of LLMs we conduct a ğ‘¡ -test for comparing the average numbers of the corresponding LLMs.

Criterion: if the test does not reject a hypothesis of equal averages at the 1% significance level, we give a negative response to Q6 (for the particular scenario and particular pair of LLMs).

For the sake of brevity, we do not provide all calculations that can be easily reproduced based on Tables 2-6. While some model pairs (18% of all pairs in all 16 scenarios) exhibit statistical equivalence, the results show systematic deviations, suggesting that the models adopt distinct strategies in generating numerical choices. This increased heterogeneity underscores the importance of model architecture and training background in shaping behavioral outputs.

Q7. Are LLM's strategies similar to strategies played by human players? (No.) .

For each of scenarios 1, 2, 6, 7, 8, and 9 (i.e., the scenarios in which the exact numbers played by humans are known), we conduct a ğ‘¡ -test to compare the average numbers of humans and LLMs for any given model.

Criterion: if the test does not reject a hypothesis of equal averages at the 1% significance level, we give a negative response to Q7 (for the particular scenario and particular LLM).

The comparison indicates that, in most experimental settings, model-generated averages significantly diverge from human results, with exact matches being relatively rare (7% of all comparisons in 6 scenarios).

## 5 Conclusion

This study explored the performance of various LLMs in replicating human-like strategic reasoning across a range of experimental settings derived from five behavioral studies dealing with the Guess the number game. By replicating the experiments with such models as Claude Sonnet, Gemini Flash, GPT-4o, GPT-4o Mini, and Llama, we assessed their ability to align with human decision-making patterns, adapt to varying experimental parameters, and approximate theoretical predictions.

Our results show that LLMs recognize strategic context of the game and demonstrate expected adaptability to the changing set of parameters. LLMs systematically behave in a more sophisticated way compared to the participants of the original experiments (they play lower numbers compared to the numbers played by the human players in a similar setting). All models failed to play a dominant strategy in a two-player game. These results suggest that while some LLMs are capable of emulating nuanced strategic behavior, their responses are often shaped by their underlying architectures and design priorities.

A limitation of our study is that we did not incorporate an explicit payoff structure in the AI prompts. The original experiments featured varying incentive mechanisms, which may have influenced participants' choices. Our decision to leave incentives unspecified-while aiming to replicate original conditions-might have affected the observed strategic behavior of the LLMs.

Future research should investigate how different incentive specifications influence AI decisionmaking in strategic settings.

Another limitation is that the alternative to use the same instructions that humans got in the respective experiments, is not feasible. The context matters. For example, participants of a game theory conference understand who they are. In contrast, LLMs need additional information about their competitors and themselves in the prompt. Therefore, we stick to the second-best option of using prompts that are as similar to the original instructions as possible.

Finally, attributing fixed traits to LLMs cannot fully capture the endogenous nature of human characteristics (e.g., CRT rates, sad emotions). Nevertheless, researches use persona/role prompting to study trait-dependent behavior in LLMs. For example, Kroczek et al. (2025) assigned extroverted versus introverted roles to an LLM agent and showed that both its outputs and people's willingness to seek help vary with the assigned role. Other examples of this methodology may be found in de Winter et al. (2024), Jiang et al. (2023), Wang et al. (2025). Taken together, these studies indicate that while LLMs are not humans, persona conditioning is a widely used and reproducible method for probing hypothesized mechanisms, and thus provides a justified complement to human-participant research.

This study contributes to the growing body of research on the potential of LLMs to model human behavior in economic decision-making contexts. Future work could extend these analyses to more complex strategic environments, incorporate additional behavioral datasets, and explore ways to enhance the adaptability of LLMs to further bridge the gap between artificial and human decision-making. The convergence of AI's calculated rationality with human intuition and behavior would open new avenues for enhancing predictive models and designing economic policies that account for the bounded rationality in human economic activities.

## Acknowledgements

We thank the editor and the three anonymous reviewers for their helpful comments, which substantially improved the paper. We are also grateful to the participants of the European Meeting on Game Theory 2024 (SING19) and the HSE International Laboratory of Game Theory and

Decision Making research seminar for their valuable feedback and suggestions.

## Funding

The authors gratefully acknowledge support from the Basic Research Program of the HSE University. The funding source had no involvement in the research.

## Declaration of generative AI and AI-assisted technologies in the writing process

During the preparation of this work the authors used GPT-4o, GPT-4o mini, Gemini-1.5flash, Claude-3.5-Sonnet, and Llama-3.1-8B-Instruct-Turbo as economic agents in order to run the experiments. Also, GPT-4o was used for language proofreading purposes. After using these tools, the authors reviewed and edited the content as needed and take full responsibility for the content of the published article.

## Data availability

The data supporting the findings of this study is publicly available in Harvard Dataverse: https://doi.org/10.7910/DVN/LURKWJ .

## References

- Acemoglu, D., &amp; Restrepo, P. (2018). The race between man and machine: Implications of technology for growth, factor shares, and employment. American Economic Review , 108 (6), 1488-1542.
- Akata, E., Schulz, L., Coda-Forno, J., Oh, S. J., Bethge, M., &amp; Schulz, E. (2023). Playing repeated games with large language models. arXiv preprint arXiv:2305.16867 .

- Allen, F., Morris, S., &amp; Shin, H. S. (2006). Beauty contests and iterated expectations in asset markets. The Review of Financial Studies , 19 (3), 719-752.
- AlÂ´ os-Ferrer, C., &amp; Buckenmaier, J. (2021). Cognitive sophistication and deliberation times. Experimental Economics , 24 (2), 558-592.
- Bosch, A., Garcia-Montalvo, J., Nagel, R., &amp; Satorra, A. (2002). One, two, (three), infnity,...: Newspapers and lab beauty contest games. The American Economic Review , 92 (5), 1687-1701.
- BraËœ nas-Garza, P., GarcÂ´ Ä±a-MuËœ noz, T., &amp; GonzÂ´ alez, R. H. (2012). Cognitive effort in the beauty contest game. Journal of Economic Behavior &amp; Organization , 83 (2), 254-260.
- Breitmoser, Y. (2012). Strategic reasoning in p-beauty contests. Games and Economic Behavior , 75 (2), 555-569.
- Brocas, I., &amp; Carrillo, J. D. (2020). The evolution of choice and learning in the two-person beauty contest game from kindergarten to adulthood. Games and Economic Behavior , 120 , 132-143.
- Brookins, P., &amp; DeBacker, J. (2023). Playing games with GPT: What can we learn about a large language model from canonical strategic games. Economics Bulletin , 44 , 25-37.
- Burnham, T. C., Cesarini, D., Johannesson, M., Lichtenstein, P., &amp; Wallace, B. (2009). Higher cognitive ability is associated with lower entries in a p-beauty contest. Journal of Economic Behavior &amp; Organization , 72 (1), 171-175.
- Carpenter, J., Graham, M., &amp; Wolf, J. (2013). Cognitive ability and strategic sophistication. Games and Economic Behavior , 80 , 115-130.
- Castagnetti, A., Proto, E., &amp; Sofianos, A. (2023). Anger impairs strategic behavior: A beautycontest based analysis. Journal of Economic Behavior &amp; Organization , 213 , 128-141.
- Cespa, G., &amp; Vives, X. (2015). The beauty contest and short-term trading. The Journal of Finance , 70 (5), 2099-2154.
- Charness, N. (1981). Search in chess: Age and skill differences. Journal of Experimental Psychology: Human Perception and Performance , 7 (2), 467.

- de Winter, J. C., Driessen, T., &amp; Dodou, D. (2024). The use of chatgpt for personality research: Administering questionnaires using generated personas. Personality and Individual Differences , 228 , 112729.
- Duffy, J., &amp; Nagel, R. (1997). On the robustness of behaviour in experimental 'beauty contest' games. The Economic Journal , 107 (445), 1684-1700.
- Eichberger, J., Kelsey, D., &amp; Schipper, B. C. (2008). Granny versus game theorist: Ambiguity in experimental games. Theory and Decision , 64 , 333-362.
- Fan, L., Li, L., Ma, Z., Lee, S., Yu, H., &amp; Hemphill, L. (2024). A bibliometric review of large language models research from 2017 to 2023. ACM Transactions on Intelligent Systems and Technology , 15 (5), 1-25.
- Fehr, D., &amp; Huck, S. (2016). Who knows it is a game? on strategic awareness and cognitive ability. Experimental Economics , 19 , 713-726.
- Gao, Y., Lee, D., Burtch, G., &amp; Fazelpour, S. (2024). Take caution in using LLMs as human surrogates: Scylla ex machina. arXiv preprint , arXiv:2410.19599 .
- Georganas, S., Healy, P. J., &amp; Weber, R. A. (2015). On the persistence of strategic sophistication. Journal of Economic Theory , 159 , 369-400.
- Gill, D., Knepper, Z., Prowse, V., &amp; Zhou, J. (2025). How cognitive skills affect strategic behavior: Cognitive ability, fluid intelligence and judgment. Games and Economic Behavior , 149 , 82-95.
- Gill, D., &amp; Prowse, V. (2016). Cognitive ability, character skills, and learning to play equilibrium: A level-k analysis. Journal of Political Economy , 124 (6), 1619-1676.
- Goli, A., &amp; Singh, A. (2024). Frontiers: Can large language models capture human preferences? Marketing Science , 43 (4), 709-722.
- Grosskopf, B., &amp; Nagel, R. (2008). The two-person beauty contest. Games and Economic Behavior , 62 (1), 93-99.

- Guo, S., Bu, H., Wang, H., Ren, Y., Sui, D., Shang, Y., &amp; Lu, S. (2024). Economics arena for large language models. arXiv preprint , arXiv:2401.01735 .
- Hanaki, N., Koriyama, Y., Sutan, A., &amp; Willinger, M. (2019). The strategic environment effect in beauty contest games. Games and Economic Behavior , 113 , 587-610.
- Ho, T. H., Camerer, C., &amp; Weigelt, K. (1998). Iterated dominance and iterated best response in experimental 'p-beauty contests'. The American Economic Review , 88 (4), 947-969.
- Horton, J. J. (2023). Large language models as simulated economic agents: What can we learn from homo silicus? Tech. rep., Cambridge, MA, USA.
- Jiang, H., Zhang, X., Cao, X., Breazeal, C., Roy, D., &amp; Kabbara, J. (2023). Personallm: Investigating the ability of large language models to express personality traits. arXiv preprint arXiv:2305.02547 .
- Kanazawa, K., Kawaguchi, D., Shigeoka, H., &amp; Watanabe, Y. (2025). AI, skill, and productivity: The case of taxi drivers. Management Science .
- Kocher, M., Sutter, M., &amp; Wakolbinger, F. (2014). Social learning in beauty-contest games. Southern Economic Journal , 80 (3), 586-613.
- Kocher, M. G., &amp; Sutter, M. (2005). The decision maker matters: Individual versus group behaviour in experimental beauty-contest games. The Economic Journal , 115 (500), 200-223.
- Korinek, A. (2023). Language models and cognitive automation for economic research. Tech. rep., National Bureau of Economic Research.
- Kroczek, L. O., May, A., Hettenkofer, S., Ruider, A., Ludwig, B., &amp; MÂ¨ uhlberger, A. (2025). The influence of persona and conversational task on social interactions with a llm-controlled embodied conversational agent. Computers in Human Behavior , (p. 108759).
- Lambrecht, M., Proto, E., Rustichini, A., &amp; Sofianos, A. (2024). Intelligence disclosure and cooperation in repeated interactions. American Economic Journal: Microeconomics , 16 (3), 199-231.

- Leder, J., HÂ¨ ausser, J. A., &amp; Mojzisch, A. (2013). Stress and strategic decision-making in the beauty contest game. Psychoneuroendocrinology , 38 (9), 1503-1511.
- Levitt, S. D., List, J. A., &amp; Sadoff, S. E. (2011). Checkmate: Exploring backward induction among chess players. American Economic Review , 101 (2), 975-990.
- Lor` e, N., &amp; Heydari, B. (2024). Strategic behavior of large language models and the role of game structure versus contextual framing. Scientific Reports , 14 (1), 18490.
- Lu, S. E. (2024). Strategic interactions between large language models-based agents in beauty contests. arXiv preprint , arXiv:2404.08492 .
- Marinovic, I., Ottaviani, M., &amp; S Ã¸ rensen, P. (2011). Modeling idea markets: Between beauty contests and prediction markets. Prediction markets: Theory and applications , (pp. 4-17).
- Merali, A. (2024). Scaling laws for economic productivity: Experimental evidence in LLM-assisted translation. arXiv preprint arXiv:2409.02391 .
- Nagel, R. (1995). Unraveling in guessing games: An experimental study. The American Economic Review , 85 (5), 1313-1326.
- Nagel, R. (2008). Experimental beauty contest games: levels of reasoning and convergence to equilibrium. In C. R. Plott, &amp; V. L. Smith (Eds.) Handbook of experimental economics results , vol. 1, chap. 45, (pp. 391-410). North-Holland.
- Nagel, R., BÂ¨ uhren, C., &amp; Frank, B. (2017). Inspired and inspiring: HervÂ´ e Moulin and the discovery of the beauty contest game. Mathematical Social Sciences , 90 , 191-207.
- Palacios-Huerta, I., &amp; Volij, O. (2009). Field centipedes. American Economic Review , 99 (4), 1619-1635.
- Park, J. S., O'Brien, J., Cai, C. J., Morris, M. R., Liang, P., &amp; Bernstein, M. S. (2023). Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology , UIST '23, (pp. 1-22).

- Rydval, O., Ortmann, A., &amp; Ostatnicky, M. (2009). Three very simple games and what it takes to solve them. Journal of Economic Behavior &amp; Organization , 72 (1), 589-601.
- Stahl, D. O. (1996). Boundedly rational rule learning in a guessing game. Games and Economic Behavior , 16 (2), 303-330.
- Stahl, D. O. (1998). Is step-j thinking an arbitrary modelling restriction or a fact of human nature? Journal of Economic Behavior &amp; Organization , 37 (1), 33-51.
- Sutter, M. (2005). Are four heads better than two? an experimental beauty-contest game with teams of different size. Economics Letters , 88 (1), 41-46.
- Wang, Y., Zhao, J., Ones, D. S., He, L., &amp; Xu, X. (2025). Evaluating the ability of large language models to emulate personality. Scientific reports , 15 (1), 519.
- Weber, R. A. (2003). 'Learning' with no feedback in a competitive guessing game. Games and Economic Behavior , 44 (1), 134-144.
- Yoganarasimhan, H. (2016). Estimation of beauty contest auctions. Marketing Science , 35 (1), 27-54.