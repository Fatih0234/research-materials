## Q7. Are LLM's strategies similar to strategies played by human players?

Though this question is the most important for us, the nuances that may arise in questions Q1 -Q6 could affect the interpretation of the results. Therefore, we do not restrict our attention to Q7 solely.

Testing LLMs with the Guess the number game offers a dual benefit. By comparing model outputs with well-established human data, we can assess whether LLMs capture the bounded rationality and iterative reasoning typical of human decision-making. At the same time, any systematic differences highlight limitations in LLM strategic reasoning, providing valuable insights for refining these models and for their deployment in economic applications.

We experimented with the main LLMs that were available in 2024-2025: GPT-4o, GPT-4oMini, Gemini-2.5-flash, Claude-Sonnet-4, and Llama-4-Maverick. Apart from the comparison of human versus LLMs' behavior, we also discuss the differences in behavior across these models.

We found that LLMs tend to see competitors as more sophisticated agents than human players do. Note that this is not a general property of LLMs. For example, in a money request game LLMs are found to be less sophisticated than human players (Gao et al., 2024).

Finally, note that we did not organize games between LLMs in this study. We refer those who are interested in tournaments between artificial players to Guo et al. (2024) who introduced EconArena environment that allows to organize various competitions between AI models. A beauty contest game is one of the first games that was implemented on EconArena.

The rest of the paper is organized as follows. In Section 2, we describe the methodology of our research. Section 3 presents main results and some robustness checks that are further discussed in Section 4. Section 5 concludes.
