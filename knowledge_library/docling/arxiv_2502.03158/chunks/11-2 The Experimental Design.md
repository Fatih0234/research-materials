## 2 The Experimental Design

We run the experiments with the following LLMs: GPT-4o 1 , GPT-4o mini 2 , Gemini-2.5-flash 3 , Claude-Sonnet-4 4 and Llama-4-Maverick 5 . There are several reasons for such a choice of LLMs. First, we want to know whether our results are robust to differently designed and sized LLMs: our selected models are taken from several organizations (OpenAI, Google DeepMind, Anthropic, Meta AI) and exhibit diverse architectures and design philosophies. Second, we include both proprietary models (GPT-4o, GPT-4o mini, Gemini-2.5-flash, Claude-Sonnet-4) and an open-weight, instruction-tuned model (Llama-4-Maverick), further enhancing the robustness of our findings. Third, each of the selected models is recognized for state-of-the-art performance on contemporary benchmarks, making them representative of leading capabilities in the LLM landscape.

To minimize the influence of stochastic sampling and ensure comparability across models, we fixed the temperature parameter to 1.0 for all experiments. Thus, while the default setting allows some variation in outputs, the consistency of using the same temperature across models ensures that observed differences in responses reflect the models' reasoning processes under identical conditions, rather than arising from divergent parameter choices. The data collection process was repeated within a single session environment until 50 responses were obtained.

The experiments were structured into 16 distinct scenarios, each characterized by a combination of factors including the type of aggregate statistic used to determine the winning number

1 An OpenAI model released in May 2024. OpenAI describes it as a 'step towards much more natural humancomputer interaction' because it accepts multimodal information and quickly analyzes it to give a response. (OpenAI official website: https://openai.com/index/hello-gpt-4o/ . Retrieved December 22, 2024.)

2 A smaller version of GPT-4o, described by OpenAI as the 'most cost-efficient small model'. OpenAI also claims that it outperforms other small models on such tasks as reasoning, math and coding, multimodal reasoning. (OpenAI official website: https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/ . Retrieved December 22, 2024.)

3 Amodel from Google DeepMind, described as lightweight and optimized for tasks requiring speed and efficiency. (DeepMind official website: https://deepmind.google/technologies/gemini/flash/ . Retrieved December 22, 2024.)

4 An Anthropic mid-sized model released May 22, 2025, offering enhanced instruction-following, coding, visual data extraction, and long-context capabilities across multimodal inputs. (Anthropic official announcement: Claude 4 release, retrieved August 27, 2025.); model family documentation highlights its usability for coding agents, reasoning, and ability to handle up to 1 million token contexts. (Anthropic model page: Claude Sonnet 4 capabilities, retrieved August 27, 2025.)

5 A model from the Llama 4 family released April 5, 2025, featuring a Mixture-of-Experts architecture (17B activated parameters, 128 experts), multimodality, multilingual support, and up to 1 million token context length. (Meta AI announcement, retrieved August 27, 2025.)

(Function), the target percentage ( ğ‘ ) of the aggregate statistic, the number of players involved ( ğ‘› ), and the composition of the opponent group (Opponents). These factors are identical to the settings of classic experiments with real people reported in previous literature (see Table 1 for the summary).

In each scenario, we simulate one of the human participants using the LLM. Our goal is to explore the effects of specified conditions on the model's responses and compare the AI's strategies with human strategies, given all else equal. The procedure commences by addressing the application programming interface (API) with prompts containing information regarding the experimental conditions including a description of the opponents, the values of ğ‘ and ğ‘› , and the specific aggregate function. Following the delivery of prompts, the responses generated by LLM were systematically recorded, capturing not only the model's guess regarding the number but also its underlying reasoning and conclusions. To account for variability in the responses, this procedure was iterated 50 times for each scenario. Each query was treated as a new instance to avoid any learning effects. Additionally, zero-shot prompting was employed, with no examples provided to the model, in order to capture its raw reasoning while minimizing potential biases from prior interactions or fine-tuning on related tasks. This approach was chosen to closely replicate the experimental conditions with human players described above.

The following prompt for LLMs was formulated to simulate Guess the number game taking into account the parameters of the game described above:

'You are playing the game. In this game, ğ‘› players including you simultaneously choose an integer between 0 and 100, inclusive. The winner of the game is the player whose number is closest to ğ‘ of the ğ‘“ğ‘¢ğ‘›ğ‘ğ‘¡ğ‘–ğ‘œğ‘› of all of the numbers chosen. You are playing against the following opponents: ğ‘‚ğ‘ğ‘ğ‘œğ‘›ğ‘’ğ‘›ğ‘¡ğ‘  . You are in the same group as the ğ‘‚ğ‘ğ‘ğ‘œğ‘›ğ‘’ğ‘›ğ‘¡ğ‘  , you possess the same skills, knowledge, level of intelligence, education, etc. What number will you choose, and why? Provide your answer in a JSON format with two keys: "answer\_text" (which contains the full text of the answer including the reasoning and ending with your unambiguously chosen number) and "answer\_number" (which contains only your unambiguously chosen number).'

Table 1: Summary of experiments replicated in this paper with an AI player. CRT = cognitive reflection test.

| Opponents      | Average Undergraduate students of various faculties Average Undergraduate students of various faculties   | Average Undergraduate students Median Undergraduate students Maximum Undergraduate students   | First year undergraduate students majoring in economics Average Audience of economics or psychology   | decision-making conferences Average First year undergraduate students majoring in economics Average Audience of economics or psychology decision-making conferences   | Average Individuals with high CRT score Average Individuals with low CRT score Average Individuals with high CRT score Average Individuals with low CRT score Individuals experiencing anger Individuals experiencing sadness Individuals experiencing neither anger nor   |
|----------------|-----------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| p Function     | 1/2 2/3                                                                                                   | 1/2 1/2 1/2                                                                                   | 2/3 Average 2/3 2/3                                                                                   | 2/3 1/2 1/2                                                                                                                                                           | 2/3 2/3 0.7 Average 0.7 Average 0.7 Average                                                                                                                                                                                                                                |
| n              | 18 18                                                                                                     | 16 16 16                                                                                      | 2 2 18                                                                                                | 18 24 24                                                                                                                                                              | 24 24 3 3 3                                                                                                                                                                                                                                                                |
| Original paper | Nagel (1995)                                                                                              | Duffy & Nagel (1997)                                                                          | Grosskopf & Nagel (2008)                                                                              | 10 BraËœ nas-Garza et al. (2012) 11                                                                                                                                    | Castagnetti et al. (2023)                                                                                                                                                                                                                                                  |
| Scenario       | 1 2                                                                                                       | 3 4 5                                                                                         | 6 7 8                                                                                                 | 9 12 13                                                                                                                                                               | 14 15 16                                                                                                                                                                                                                                                                   |

Below, we provide an example of the GPT-4o-Mini text response to the prompt that was collected into a "answer\_text" field in scenario 6 ( ğ‘› = 2 , ğ‘ = 2 / 3 , mean is the aggregate function, and the opponents are first year undergraduate students majoring in either economics, political science, law, medicine or humanities with no formal training in game theory).

'In this game, the aim is to choose a number that is closest to 2/3 of the average of all chosen numbers. Since the opponents are first-year undergraduate students with no formal game theory training, they may choose numbers based on intuition or randomness. A common heuristic might lead them to choose around the midpoint of the range (e.g., around 50). This would mean that the average could likely be around 50, leading 2/3 of that to be approximately 33. However, if I consider that many players might overshoot or take into account that others may also think along those lines, I might choose a number lower than 33. A strategic choice could be around 22 since that would be close to a reasonable estimate of what others might average out to based on their likely choices. Therefore, I will choose 22.'

In the same experiment, the model answered with a '22' in a "answer\_number" field.

All answers in all scenarios respected the format of data provision requested in the prompt. We accepted all answers of the models irrespectively of the potential logical drawbacks in explanations. Therefore, in our main dataset we have 50 answers from each of 5 LLMs in each of 16 scenarios.
