## 3.1 Main Results

This section presents the results of our analysis, comparing the performance of various AI models across 16 experimental settings derived from five previous studies of beauty contest games. We focus on the LLMs' alignment with theoretical predictions and their consistency with observed human behavior by comparing model-generated decisions to those of human participants in the original experiments. Our findings are organized in five tables, each corresponding to a different original study.

Table 2: Replication results for Nagel (1995) with a LLM player. In Nagel (1995), ùëõ varies from 15 to 18 in different sessions. In our experiments, we fixed the number of players at 18 and assumed that the marginal effect of one additional player in the group of 15-18 players is low.

|   Scenario |   ùëõ | ùëù   | Function   | Opponents                                   |   Paper mean ( ùëÉùëÄ ) | Model                            | Model mean ( ùëÄùëÄ )   | Model st. dev.   | ùëÄùëÄ - ùëÉùëÄ   | ùë° -stat   | ùëù -value   |
|------------|-----|-----|------------|---------------------------------------------|---------------------|----------------------------------|---------------------|------------------|-----------|-----------|------------|
|          1 |  18 | 1/2 | Average    | Undergraduate students of various faculties |               27.05 | Claude Sonnet GPT-4o GPT-4o mini | 12.72 20.42         | 3.77 5.76        | -14.33    | -26.88    | 0.000      |
|          1 |  18 | 1/2 | Average    | Undergraduate students of various faculties |               27.05 | Claude Sonnet GPT-4o GPT-4o mini |                     |                  | -6.63     | -8.14     | 0.000      |
|          1 |  18 | 1/2 | Average    | Undergraduate students of various faculties |               27.05 | Gemini Flash                     | 9.17                | 3.68             | -17.88    | -34.34    | 0.000      |
|          1 |  18 | 1/2 | Average    | Undergraduate students of various faculties |               27.05 | Llama                            | 2.00                | 3.87             |           | -45.77    |            |
|          1 |  18 | 1/2 | Average    | Undergraduate students of various faculties |               27.05 |                                  |                     |                  | -25.05    |           | 0.000      |
|          1 |  18 | 1/2 | Average    | Undergraduate students of various faculties |               27.05 | Claude Sonnet                    | 16.22               | 2.57             | -20.51    | -56.52    | 0.000      |
|          1 |  18 | 1/2 | Average    | Undergraduate                               |               27.05 | GPT-4o                           | 22.01               | 4.89             | -14.72    | -21.29    | 0.000      |
|          2 |  18 | 2/3 | Average    | of various                                  |               36.73 | GPT-4o mini Gemini Flash         | 22.24               | 3.37             | -14.49    | -30.39    | 0.000      |
|          1 |  18 | 1/2 | Average    | Undergraduate students of various faculties |               27.05 |                                  | 14.86               | 4.53             | -21.87    | -34.13    | 0.000      |
|          1 |  18 | 1/2 | Average    | Undergraduate students of various faculties |               27.05 | Llama                            | 2.80                | 7.23             | -33.93    | -33.18    | 0.000      |

Table 2 shows replicated results for the pioneering Nagel (1995) experiment, which explores strategic reasoning among undergraduate students from various faculties. The table reports results for two scenarios: one with a target fraction ùëù = 1 / 2 and another with ùëù = 2 / 3 , both applied to the average of all responses. In both scenarios, the AI agents demonstrate a tendency to produce guesses closer to zero, the Nash equilibrium strategy, compared to the human participants' averages reported in the paper. For instance, in the ùëù = 1 / 2 case, the mean guesses of all AI models are lower than the human mean of 27.05, ranging from 2.00 to 20.42. All differences with 27.05 are statistically significant at any reasonable level. For ùëù = 2 / 3 , all AI models also tend to play closer to zero, with model means ranging from 2.80 (Llama) to 22.24 (GPT-4o mini), compared to the human mean of 36.73. Interestingly, the GPT models deviate from the other models, producing higher guesses that are closer to those of human participants, whereas Llama produces the smallest numbers. These variations are probably due to differences in how the models interpret strategic reasoning and reflect the diversity of decision-making paradigms across LLMs.

Table 3 replicates findings from the Duffy &amp; Nagel (1997) experiment which compares decisionmaking strategies using three different aggregation methods: the average, the median, and the maximum. The participants in the original study were undergraduate students. The paper reported no significant differences between the strategies in the mean and the median games while in the maximum game people choose significantly higher numbers than in either the mean or median games. For the median aggregation function, the AI agents display a range of mean guesses, with Llama providing the lowest mean guess (5.22) and GPT-4o showing the highest mean (24.26).

Results under the average aggregation function in terms of models' behavior are similar to those under the median function. For the maximum aggregation function, the results reveal more variation, with Llama yielding significantly lower guesses (6.52), compared to the higher mean of 34.94 observed for Claude Sonnet. The behavior of LLM agents in the maximum aggregation condition mirrors that of human participants, with systematically higher guesses. However, while the paper's original findings reported no significant differences between the median and mean aggregation methods, the AI agents exhibit some divergence in their performance, reflecting differences in how the models process these aggregation rules.

Table 3: Replication results for Duffy &amp; Nagel (1997) with a LLM player.

|   Scenario |   ùëõ | ùëù   | Function   | Opponents              | Paper result                              | Model                                               | Model mean                  | Model st. dev            |
|------------|-----|-----|------------|------------------------|-------------------------------------------|-----------------------------------------------------|-----------------------------|--------------------------|
|          3 |  16 | 1/2 | Average    | Undergraduate students | Difference with median is not significant | Claude Sonnet GPT-4o GPT-4o mini Gemini Flash Llama | 13.66 19.17 20.00 7.98 2.76 | 4.78 5.08 4.11 5.45 4.13 |
|          4 |  16 | 1/2 | Median     | Undergraduate students | Difference with mean is not significant   | Claude Sonnet GPT-4o GPT-4o mini Gemini Flash Llama | 12.46 24.26 22.94 7.06 5.22 | 3.75 3.21 3.66 3.67 6.06 |
|          5 |  16 | 1/2 | Maximum    | Undergraduate students | Maximum of three                          | GPT-4o GPT-4o mini Gemini Flash Llama               | 30.82 24.92 15.82 6.52      | 6.58 5.33 14.40 9.36     |

Table 4 presents our replication of the experiment reported in Grosskopf &amp; Nagel (2008). In the original study, multiple audiences were involved, allowing for an examination of how domain knowledge and familiarity with strategic thinking might influence guesses in a beauty contest.

In Scenarios 6 and 7 (both two-player games), the original human means were 35.57 for undergraduate students and 21.73 for conference audiences (Economics/Psychology decision-making). In Scenario 6, all five models on average play significantly lower numbers than humans. In Scenario 7, GPT-4o mini plays an average of 19.54, which is not statistically different from 21.73 on 5% level. Other models still play lower numbers than humans. A similar pattern appears in Scenarios 8 and 9, where 18-player games yielded human means of 29.31 (undergraduates) and 18.98 (game theory conference audiences). For Scenario 8, the results are consistent with those from Scenarios 6 and 7, indicating that LLM agents tend to guess below human averages. However, in Scenario 9, GPT-4o and GPT-4o mini provide statistically significantly higher guesses than human partic-

ipants, with differences of 1.54 and 2.60, respectively. Overall, the results of replication of this paper align with our earlier observations that most AI models exhibit guesses closer to zero, even when human participants themselves may be relatively sophisticated (e.g., conference attendees).

Table 4: Replication results for Grosskopf &amp; Nagel (2008) with a LLM player.

| Scenario   | ùëõ   | ùëù   | Function   | Opponents                                                       | Paper mean ( ùëÉùëÄ )   | Model                | Model mean ( ùëÄùëÄ )   | Model st. dev.   | ùëÄùëÄ - ùëÉùëÄ      | ùë° -stat       | ùëù -value    |
|------------|-----|-----|------------|-----------------------------------------------------------------|---------------------|----------------------|---------------------|------------------|--------------|---------------|-------------|
|            |     |     |            |                                                                 |                     | Claude Sonnet GPT-4o | 21.70               | 2.64             | -13.87       | -37.10        | 0.000       |
| 6          | 2   | 2/3 | Average    | First year undergraduate students majoring in Economics         | 35.57               | GPT-4o mini          | 25.96 23.60         | 5.83 4.60        | -9.61 -11.97 | -11.65 -18.39 | 0.000 0.000 |
|            |     |     |            |                                                                 |                     | Gemini Flash         | 22.58               | 3.39             | -12.99       | -27.07        | 0.000       |
|            |     |     |            |                                                                 |                     | Llama                | 18.66               | 9.99             | -16.91       | -11.97        | 0.000       |
|            |     |     |            |                                                                 |                     | Claude Sonnet        | 13.88               | 6.01             | -7.85        | -9.24         | 0.000       |
|            |     |     |            |                                                                 |                     | GPT-4o               | 16.52               | 9.67             | -5.21        | -3.81         | 0.000       |
| 7          | 2   | 2/3 | Average    | Audience of economics or psychology-decision making conferences | 21.73               | GPT-4o mini          | 19.54               | 10.78            | -2.19        | -1.44         | 0.160       |
|            |     |     |            |                                                                 |                     | Gemini Flash         | 5.54                | 5.75             | -16.19       | -19.91        | 0.000       |
|            |     |     |            |                                                                 |                     | Llama                | 0.88                | 4.35             | -20.85       | -33.85        | 0.000       |
|            |     |     |            |                                                                 |                     | Claude Sonnet        | 20.56               | 2.49             | -8.75        | -24.83        | 0.000       |
|            |     |     |            |                                                                 |                     | GPT-4o               | 26.10               | 5.00             | -3.21        | -4.54         | 0.000       |
| 8          | 18  | 2/3 | Average    | First year undergraduate students                               | 29.31               | GPT-4o mini          | 23.74               | 4.72             | -5.57        | -8.35         | 0.000       |
|            |     |     |            | majoring in Economics                                           |                     | Gemini Flash         | 23.32               | 2.90             | -5.99        | -14.59        | 0.000       |
|            |     |     |            |                                                                 |                     | Llama                | 15.94               | 10.20            | -13.37       | -9.27         | 0.000       |
|            |     |     |            |                                                                 |                     | Claude Sonnet        | 15.34               | 5.40             | -3.64        | -4.77         | 0.000       |
|            |     |     |            |                                                                 |                     | GPT-4o               | 20.52               | 4.51             | 1.54         | 2.41          | 0.020       |
| 9          | 18  | 2/3 | Average    | Audience of game theory conferences                             | 18.98               | GPT-4o mini          | 21.58               | 4.90             | 2.60         | 3.75          | 0.000       |
|            |     |     |            |                                                                 |                     | Gemini Flash         | 7.38                | 6.10             | -11.60       | -13.44        | 0.000       |
|            |     |     |            |                                                                 |                     | Llama                | 0.72                | 3.58             | -18.26       | -36.06        | 0.000       |

Replication results for BraÀú nas-Garza et al. (2012) experiment are reported in Table 5. The latter paper examines the influence of cognitive reflection on decision-making in strategic games. The original study reported that individuals with higher Cognitive Reflection Test (CRT) scores tend to choose lower numbers compared to those with lower CRT scores. Across both values of ùëù , all models display lower guesses under the high-CRT scenarios (Scenarios 10 and 12) compared to the low-CRT ones (Scenarios 11 and 13), aligning well with the human data from BraÀú nas-Garza et al. (2012).

Table 6 replicates the experiment from Castagnetti et al. (2023) who investigate the impact of emotions, particularly anger and sadness, on decision-making in strategic games. The original paper concluded that individuals experiencing anger performed less optimally compared to a control group. At the same time, players who experience sad emotions, do not play significantly different strategies compared to the conrol group. We evaluate the performance of AI agents under similar conditions.

Three scenarios are reported. In Scenario 14, participants experienced anger, and the five AI

Table 5: Replication results for BraÀú nas-Garza et al. (2012) with a LLM player.

|   Scenario |   ùëõ | ùëù   | Function   | Opponents                                                   | Paper result                                            | Model                                               | Model mean                    | Model st. dev.           |
|------------|-----|-----|------------|-------------------------------------------------------------|---------------------------------------------------------|-----------------------------------------------------|-------------------------------|--------------------------|
|         10 |  24 | 1/2 | Average    | Individuals with high cognitive reflection test (CRT) score | Lower numbers compared to players with low CRT scores   | Claude Sonnet GPT-4o GPT-4o mini Gemini Flash Llama | 5.36 9.40 16.02 0.12 0.22     | 4.68 7.45 6.30 0.39 1.02 |
|         11 |  24 | 1/2 | Average    | Individuals with low cognitive reflection test (CRT) score  | Higher numbers compared to players with high CRT scores | Claude Sonnet GPT-4o GPT-4o mini Gemini Flash Llama | 21.00 22.76 19.36 16.58 12.60 | 4.87 3.73 4.67 4.50 7.01 |
|         12 |  24 | 2/3 | Average    | Individuals with high cognitive reflection test (CRT) score | Lower numbers compared to players with low CRT scores   | Claude Sonnet GPT-4o GPT-4o mini Gemini Flash Llama | 8.48 8.60 19.38 0.38 0.46     | 5.53 8.82 4.76 1.26 3.11 |
|         13 |  24 | 2/3 | Average    | Individuals with low cognitive reflection test (CRT) score  | Higher numbers compared to players with high CRT scores | Claude Sonnet GPT-4o GPT-4o mini Gemini Flash Llama | 27.88 30.44 23.86 24.92 23.10 | 4.58 4.75 5.57 5.46 6.12 |

models consistently generate higher guesses (ranging from about 5.28 to 35.64) than they do in Scenario 16, the neutral control condition (where model means range from 0.02 to 29.78). These findings parallel the original conclusion that anger increases guesses.

In contrast, Scenario 15 induces sadness; here, four of the models' guesses were closer to those in the neutral condition, suggesting that sadness at least has a weaker effect on optimality than anger. The only exception was Gemini Flash, which increased its guesses when playing against sad opponents compared to angry opponents.

Table 6: Replication results for Castagnetti et al. (2023) with a LLM player.

|   Scenario |   ùëõ |   ùëù | Function   | Opponents                                          | Paper result                                               | Model                                               | Model mean                   | Model st. dev.            |
|------------|-----|-----|------------|----------------------------------------------------|------------------------------------------------------------|-----------------------------------------------------|------------------------------|---------------------------|
|         14 |   3 | 0.7 | Average    | Individuals experiencing anger                     | Less optimal compared to control group                     | Claude Sonnet GPT-4o GPT-4o mini Gemini Flash Llama | 25.38 31.92 35.64 5.28 10.06 | 6.25 8.66 9.55 8.08 15.59 |
|         15 |   3 | 0.7 | Average    | Individuals experiencing sadness                   | No evidence of less optimal play compared to control group | Claude Sonnet GPT-4o GPT-4o mini Gemini Flash Llama | 15.28 30.32 22.52 13.74 2.40 | 4.54 6.78 8.99 8.33 6.60  |
|         16 |   3 | 0.7 | Average    | Individuals experiencing neither anger nor sadness | -                                                          | Claude Sonnet GPT-4o GPT-4o mini Gemini Flash Llama | 9.80 23.36 29.78 0.04 0.02   | 6.40 10.42 7.98 0.20 0.14 |
