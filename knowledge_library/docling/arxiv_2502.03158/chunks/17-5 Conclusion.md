## 5 Conclusion

This study explored the performance of various LLMs in replicating human-like strategic reasoning across a range of experimental settings derived from five behavioral studies dealing with the Guess the number game. By replicating the experiments with such models as Claude Sonnet, Gemini Flash, GPT-4o, GPT-4o Mini, and Llama, we assessed their ability to align with human decision-making patterns, adapt to varying experimental parameters, and approximate theoretical predictions.

Our results show that LLMs recognize strategic context of the game and demonstrate expected adaptability to the changing set of parameters. LLMs systematically behave in a more sophisticated way compared to the participants of the original experiments (they play lower numbers compared to the numbers played by the human players in a similar setting). All models failed to play a dominant strategy in a two-player game. These results suggest that while some LLMs are capable of emulating nuanced strategic behavior, their responses are often shaped by their underlying architectures and design priorities.

A limitation of our study is that we did not incorporate an explicit payoff structure in the AI prompts. The original experiments featured varying incentive mechanisms, which may have influenced participants' choices. Our decision to leave incentives unspecified-while aiming to replicate original conditions-might have affected the observed strategic behavior of the LLMs.

Future research should investigate how different incentive specifications influence AI decisionmaking in strategic settings.

Another limitation is that the alternative to use the same instructions that humans got in the respective experiments, is not feasible. The context matters. For example, participants of a game theory conference understand who they are. In contrast, LLMs need additional information about their competitors and themselves in the prompt. Therefore, we stick to the second-best option of using prompts that are as similar to the original instructions as possible.

Finally, attributing fixed traits to LLMs cannot fully capture the endogenous nature of human characteristics (e.g., CRT rates, sad emotions). Nevertheless, researches use persona/role prompting to study trait-dependent behavior in LLMs. For example, Kroczek et al. (2025) assigned extroverted versus introverted roles to an LLM agent and showed that both its outputs and people's willingness to seek help vary with the assigned role. Other examples of this methodology may be found in de Winter et al. (2024), Jiang et al. (2023), Wang et al. (2025). Taken together, these studies indicate that while LLMs are not humans, persona conditioning is a widely used and reproducible method for probing hypothesized mechanisms, and thus provides a justified complement to human-participant research.

This study contributes to the growing body of research on the potential of LLMs to model human behavior in economic decision-making contexts. Future work could extend these analyses to more complex strategic environments, incorporate additional behavioral datasets, and explore ways to enhance the adaptability of LLMs to further bridge the gap between artificial and human decision-making. The convergence of AI's calculated rationality with human intuition and behavior would open new avenues for enhancing predictive models and designing economic policies that account for the bounded rationality in human economic activities.
