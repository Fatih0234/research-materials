## 2.1 Observing Behavioral Features of LLM-based Agents

This subsection focuses on the behavioral characteristics exhibited by LLM-based agents in diverse game environments, capturing their decision-making tendencies, social interaction patterns, and responses to dynamic game states. By deploying LLMs in structured scenarios, ranging from matrix games and negotiation to deception and cooperation, researchers can uncover critical traits such as risk sensitivity, strategic adaptation, and behavioral consistency. These patterns not only reveal how LLMs interpret rules and model opponents but also expose cognitive limitations and human-like biases. A taxonomy of studies categorized by game types is presented in Table 1 to provide a structured overview of the empirical landscape.

2.1.1 Basic Matrix Games. Matrix games represent foundational strategic settings in which players have complete information about the game structure, including all possible actions and payoffs. These games are often used as elementary tests of rational behavior [1]. Recent studies employ natural language prompts to position LLMs as players in matrix games, instructing them to make decisions under various utility assumptions. Findings reveal that LLMs frequently exhibit pro-social biases, often prioritizing fairness and cooperation over game-theoretic rationality . For instance, Manuscript submitted to ACM

LLMs consistently display higher cooperation rates than humans in social dilemmas such as the Dictator Game [35, 5254], and often reject unfair offers in the Ultimatum Game due to inequity aversion [36, 37, 55]. The Turing Test conducted by Mei et al. [56] also confirms that LLMs tend to behave more altruistically and cooperatively, which suggests that LLMs are maximizing the average of their and the partner's payoff by default. These behaviors likely stem from human-aligned moral patterns encoded during pre-training [38, 57].

At the same time, LLMs often deviate from optimality , especially in tasks demanding probabilistic reasoning or adaptive play. For example, in zero-sum games like Rock-Paper-Scissors, many models fail to approximate mixed-strategy Nash equilibria [38, 58, 59]. Repeated games like the Battle of the Sexes further reveal prompt sensitivity and coordination fragility [12, 60-62]. In adversarial contexts, LLMs often revert to risk-averse or heuristic strategies [51, 63] and display bounded rationality that favors symmetric or fair outcomes over payoff maximization [37, 55]. Beyond specific games, systematic benchmarks such as FAIRGAME [64] and SmartPlay [58] also highlight the sensitivity of LLM behavior to prompt framing and contextual cues.

2.1.2 Identity Games. Identity games like Avalon, Werewolf, and Jubensha involve hidden roles, incomplete information, and social deception, making them rich environments for evaluating higher-order reasoning and strategic communication. In these games, agents must navigate uncertainty, infer hidden roles, and engage in deception, persuasion, and collaboration. Research in this area highlights a key duality, like LLM behavior. Several studies employ LLM-based agents in an identity game-playground, letting different agents compete in the same game. Through thorough experimental observations, researchers find that LLMs can engage in recursive reasoning and social modeling. For example, in Avalon, agents simulate others' beliefs and behaviors effectively [13], while in Werewolf, some LLMs serve as 'opinion leaders,' influencing teammates via persuasive summarization [39]. Narrative games like Jubensha highlight LLMs' abilities to process long-form clues, infer roles, and construct coherent narratives [20]. On the other hand, LLMs often exhibit strategic inconsistencies and fragility , particularly when faced with adversarial conditions or the need for robust logical deduction. AvalonBench reveals that current models have notable gaps in maintaining a consistent strategy and fully adapting to their assigned roles, especially when pressured by opponents [40]. In deduction-focused games like Werewolf, LLMs are prone to hallucination and flawed logical inference without external guidance [65, 66]. Despite appearing socially adept, these models remain susceptible to hallucinations, premature inferences, and are less aware of other players' intentions in high-stakes interactions [67].

2.1.3 Negotiation and Coordination Games. Communication-based games, such as bargaining, provide a lens into LLMs' negotiation acumen and collaborative reasoning. Studies show agents employing recognizable strategies like bluffing, anchoring, and making concessions during negotiations [18, 41, 68]. More advanced models like GPT-4 are proficient at goal-directed planning and successfully making deals [19, 69, 70]. In contrast, less capable models often fail to maintain a consistent persona or even complete the negotiation task [71-73]. Meanwhile, their behavior remains highly sensitive to prompt phrasing, revealing a mixture of rational optimization and socially biased responses characteristic of bounded rationality [74, 75]. In teamwork tasks, LLMs demonstrate emerging Theory of Mind (ToM) but struggle to maintain coordination in complex, belief-intensive environments. While LLMs show a basic ability to infer the goals and intentions of teammates in games like Overcooked and Hanabi, their capacity for robust joint planning is limited [42, 43, 68]. Davidson et al. [44] showed that cooperative bargaining games are the most challenging for LLMs. More specifically, models often regress to selfish or inconsistent strategies when social scaffolding is absent [76, 77] and there are persistent challenges related to memory, deception, and adaptation, particularly in non-cooperative or socially complex contexts [70, 71, 76].

2.1.4 Economic Games. Economic games simulate market dynamics, requiring agents to make optimal decisions about pricing, bidding, and resource allocation under competitive pressure. Success in these environments demands a nuanced understanding of market forces, risk assessment, and the ability to anticipate and respond to competitors' actions. In these scenarios, including pricing games and auctions, LLMs demonstrate adaptive and sophisticated economic strategies, yet their performance is consistently bounded by imperfect reasoning and risk assessment. On the one hand, LLMs can emulate complex economic behaviors. For instance, tacit collusion and reward-punishment strategies emerge in repeated Bertrand games, with GPT-4 agents learning to maintain supracompetitive prices [45]. In dynamic auction environments, LLMs also show strategic planning by updating beliefs and reprioritizing items based on evolving conditions [15, 46]. However, this strategic acuity is often fragile and lacks robustness. Studies show that simpler heuristics can outperform LLMs, and equilibrium strategies are inconsistently achieved, revealing gaps in their strategic depth [15, 46, 51]. These observations reinforce the framing of LLMs as boundedly rational economic agents whose strategic capabilities are often superficial [47].

2.1.5 Board and Card Games. Classic board and card games serve as demanding benchmarks for AI, as they require capabilities that are distinct from natural language processing. Success in these domains hinges on deep strategic planning, precise calculation, and sophisticated management of uncertainty. Across both perfect and imperfect-information games, baseline LLMs exhibit significant strategic deficiencies, struggling with deep calculation, logical consistency, and the management of uncertainty. In perfect-information games like Chess and Go, which demand rigorous, forward-looking planning, LLMs often fail to produce strategically coherent or even legally valid sequences of moves-a core deficiency that has necessitated the creation of specialized, search-augmented models to achieve competence [48, 78]. This challenge extends to imperfect-information games, where the core difficulty shifts from pure calculation to managing strategic uncertainty. In poker, for instance, benchmarks show that vanilla LLMs perform poorly without significant fine-tuning, primarily due to their inability to handle hidden information and assess risk effectively [17, 49]. Similarly, in complex tactical battle games, these models are prone to hallucination and erratic play [50]. Taken together, evidence from multi-game testbeds confirms that deep planning limitations and strategic inconsistencies are pervasive weaknesses of general-purpose LLMs in these calculation-intensive contexts [51].

2.1.6 Established Benchmarks. To systematically probe the strategic capabilities of LLMs, a growing number of specialized benchmarks have been developed. These platforms provide controlled environments to assess specific facets of agent behavior, from pure rationality to complex social interaction. Several benchmarks focus on foundational strategic reasoning in classical games. For example, GTBench [79], ùõæ -bench [80], GameBench [81], and the work by Topsakal et al. [82] primarily assess whether LLM decisions align with theoretical equilibria. Building on this, other frameworks like FAIRGAME [64] and SmartPlay [58] investigate the nuances of these decisions, examining deviations such as fairness biases, prompt sensitivity, and other signatures of bounded rationality.

The evaluation landscape has also expanded to encompass more dynamic and socially complex scenarios. Benchmarks like LLMArena [83], AvalonBench [40], and NegotiationArena [84] place LLMs in multi-agent settings to test higherorder recursive reasoning, coordination, and deception. The scope further extends into economic and hybrid domains, with GLEE [85] and TMGBench [86] evaluating agents in pricing and auction tasks, while ALYMPICS [87] and Welfare Diplomacy [72] integrate negotiation and moral reasoning. To address concerns about agent fragility, recent additions like lmgame-Bench [88] and Playing Games [38] introduce metrics for disqualification and randomness-handling to better identify brittle behaviors. Collectively, this diverse and rapidly evolving suite of benchmarks provides an essential toolkit for diagnosing, comparing, and ultimately improving the strategic capabilities of LLM-based agents. Manuscript submitted to ACM
