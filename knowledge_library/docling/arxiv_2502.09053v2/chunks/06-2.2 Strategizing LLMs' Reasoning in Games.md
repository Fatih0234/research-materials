## 2.2 Strategizing LLMs' Reasoning in Games

This subsection explores methods to enhance LLMs' strategic reasoning and performance in game-based environments, addressing the challenges of optimizing their decision-making processes. Techniques such as fine-tuning on gamespecific data, incorporating reinforcement learning, or integrating external reasoning frameworks enable LLMs to better navigate complex game scenarios, from perfect-information games like chess to imperfect-information settings like poker. These approaches aim to improve LLMs' ability to anticipate opponents' moves, optimize strategies, and adapt to dynamic game states. This subsection highlights the advancements in enabling LLMs to exhibit sophisticated, goal-oriented reasoning in competitive and cooperative game contexts.

2.2.1 Stimulating Reasoning with Advanced Prompting. Advanced prompt techniques are often used to improve LLMs' reasoning capability [89-91]. Several studies have designed more complex prompts for specific game tasks. Wang et al. [13] developed the Recursive Contemplation (ReCon) framework to enhance the strategic reasoning of LLMs in Avalon. By prompting LLMs to employ first- and second-order perspective-taking, this framework mitigates common failures like deceptive behavior. Similarly, Duan et al. [60] proposed a method where LLMs predict future moves in multi-turn games, improving their ability to anticipate opponents' strategies. Additionally, Zhang et al. [16] advanced LLMs' reasoning through ùêæ -level rationality, which enhances multi-level thinking and significantly increases their win rates in competitive settings. These findings suggest that recursive reasoning can substantially improve LLMs' strategic capabilities. Kempinski et al. [92] proposed algorithms that guide LLMs to iteratively refine their action choices by simulating game outcomes in self-play. These methods align directly with the themes discussed in this section, such as recursive reasoning and advanced prompting techniques for strategic capabilities. Beyond recursive approaches, advanced prompting techniques also focus on integrating feedback, human-like reasoning, and Theory of Mind. Fu et al. [18] demonstrated that LLMs can autonomously improve negotiation strategies through self-play, leveraging in-context learning from AI feedback where a critic LLM provides structured critiques to a player LLM. In the context of multi-agent mystery games, Wu et al. [20] enhanced agents' information gathering and logical reasoning by incorporating advanced prompting engineering, allowing them to decipher complex scenarios more effectively. Building on this, Guo et al. [93] introduced 'Suspicion-Agent,' which utilizes prompt engineering to harness GPT-4's high-order Theory of Mind capabilities, enabling it to understand and intentionally influence opponents' behavior in imperfect information games. Furthermore, Abdelnabi et al. [69] employed systematic zero-shot Chain-of-Thought (CoT) prompting to enable LLM agents to successfully negotiate in multi-agent games, highlighting the role of explicit reasoning steps. In a similar vein, Yim et al. [42] proposed a ToM planning technique for LLM agents to adapt their strategies in cooperative games under imperfect information, demonstrating how specific prompts can simulate an understanding of other agents' beliefs. Lastly, Gandhi et al. [63] showed that systematically generated few-shot CoT examples can enable LLMs to achieve strategic reasoning that generalizes across diverse game structures and objectives. These diverse methods underscore the power of carefully designed prompts in stimulating sophisticated reasoning in LLMs across various game settings.

2.2.2 Developing Task-Specific Ability with Training. Complementing cognitive frameworks, novel training paradigms leverage self-play and AI feedback to overcome data limitations and improve strategic adaptability. Fu et al. [18] employed iterative self-play with AI-generated feedback to refine negotiation strategies in dynamic environments with hidden goals. Guo et al. [94] introduced self-supervised learning with auxiliary state-derived rewards, enabling mastery of complex games like Hanabi without human data. Kwon et al. [95] used LLMs as intrinsic reward designers, reducing

Manuscript submitted to ACM

dependency on human-engineered reward functions for reinforcement learning. Complementing these, Zhang et al. [96] implemented policy-level reflection via evolutionary algorithms, enabling LLM agents to self-optimize strategies without parameter retraining. Similarly, Wang et al. [78] tackled data scarcity in sensitive domains through algorithmic synthesis of statistically faithful game-theoretic scenarios using non-parametric copula simulators. These methods collectively enhance LLMs' ability to develop robust strategies through experiential learning.

Further advancements include an LLM-based framework by Wei et al. [97], which automates reward function discovery for reinforcement learning in cooperative platoon coordination, initializing rewards via a chain of thought and iteratively optimizing them through an evolutionary module based on training feedback. Suzuki and Arita [98] proposed an evolutionary model where LLMs are instructed with high-level psychological and cognitive character descriptions as 'genes' to simulate human behavior in game-theoretical scenarios, evolving the population through selection based on average payoff and mutation of these linguistic trait descriptions. Feng et al. [48] introduced ChessGPT, which bridges policy learning and language modeling by integrating historical policy data and natural language analytical insights from chess games, training models on this large-scale combined dataset. Liao et al. [99] demonstrated the efficacy of language model self-play in non-zero-sum games by fine-tuning LLMs over multiple rounds of filtered behavior cloning, showing substantial improvements in task reward. Wang et al. [78] empowered LLMs in decision games through targeted post-training by designing data synthesis strategies to curate extensive offline datasets from games like Doudizhu and Go, then developing techniques to effectively incorporate this data into LLM training. Jin et al. [100] proposed an RL-instructed language agent framework for One Night Ultimate Werewolf, where a discussion policy is trained by reinforcement learning to determine strategic discussion tactics, guiding the LLM's communication based on game context. Zhuang et al. [49] introduced POKERBENCH, a benchmark for evaluating poker-playing abilities, demonstrating marked improvements in LLM performance after fine-tuning using structured 'Few-Shot Prompts' that provide detailed game scenarios for strategic decision-making. Huang et al. [17] presented PokerGPT, an end-to-end solver for multi-player Texas Hold'em that fine-tunes a lightweight LLM using reinforcement learning from human feedback based on textual records from real games. Finally, Yang and Berthellemy [71] enhanced LLMs in noncooperative games by integrating a tree of thoughts and a multi-agent framework, where game-solving is decomposed into incremental tasks and an automated fine-tuning process optimizes performance by ranking query-response pairs based on game outcomes.

2.2.3 Integrating Auxiliary Modules and Tools. Beyond direct prompting and training, integrating auxiliary modules and external tools is crucial for enhancing LLMs' game-playing ability by providing structured knowledge or specialized reasoning [101, 102]. For instance, Yim et al. [42] integrated a Theory of Mind planning technique with an external tool in Guandan, using prompts for strategic adaptation based on game context. Similarly, Xia et al. [19] enhanced bargaining with OG-Narrator, which employs prompts to structure offers and translate them into natural language. Several works focus on infusing logical or strategic frameworks: Watanabe et al. [65] improved Werewolf agents by embedding explicit logical structures via prompts for deductive reasoning. Wu et al. [20] used advanced prompting within a multi-agent framework to boost information gathering and logical reasoning in mystery games. Hua et al. [81] developed a game-theoretic agent workflow for negotiation, guiding LLM decisions with specific prompts based on game theory. Lan et al. [83] utilized a multi-agent system for Avalon, where the system prompts directed agents' gameplay and social behavior analysis. Other approaches include Hu et al. [50]'s POK√âLLMON, which uses prompts to enable in-context reinforcement learning and knowledge-augmented generation for Pok√©mon battles. Guan et al. [73] enhanced AI Diplomacy agents with a strategic planner that specifies sub-goals for long-term objectives. Lastly, the Manuscript submitted to ACM

STRIDE framework [103] integrates memory and specialized tools, with prompts enabling LLM agents to interact for rule adherence, planning, exploration, and opponent anticipation. These diverse integrations significantly bolster LLMs' strategic capabilities.
