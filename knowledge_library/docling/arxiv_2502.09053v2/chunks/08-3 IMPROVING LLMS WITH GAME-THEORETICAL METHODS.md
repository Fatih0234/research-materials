## 3 IMPROVING LLMS WITH GAME-THEORETICAL METHODS

Game-theoretical approaches have been more frequently utilized to describe LLMs' theoretical characteristics and to develop practical algorithms that improve their empirical outcomes. This section explores how principles and methodologies from game theory contribute to addressing key challenges in the development and optimization of LLMs. The discussion is then organized into four subsections, each corresponding to a critical challenge faced by LLMs: Subsection 3.1 addresses the challenge of interpretability . A line of work constructs cooperative game scenarios involving an LLM's input, training data, and internal components, and utilizes the Shapley value [104] to provide principled credit assignment for each contributing factor. Subsection 3.2 focuses on aligning general preferences . Nash Learning from Human Feedback (NLHF) formulates a minimax game between policies to capture intransitive preferences. Further extensions develop more efficient algorithms with provable theoretical guarantees and broader applicability. Subsection 3.3 discusses the challenge of heterogeneity in value alignment. Recent work integrates social choice theory with reinforcement learning from human feedback (RLHF), offering axiomatic frameworks for alignment and principled strategies to resolve diverse, potentially conflicting preferences. Subsection 3.4 investigates the issue of dynamic adaptation by modeling LLM development as a competitive game involving multiple co-evolving players. Table 2 summarizes the key challenges, core game-theoretical concepts, and corresponding game formulations.
