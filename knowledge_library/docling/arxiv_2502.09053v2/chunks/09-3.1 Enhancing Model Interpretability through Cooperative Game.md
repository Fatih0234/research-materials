## 3.1 Enhancing Model Interpretability through Cooperative Game

Interpreting the behavior of deep learning models is a highly desirable objective, as it enables researchers to gain insights into how models function, thereby facilitating further improvements and providing guarantees of reliability [116-118]. Such an interpretability challenge becomes more critical for LLMs, due to their massive scale, often comprising billions or even trillions of parameters and trained on vast datasets. The SHAP framework [119] adopts the Shapley value [104]

Manuscript submitted to ACM

Table 2. Application of Game-theoretic Methods for Improving Large Language Models.

| Challenge                                                                                                                                                                         | Core Game-theoretic Concepts                                                                                                                                                      | Game Formulation                                                                                                                                                                                                                                                                                         |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Model Interpretability §3.1                                                                                                                                                       | Cooperative Games (Shapley Value)                                                                                                                                                 | Players: LLM components (tokens, data points, layers, heads). Goal: Cooperate to generate the model's output.                                                                                                                                                                                            |
| Spotlight: Provides a principled method for credit assignment, enabling input attribution, data valuation, and model pruning. Examples: TokenSHAP, Data Shapley, SV-NUP [105-107] | Spotlight: Provides a principled method for credit assignment, enabling input attribution, data valuation, and model pruning. Examples: TokenSHAP, Data Shapley, SV-NUP [105-107] | Spotlight: Provides a principled method for credit assignment, enabling input attribution, data valuation, and model pruning. Examples: TokenSHAP, Data Shapley, SV-NUP [105-107]                                                                                                                        |
| Aligning General Preferences §3.2                                                                                                                                                 | Minimax Games (Nash Equilibrium)                                                                                                                                                  | Players: Two policies competing to be preferred by a human or learned oracle. Goal: Find a stable policy that cannot be consistently defeated (a Nash Equilibrium).                                                                                                                                      |
| Spotlight: Overcomes limitations of scalar reward models, enabling robust alignment with complex (e.g., intransitive) preferences. NLHF, SPO, DNO, MPO [23, 108-110]              | Spotlight: Overcomes limitations of scalar reward models, enabling robust alignment with complex (e.g., intransitive) preferences. NLHF, SPO, DNO, MPO [23, 108-110]              | Spotlight: Overcomes limitations of scalar reward models, enabling robust alignment with complex (e.g., intransitive) preferences. NLHF, SPO, DNO, MPO [23, 108-110]                                                                                                                                     |
| Capturing Preference Heterogeneity §3.3                                                                                                                                           | Social Choice Theory & Cooperative Bargaining                                                                                                                                     | Players: User subgroups or annotators with diverse values. Goal: Aggregate preferences or negotiate outcomes according to formal fairness axioms.                                                                                                                                                        |
| Spotlight: Moves beyond monolithic alignment to design equitable systems that respect minority views and handle conflicting values.                                               | Spotlight: Moves beyond monolithic alignment to design equitable systems that respect minority views and handle conflicting values.                                               | Spotlight: Moves beyond monolithic alignment to design equitable systems that respect minority views and handle conflicting values.                                                                                                                                                                      |
| Dynamic Adaptation §3.4                                                                                                                                                           | Competitive Self-Play, Stackelberg Games, & Bilevel Optimization                                                                                                                  | A co-evolving game with two primary forms, mapping to the section's structure: • Evolving Data (§3.4.1): A generator model creates challenging data for a critic or its past self. • Evolving Rewards (§3.4.2): A "leader" policy optimizes against a "follower" reward model that finds its weaknesses. |
| Spotlight: Replaces static components with a dynamic process that prevents reward hacking and enables continuous, autonomous improvement. SPIN, STA-RLHF, Decoding Game [113-115] | Spotlight: Replaces static components with a dynamic process that prevents reward hacking and enables continuous, autonomous improvement. SPIN, STA-RLHF, Decoding Game [113-115] | Spotlight: Replaces static components with a dynamic process that prevents reward hacking and enables continuous, autonomous improvement. SPIN, STA-RLHF, Decoding Game [113-115]                                                                                                                        |

to quantify the contribution of individual components of a deep learning model. Inspired by this, recent work considers formulating cooperative games in different stages of LLMs, in which each player represents a semantically meaningful part, such as individual tokens in a prompt, specific datasets used in training, or particular layers within the LLM architecture. By choosing appropriate performance metrics, the researchers apply the Shapley value to measure how each player contributes to the overall performance, offering a theoretically grounded measure of interpretability. However, the exponential complexity of exact Shapley value calculation necessitates the development of efficient and accurate approximation techniques, which has become a central theme in this line of research.

3.1.1 Input Attribution. Several studies focus on quantifying the importance of an LLM's input components, from entire prompts to individual tokens. At the prompt level, Liu et al. [120] applied the Shapley value to multi-prompt learning. They proposed a learning-based approach to predict the value of each prompt, facilitating more effective prompt engineering by identifying prompts that enhance performance. At a finer granularity, TokenSHAP [105] and TextGenSHAP [21] model individual tokens as players in a cooperative game to attribute the model's output to specific Manuscript submitted to ACM

parts of the input. TextGenSHAP [21] introduces speculative decoding and in-place encoder resampling to make tokenlevel attribution for long-context inputs computationally tractable. Using a similar method, Mohammadi [121] uncovered a 'token noise' phenomenon: LLM decisions are disproportionately affected by tokens with minimal semantic content, like invisible newline characters. Moving from token-level to document-level analysis, Ye and Yoganarasimhan [122] applied the Shapley framework to value source documents in LLM-generated summaries, a key challenge in RetrievalAugmented Generation systems. They introduced Cluster Shapley, which groups semantically similar documents, to overcome the computational expense. In their model, each source document is a player whose value is determined by its marginal contribution to the quality of the final summary.

3.1.2 Training Data Valuation. As LLMs are trained on various massive datasets, it is insightful to quantify how each dataset or data instance influences the model's performance. Data Shapley [106] models each training sample as a player and uses the Shapley value to evaluate its marginal contribution to the final model's performance across all possible training subsets. In the LLM context, this concept has been operationalized for both dataset refinement and dynamic learning. For dataset curation, the SHED framework [123] offers a scalable solution for instruction fine-tuning. It approximates Shapley values over clusters of training data to assemble an optimized dataset. The framework also demonstrates strong cross-model transferability, where data selected using a smaller model remains effective for larger ones, significantly reducing curation costs. Beyond static dataset curation, this principle extends to the dynamic process of RLHF. The SCAR framework [124] addresses the sparse reward problem in RLHF by modeling text segments as players in a cooperative game. By distributing the final reward among text units based on their Shapley value-approximated efficiently using Owen values-SCAR provides a dense, principled reward signal. Empirically, this method improves both convergence speed and final policy performance in tasks like summarization and instruction following.

3.1.3 Probing Internal Components. Beyond external inputs and data, Shapley-based methods are also applied to the internal components of LLMs. By conceptualizing architectural components such as layers and attention heads as 'players' in a cooperative game, researchers can assess the marginal contribution of each part to the model's overall performance. This abstraction provides not only interpretability insights but also practical guidance for model optimization and compression. One line of work evaluates the importance of entire layers. Zhang et al. [125] used a Shapley-based neighborhood sampling method to identify cornerstone layers , a small subset of layers whose removal causes a drastic performance collapse. Following this, Sun et al. [107] proposed the Shapley Value-based Non-Uniform Pruning (SV-NUP) framework. It employs an efficient Sliding Window-based Shapley Value (SWSV) approximation to assign pruning ratios based on layer contributions, achieving superior compression-performance trade-offs. Other studies focus on finer-grained components like attention heads. Held and Yang [126] applied Shapley Head Values (SHVs) to identify and remove interfering attention heads in multilingual models, achieving performance improvement without adding or retraining parameters. To make such fine-grained analysis computationally feasible, amortized methods have been introduced. Yang et al. [127] trained an auxiliary model to directly predict Shapley values, achieving speedups while ensuring deterministic, stable explanations. Fekete and Bjerva [128] used SHVs and clustered the resulting value vectors to reveal how different attention heads specialize in processing specific morphosyntactic phenomena. Furthermore, Yang et al. [129] proposed a benchmark and evaluation metrics based on Shapley value to evaluate how each module, such as planning, reasoning, action execution, and reflection, contributes to the reasoning improvement of an LLM.
