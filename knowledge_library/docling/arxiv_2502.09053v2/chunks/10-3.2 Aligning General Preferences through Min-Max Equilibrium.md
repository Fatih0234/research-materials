## 3.2 Aligning General Preferences through Min-Max Equilibrium

Aligning LLMs with general human preferences is a core challenge in the deployment of responsible, reliable language models. Traditional RLHF typically relies on reward models built upon the Bradley-Terry (BT) assumption, which imposes a strict transitive and scalar reward structure [130, 131]. However, human preferences are often stochastic and intransitive, which are poorly modeled by BT-style assumptions [23, 108]. Recent research reconceptualizes alignment as a two-player min-max game between policies and proposes a series of novel algorithms based on Nash equilibrium, self-play, and preference optimization without explicit reward modeling. These approaches aim to directly capture complex, expressive feedback and offer both theoretical guarantees and empirical improvements.

3.2.1 Preference Optimization. The foundational idea of NLHF [23] is to model alignment as a two-player zero-sum game, where each policy aims to outperform the other based on a learned preference function. This approach does not need reward modeling and allows convergence toward a Nash equilibrium policy under general preferences. Building on NLHF, Self-Play Preference Optimization (SPO) [108] proposes a minimalist yet generalizable framework. SPO treats trajectory comparison as a symmetric game and uses self-play to train a policy against itself using win-rate scores, achieving robustness to non-Markovian and intransitive preferences. Self-Play Preference Optimization for LLMs (SPPO) [132] adapts SPO for practical LLM fine-tuning by introducing a new squared-loss objective. To address sample efficiency and training stability, Direct Nash Optimization (DNO) [109] introduces a batched, on-policy, regression-based objective. DNO combines the scalability of contrastive learning with Nash theoretical soundness and is more empirically efficient. Similarly, Iterative Nash Policy Optimization (INPO) [133] proposes a no-regret online learning framework that avoids costly win-rate estimation by directly minimizing a surrogate loss over a preference dataset. Furthering this direction, Ye et al. [134] provided a theoretical study of KL-regularized RLHF under a general preference oracle, completely bypassing the BT model. They formulate alignment as a minimax game and propose distinct algorithms for offline (Pessimistic Equilibrium Learning, PELHF) and online (Optimistic Equilibrium Learning, OELHF) settings, both with finite-sample theoretical guarantees. The learnability of the KL-regularized NLHF is further verified in the work by Ye et al. [135].

3.2.2 Theoretical Advancements in Convergence and Efficiency. Further studies are focusing on achieving provably efficient and convergent training dynamics for NLHF. Several recent works observe oscillatory behavior, high variance, and slow convergence in preference-based self-play. Nash Mirror Prox (NashMP) [136] addresses these issues by leveraging the Mirror Prox method in online Nash Learning from Human Feedback. NashMP achieves last-iterate linear convergence to the ùõΩ -regularized Nash equilibrium, with convergence rates that are independent of action space size. Extragradient Preference Optimization (EGPO) [137] extends this idea, providing both linear convergence for regularized games and polynomial convergence for unregularized settings. Magnetic Preference Optimization (MPO) [110] is designed to converge to the Nash equilibrium of the original, non-regularized game. It achieves this by periodically updating its reference 'magnetic' policy to the equilibrium of the previous regularized step, guiding the policy sequence toward the unregularized objective. Beyond theoretical convergence, Wang et al. [138] proposed TANPO (Two-Agent Nash Policy Optimization) to improve sample efficiency and exploration-exploitation balancing. In TANPO, the 'min-player' is incentivized to explore via an exploration bonus, thereby generating more diverse and informative training data for the 'max-player'. Other work stabilizes training through regularization. Tang et al. [139] introduced Regularized Self-Play Policy Optimization (RSPO) that integrates forward and reverse KL divergence regularization into self-play, allowing for tunable trade-offs between response length, win-rate, and diversity. Alami et Manuscript submitted to ACM

al. [140] further explored the dynamics of KL-based regularization in self-play, showing that geometric mixing of the base and reference policies improves performance stability. They also highlight how fictitious play helps smooth the training trajectory and prevent performance oscillations, which helps prevent training collapse and boost generalization across benchmarks.

3.2.3 Limitations of Game-Theoretic Alignment. Although game-theoretic methods for alignment provide flexibility and robustness to preference inconsistencies, there are foundational critiques that raise concerns about the sufficiency of preferences for full value alignment. Sun et al. [141] revisited the foundational Bradley-Terry model, arguing that while it offers order consistency, it may not be necessary or optimal. They advocate for classification-based alternatives and emphasize the role of annotation sparsity and structure in reward modeling quality. Going deeper, Shi et al. [142] investigated theoretical limits of game-theoretic alignment, demonstrating fundamental limits to preference matching. They proved that no smooth payoff mapping can guarantee an equilibrium solution that precisely matches a target preference profile, nor that such an equilibrium is unique. They further showed that achieving other desirable properties, such as Smith consistency, imposes strict structural constraints on the game, requiring it to be equivalent to a symmetric zero-sum game. From a philosophical perspective, Zhi et al. [143] challenged the entire preferentist paradigm. They argued that preferences lack the semantic richness and context-dependence of human values, and thus fail as alignment targets. They called for a reframing of alignment to be based on normative standards tailored to AI roles, negotiated across stakeholders, rather than preference aggregation.
