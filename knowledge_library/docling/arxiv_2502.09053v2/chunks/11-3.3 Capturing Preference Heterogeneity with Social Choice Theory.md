## 3.3 Capturing Preference Heterogeneity with Social Choice Theory

Another formidable challenge in aligning large language models with human preferences is the heterogeneity of human preferences themselves. Conventional alignment methods, which often rely on learning a single, scalar reward function, overly prefer the majority's point and fail to handle this heterogeneity [144]. To address this problem, game theory and social choice theory offer both a formal language to diagnose the core difficulties of this problem and a constructive paradigm for developing more robust and equitable alignment algorithms.

3.3.1 An Axiomatic Social Choice Perspective. Recent work has recognized the limitations of RLHF by formally mapping the alignment problem to the field of social choice theory . This perspective reveals that many challenges are not merely technical but are rooted in fundamental paradoxes of collective decision-making. Mishra [145] showed that preference aggregation in AI is subject to classic impossibility theorems from social choice. By mapping AI alignment to this setting, he demonstrated that any aggregation rule inevitably violates at least one of Arrow's axioms-such as unanimity, independence of irrelevant alternatives, or non-dictatorship-meaning no single 'fair' preference aggregator can exist. Complementing this, Dai and Fleisig [146] formalized the mapping between RLHF and social choice, enabling a deeper critique. For example, it has been shown that canonical RLHF methods based on the Bradley-Terry model are mathematically analogous to the Borda count voting rule [147], where an option's score is derived from the sum of its pairwise 'wins.' This connection highlights how RLHF implicitly adopts a specific, and not always desirable, social choice function. This axiomatic lens also provides a powerful tool for critiquing current alignment mechanisms by testing them against formal properties. Ge et al. [22] evaluated RLHF algorithms and found they violate fundamental axioms like Pareto Optimality and Pairwise Majority Consistency. Extending this critique, empirical work by Hosseini and Khanna [148] confirms this misalignment, showing that LLMs often default to welfare-maximization in resource allocation tasks, violating human concepts of distributive fairness like equitability. Procaccia et al. [149] identified a critical vulnerability to 'approximate clones'-semantically similar responses that trick MLE-based methods by artificially inflating an Manuscript submitted to ACM

option's representation in the data; they showed these methods fail the axiom of clone-independence and propose a weighted MLE solution to correct it. Extending the critique to utility, GÃ¶lz et al. [150] introduced the concept of 'distortion' as the worst-case ratio of optimal social welfare to the welfare achieved by an RLHF policy, proving that for methods like RLHF, this distortion can grow exponentially with preference intensity, and can even be unbounded under certain data sampling conditions. In contrast, Xiao et al. [151] offered a reconciliation, explaining why these theoretical failures don't always break RLHF in practice. They demonstrated that under a common and empirically plausible condition-where each response pair is evaluated by at most one annotator-the complex preference cycles needed to trigger many axiomatic violations rarely form, thus preserving properties like Condorcet Consistency. This highlights a central tension in the field: the clash between theoretical impossibility results and the empirical effectiveness of existing methods under specific, practical conditions.

Building on these insights, researchers have proposed new axioms and normative frameworks to guide the development of more principled alignment systems . Position papers by Conitzer et al. [152] and Zhang et al. [153] argue for embedding principles from social choice and mechanism design directly into the alignment process. The latter introduces the Incentive Compatibility Sociotechnical Alignment Problem (ICSAP), which uses mechanism design to create protocols where stakeholders are incentivized to reveal their true preferences. This concern with strategic incentives is formalized by Wu et al. [154], whose game-theoretic analysis of competing data providers shows that strategic exaggeration of preferences is an almost inevitable Nash Equilibrium, highlighting a fundamental source of data corruption. To build more legitimate systems, others are designing new elicitation processes, such as the 'Moral Graph' proposed by Klingefjord et al. [155], which uses a structured dialogue to construct a graph of human values capable of surfacing nuanced and expert opinions. Another direction focuses on axioms of representation. Kim et al. [156] introduced a framework grounded in two new axioms: Population-Proportional Representation (PPR) and Population-Bounded Robustness (PBR), achieved by learning a latent mixture of preference groups. This call for proportional representation is echoed by Peters [157], who argues for its broad applicability in AI to mitigate the 'tyranny of the majority' in contexts ranging from RLHF to the aggregation of LLM outputs. Complementing this, Qiu [158] connects alignment to statistical learning theory, proposing axioms that ensure a preference model learned from a sample of users will generalize fairly to the entire population.

3.3.2 Developing Practical Algorithms. The theoretical shortcomings of a single, monolithic reward function, particularly its failure to satisfy axioms like Pareto Optimality [22] and its tendency towards high distortion [150], motivate a new class of practical algorithms designed to explicitly handle preference diversity. Empirical work confirms this necessity. For example, Fleisig et al. [144] treated annotator disagreement as a crucial signal, developing a model to predict the ratings of a text's targeted group to identify instances where the majority opinion is 'wrong'. Shirali et al. [147] provided theoretical proof that Direct Preference Optimization (DPO) fails to find the utilitarian optimum with heterogeneous users, instead implicitly optimizing for the Borda count and thus being sensitive to data sampling distribution. This work also uncovers a trade-off: achieving a consistent estimate of the optimal policy requires discarding data with annotator disagreement, thereby sacrificing sample efficiency. Directly addressing this, Cheng et al. [159] proposed Vote-based Preference Optimization (VPO), which leverages the quantitative vote counts in preference data. By modeling preference strength, VPO can distinguish between clear consensus and controversial opinions, leading to more stable and effective alignment. This issue of inconsistency is underscored by Liu et al. [160], who show that Condorcet cycles ( e.g., A &gt; B, B &gt; C, C &gt; A) are a near-certainty in diverse preference data. Since a single scalar reward function cannot represent such cycles, this proves its structural inadequacy and motivates algorithms capable of producing mixed or pluralistic outputs. Manuscript submitted to ACM

One major line of work involves learning distinct reward models for different preference clusters and then aggregating their outputs based on principles from social choice. Chakraborty et al. [111] proposed MaxMin-RLHF, which implements the Rawlsian 'max-min' social welfare function by learning a mixture of reward models for latent subgroups and maximizing the utility of the worst-off group. Similarly, Chen et al. [161] used a mixture-of-experts approach to learn latent preference prototypes, while Park et al. [162] used spectral clustering to identify taste-based groups before training group-specific reward models. Providing a theoretical backing, Zhong et al. [163] used meta-learning to extract group-specific rewards and analyze the sample complexity of aggregating them using different social welfare functions.

Other approaches innovate on the aggregation mechanism itself, moving beyond the 'learn-then-aggregate' paradigm. Alamdari et al. [164] proposed aggregating at the policy level, where multiple policies trained on individual preferences have their actions combined at inference time using voting rules. The viability of such voting mechanisms is empirically explored by Yang et al. [165], who find that LLM collective decisions are sensitive to voting protocols and exhibit biases. While using 'personas' can improve alignment with human choices, it reveals a difficult trade-off between alignment accuracy and preference diversity. Halpern et al. [166] introduced Pairwise Calibrated Rewards, an ensemble method that learns a distribution of reward functions calibrated to match the proportion of human annotators holding a given preference, thus preserving pluralism. Drawing from cooperative game theory, Mushkani et al. [112] proposed Negotiative Alignment, a multi-agent framework where agents representing stakeholder groups use bargaining protocols to reach collective decisions. The potential of such negotiative approaches is highlighted by behavioral experiments from Wang et al. [167], who show that an 'imperfectly fair' LLM agent-one that engages in human-like strategic communication-can overcome the typical 'machine penalty' and foster cooperation in social dilemmas, suggesting the power of dynamically negotiated alignment.

The integration of social choice theory has transformed the understanding of AI alignment with heterogeneous preferences, shifting the focus from a single reward function to a rich tapestry of methods that embrace diversity. Future work will likely focus on bridging the gap between theoretically ideal but computationally expensive mechanisms, such as those involving negotiation or full pairwise calibration, and scalable algorithms that can be deployed in real-world systems, while also addressing the crucial challenge of incentive compatibility in data collection.
