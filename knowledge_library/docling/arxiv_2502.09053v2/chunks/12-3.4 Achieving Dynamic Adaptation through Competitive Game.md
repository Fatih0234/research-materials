## 3.4 Achieving Dynamic Adaptation through Competitive Game

Traditional LLM training pipelines often rely on two fixed components: a static, pre-collected dataset for supervised fine-tuning, and a static reward model trained on human preferences. This static nature imposes fundamental limitations: a model's capabilities are constrained by the fixed data it has seen, and an evolving policy can learn to exploit loopholes in the fixed reward model, leading to a brittle alignment known as 'reward hacking' [168]. Game theory provides a robust alternative by recasting LLM training and deployment as a dynamic, strategic, and interactive process.

3.4.1 Overcoming Static Datasets. Static datasets represent a finite and fixed data distribution, and collecting humanannotated data is expensive. This restricts a model's ability to generalize to novel scenarios. To address this, a prominent line of research uses self-play, where an LLM iteratively generates its training data. The SPIN framework [113], for example, has the LLM play against previous versions of itself. In each round, the model generates new responses and learns to distinguish these from a seed set of human-annotated data, effectively creating a curriculum of progressively higher-quality data without needing continuous human feedback. This concept of self-alignment is also explored by Azarafrooz et al. [169], who propose an online, two-player game that can be viewed as a simplified form of DPO operating without any human preference data, using Nash-learning and adaptive feedback to enable autonomous improvement.

Manuscript submitted to ACM

Similarly, Chu et al. [170] tackled the data scarcity and noise problem simultaneously with their Stackelberg Game Preference Optimization (SGPO). This framework uses self-annotation to create worst-case preference data efficiently, enhancing the robustness of the model.

Adversarial games are also used to specifically target and patch a model's weaknesses. In this paradigm, one agent's goal is to generate inputs that the other agent finds difficult. Zheng et al. [171] let an adversarial agent generate prompts that expose the weaknesses of a defensive agent. They also introduce an innovative diversity constraint to prevent the adversary from collapsing to a narrow set of attacks. The utility of such adversarial dynamics is profound; for instance, self-play in an 'Adversarial Taboo' game is more effective for enhancing LLM reasoning than standard supervised fine-tuning [159]. Ye et al. [172] proposed a creator-solver dynamic where a 'creator' model strategically crafts new prompts by aiming to maximize the 'solver' model's regret. Similarly, Liu et al. [173] used online self-play to co-evolve attacker and defender agents with mechanisms like a 'Hidden Chain-of-Thought' to enhance strategic planning.

Another strategy involves using game dynamics not just to generate prompts, but to create fine-grained, step-bystep feedback, which is notoriously difficult to obtain from humans. Chen et al. [174] introduced the Self-Play Critic (SPC), where a 'sneaky generator' deliberately produces flawed reasoning steps to challenge a 'critic' model. This forces the critic to evolve its assessment capabilities, ultimately achieving performance that surpasses existing process reward models without manual step-level annotation. Zhou et al. [175] proposed a two-player online game between a 'proposer' that generates a response and a 'reflector' that provides immediate, dense feedback by critiquing it. This dynamic interaction generates rich, supervisory signals that are absent in static feedback datasets. Xie et al. [176] used a Stackelberg game framework to learn detoxification from non-parallel data. The insight of their work is the finding that the success of such methods is often highly dependent on the accuracy of the feedback signal, such as the toxicity classifier.

3.4.2 Evolving the Reward Signal. A static reward model (RM), no matter how well-trained, is inevitably exploitable. As the LLM policy improves, it can discover and exploit edge cases or loopholes in the RM's fixed reward function, a phenomenon known as reward hacking [168]. To counter this, researchers have reframed the interaction between the LLM and the RM as a dynamic game where the reward signal co-evolves with the policy. The Adversarial Preference Optimization (APO) framework [177] lets the LLM and RM update in a min-max game. The RM is trained to find outputs where the current LLM policy is poorly calibrated, and the LLM is then trained to improve on these adversarial examples. Its update also incorporates a KL divergence regularization term, ensuring it remains faithful to the original human preferences while adapting to the LLM's new distribution. This dynamic can be formalized using frameworks from game theory, like bilevel optimization and Stackelberg games. STA-RLHF [114] models the interaction as a Stackelberg game where the LLM policy is the 'leader' and the preference model is the 'follower.' The policy makes the first move, and the preference model must best respond to it, forcing the policy to learn an alignment that is robust to an adaptive reward signal. Shen et al. [178] considered a more principled algorithmic framework, introducing a provably convergent first-order algorithm for such bilevel problems using a penalty-based method. Concurrently, Chakraborty et al. [179] developed a framework that, by precisely modeling the objective's dependency on policy-generated trajectories, improves the sample efficiency and mitigates reward over-optimization. These principled approaches are also being extended to more complex scenarios, such as Contextual Bilevel RL, which enables solving complex, real-world incentive alignment problems like tax design [180].

3.4.3 Other Game-Theoretic Dynamics. Game theory also provides novel frameworks for LLM alignment and generation that go beyond the data-reward dichotomy. One such area is cooperative and mixed-motive games. In the Manuscript submitted to ACM

COEVOLVE framework [181], an LLM is fine-tuned by interacting with a copy of itself in a sequential cooperative game. Complementing this, Liao et al. [99] provided empirical evidence that self-play is highly effective in non-zero-sum negotiation games. Beyond fine-tuning, evolutionary game theory is used to analyze emergent collective behaviors of LLM agent systems. Gemp et al. [182] formulated the natural language dialogue generation as a game process. By applying equilibrium solvers, the method equips LLM with more stable and rational conversational strategies.

Another line of work applies game theory to the decoding process. The Consensus Game [183] uses equilibrium search in a cooperative signaling game, where a Generator and Discriminator reconcile their predictions to find a consensus, leveraging no-regret learning to produce truthful and coherent outputs. The Peer Elicitation Games (PEG) [184] extends this idea by replacing a single agent discriminator with a multi-agent peer elicitation process. This equilibrium-seeking principle has been successfully applied to complex, embodied AI tasks; for instance, Yu et al. [185] integrated it into a vision-language navigation system to reduce model hallucinations. On the other hand, Chen et al. [115] proposed the Decoding Game, a theoretical framework that reimagines text generation as a zero-sum game against an adversarial 'Nature.' Their analysis shows that this framing provides the first theoretical justification for the empirical success of heuristic methods like Top-k sampling. Zhang et al. [186] reframed decoding as a Bayesian game between two internal LLM agents: a Generator and a Verifier. The two agents strategically interact to reach a 'Separating Equilibrium,' a stable state where the verifier can reliably distinguish high-quality outputs from low-quality ones. This strategic decoding process acts as a powerful, training-free verification mechanism. Game dynamics can also steer generation at inference time, for instance by using solvers like Counterfactual Regret Minimization to guide dialogue toward less exploitable strategies [187], or using Nash equilibrium concepts to dynamically control text attributes [188].

Besides, game-theoretic mechanisms are being used to improve the efficiency of the alignment process. Zhang et al. [189] introduced an auction-based mechanism for collecting preference data, using principles from mechanism design to improve cost-efficiency. Xie et al. [190] introduced Efficient Coordination via Nash Equilibrium (ECON), which recasts multi-LLM coordination as an incomplete-information game seeking a Bayesian Nash equilibrium. This framework allows each LLM to independently select responses based on its beliefs about co-agents, achieving a tighter regret bound and outperforming existing multi-LLM approaches.
