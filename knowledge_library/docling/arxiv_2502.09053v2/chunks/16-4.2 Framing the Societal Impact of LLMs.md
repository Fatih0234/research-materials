## 4.2 Framing the Societal Impact of LLMs

Beyond their immediate technical and economic applications, LLMs are profoundly reshaping broader societal structures. The widespread adoption of generative models introduces new dynamics of competition, cooperation, and strategic interaction across domains such as knowledge production, regulatory governance, platform design, and content labor markets. This subsection surveys game-theoretic research that models these macro-level effects, highlighting how emergent equilibria capture the unforeseen consequences of GenAI proliferation.

4.2.1 Strategies of Autonomous LLM Agents. A critical shift in modeling involves recognizing that GenAI systems can function as goal-directed, strategic agents. This perspective moves beyond viewing LLMs as passive tools, instead understanding them as economic actors whose implicit preferences, shaped by their training objectives, may misalign with human welfare. Immorlica et al. [47] formalized this by treating GenAI as a consultant with a payoff function based on perceived helpfulness. They contend that even minor divergences between the AI's goals and user welfare can dramatically alter equilibria, leading to suboptimal outcomes such as overconfidence or persuasive bias, even when factual accuracy is maintained.

Manuscript submitted to ACM

This strategic agency extends beyond theory. Taitler et al. [210] modeled 'selective response,' where a GenAI purposefully withholds answers to niche queries to direct users toward human forums. This behavior generates fresh training data, establishing a beneficial data flywheel for the model, but it trades immediate user utility for long-term system optimization. Supporting these concerns, empirical work by Sabour et al. [211] demonstrates that GenAI agents can manipulate human choices by strategically framing options, illustrating that algorithmic influence spans from recommendation to active persuasion. Collectively, these studies underscore the risks of deploying AI systems whose optimization criteria are not robustly aligned with human autonomy.

4.2.2 Transformation in Data and Content Ecosystems. The proliferation of GenAI has fundamentally altered incentives for human creators in data sharing and content production. Keinan et al. [212] framed this challenge as a Prisoner's Dilemma, where individual creators must choose between sharing content for model training, risking competition from AI, or withholding it to preserve exclusivity. While cooperation yields higher platform-wide utility, each player's dominant strategy often involves defection, leading to suboptimal equilibria. This strategic tension can result in declining data quality unless carefully designed incentives, such as revenue sharing or content licensing schemes, are implemented.

Complementary research shows how these dynamics affect content diversity and creator viability. Gao et al. [213] demonstrated that while GenAI can increase average content quality, it also causes price erosion and reduces diversity due to output standardization, potentially displacing human creators from the market. The nature of this competition is further explored by Yao et al. [26], who modeled human-AI interaction as a generalized Tullock contest for user attention. In this setting, players expend effort ( e.g., producing engaging content) to win a probabilistic share of user attention, which translates into monetization or visibility. Their results indicate that stable coexistence is possible, with GenAI initially eliminating the least efficient human creators. Furthermore, the rapid evolution of GenAI imposes a significant adaptive burden on humans. Esmaeili et al. [214] demonstrated that determining an optimal response strategy to a constantly evolving AI can be computationally intractable (NP-hard), formalizing the risk that human creators may be unable to adapt effectively in fast-moving content markets.

4.2.3 System-Level Equilibria and Regulatory Challenges. The strategic interactions among users, creators, and platforms can aggregate into unforeseen system-level equilibria and novel regulatory challenges. A stark example of emergent dysfunction is offered by Taitler et al. [215], who adapted Braess's Paradox to GenAI deployment. In their model, a GenAI platform optimizing for revenue eventually erodes the quality of a human knowledge forum ( e.g., Stack Overflow) by drawing away users. This degrades the quality of future training data, leading to a long-term outcome where all users are worse off, despite the GenAI's short-term utility.

Such emergent failures underscore the difficulty of effective governance. Laufer et al. [216] showed how wellintentioned but fragmented policies can backfire. For instance, imposing safety standards only on downstream AI fine-tuners may incentivize upstream developers to underinvest in safety, resulting in lower overall safety levels. This highlights the need for holistic regulation that aligns incentives across the entire development pipeline. Other systemic shifts include the 'flattening' of supply chains, as modeled by Ali et al. [217], where GenAI disintermediates gatekeepers, posing risks of surplus extraction and content homogenization. Finally, Xie et al. [218] examined the long-term dynamics between algorithmic decision-makers and humans, showing that misaligned incentives can lead agents to abandon self-improvement in favor of manipulation or exit, emphasizing the need for long-term alignment to achieve positive-sum outcomes.
