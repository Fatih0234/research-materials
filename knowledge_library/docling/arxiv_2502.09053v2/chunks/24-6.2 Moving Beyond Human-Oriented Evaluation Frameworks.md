## 6.2 Moving Beyond Human-Oriented Evaluation Frameworks

Current Landscape: One of the predominant approaches to evaluating the strategic capabilities of LLMs is through metrics designed to capture strategic optimality, such as Nash regret [52, 53, 56] or ùëò -level rationality [16]. Interestingly, empirical observations reveal that LLMs often exhibit prosocial behaviors (Subsection 2.1), frequently eschewing self-optimal strategies. One possible reason is that the RLHF has shaped LLM behavior toward more altruistic responses. Moreover, the intrinsic training objective of LLMs, next-token prediction, diverges substantially from the principles underlying these evaluation metrics. As a result, it remains unclear whether success or failure on such metrics reliably reflects the true reasoning or strategic capacity of LLMs. This disconnect raises important questions about the adequacy and appropriateness of current evaluation paradigms.

Future Directions: Developing evaluation frameworks specifically tailored to neural network-based agents is a valuable future direction. Merely repurposing benchmarks originally designed for humans is insufficient for capturing the unique behaviors and limitations of LLMs. A meaningful starting point is the design of tasks that not only draw upon game-theoretic settings but also reflect the fundamental properties of LLM training. Consequently, evaluation metrics must also be custom-developed for LLMs, rather than borrowed wholesale from human cognitive testing. An initial approach may involve the use of subjective or qualitative measures. And we should strive toward evaluation metrics that are robust, interpretable, and operationally meaningful. These attributes are crucial for ensuring that our assessments of LLM capabilities remain rigorous and actionable.
