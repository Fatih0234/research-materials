## 8 Limitations and future work

This paper represents an initial exploration of multi-agent LLM systems as a tool for simulating human behavior and has several limitations. While the findings demonstrate that the proposed approach is feasible, there remain many open questions and challenges that need to be addressed.

This paper focused on prosocial cooperative behavior and used the public goods game (PGG) as a vehicle for many of the preliminary studies (Studies 1 and 2). Although the PGG closely mimics real-world scenarios like taxation and charitable donations, it has limitations. First, the outcomes of the PGG depend on its multiplication factor for increasing the public pot. Typically, a factor of 1.6 is used, but if the multiplication factor is lower, the potential benefit is reduced, which could alter the game's dynamics. We lack human data for all possible values of the multiplication factor for comparison. Additionally, there are many variants of the PGG that were not tested: for example, punishment is an important mechanism that was not explored, though it has been shown to replicate in other games like the Ultimatum Game. There are countless mechanisms to replicate, and the more games we replicate, the more confidence we can have that multi-agent LLM systems are consistent with human behavior. At this point, there is a growing body of evidence from AI, HCI, economics, and psychology suggesting that LLM-agents can capture much of human behavior; what remains is to identify and refine the mechanisms that enable successful simulation.

Our second study, along with other studies replicating multiple unpublished psychology experiments [15], indicates that LLMs are not simply echoing the results of previous papers - there is potential for LLMs to simulate human behavior in novel scenarios. It is unclear how LLMs possess these abilities since they don't have inherent motivations like humans. Perhaps they are using knowledge from past experiments and transferring principles learned from one context to new situations. An extreme hypothesis is that LLMs may not be relying on experimental knowledge at all but have genuinely understood aspects of human nature from their training data. However, the limits of these abilities are also uncertain. For any novel scenario in a simulation, we cannot be certain whether the LLM is capable of simulating the required behaviors. While previous work has shown that not all studies replicate, future research will

need to test the boundaries of when LLMs are more or less likely to simulate human behavior accurately.

Our studies primarily tested single interventions, but real-world scenarios are more complex, often involving multiple policies that can interact in unpredictable ways. For example, adding transparency might increase donations in the PGG, while negative priming might decrease them. If both policies are present, the outcome could vary depending on the specifics of the situation, and there may not be a definitive ground truth to compare against. In the future, more psychological experiments may need to be conducted to inform simulation design.

Much of the experimental evidence from the social sciences is based on Western, Educated, Industrialized, Rich, and Democratic (WEIRD) populations. If LLMs do replicate human behavior, it is uncertain whether they can replicate all human behaviors, or only those represented in training data, such as experiments and online content. Further research is needed to understand to what extent LLMs can simulate behaviors of diverse groups, including children, marginalized communities, and others who are underrepresented in the training data. Past research has demonstrated that people from different types of communities act differently in the same strategic scenarios [14, 18, 23] - it is important to ensure that simulations are representative of the population for which policymakers using them are designing for. Additionally, people often behave differently toward their in-group versus an out-group, and it is unclear whether LLMs can capture these nuances.

The foundational LLMs used in these simulations continue to evolve. Simulations that replicate human behavior today may not do so in the future. As LLMs are trained on new datasets, their behavior could change for better or worse; high-quality human behavior data (e.g., from video sources) could improve the model, while low-quality or synthetic data could lead to degradation (e.g., mode collapse). Reinforcement Learning from Human Feedback (RLHF) can also cause certain LLMs, such as GPT-4, to display hyperrational behaviors that exceed typical human responses, especially in emotionally charged situations where humans would behave more irrationally. Experimental results show that LLMs may react to induced emotions (e.g., happiness or anger) in disproportionately rational ways [22]. Future work may need to track how well simulation results replicate on different models over time.

Lastly, there is a gap between how role-playing tasks are simulated and actual cognitive processes. Naive priming in role-playing situations often leads to caricature-like behaviors instead of genuine cognitive modeling. This is because the underlying RLHF mechanism relies on human crowd workers providing feedback on what they think seems like plausible behavior for a particular character in a particular situation. However, stereotypical perceptions of behavior can often not match real human behavior in moral dilemmas. For instance, many people may think a Buddhist monk would react to the trolley problem by avoiding action, when in reality surveys have shown the opposite. Moreover, the humans in the loop tend to belong to similar populations characterized as 'poorly paid gig workers' [5], biasing the kinds of perceptions that are represented. Addressing these limitations may involve leveraging techniques like Bayesian induction and internal dialogue to foster more sophisticated representation of agent behaviors.
