## 1 Introduction

Role-playing agents based on large language models are increasingly used to generate synthetic datasets of human behavior to reduce the high cost of running human subject studies (Park et al., 2023; Hou et al., 2025; Huang et al., 2024; Balog &amp; Zhai, 2025). As the promise of using role-playing agents for performing scientific research and informing policy decision-making gains traction (Sarstedt et al., 2024), it is imperative to rigorously assess their validity to safeguard against flawed inferences and ensure the reliability of ensuing conclusions (Rossi et al., 2024; Agnew et al., 2024).

However, existing evaluation frameworks for LLM-based role-playing agents are inherently post-hoc: they assess an agent's behavior only after the simulation is complete (Argyle et al., 2023; Huang et al., 2024; Wang et al., 2023b; Bhandari et al., 2025; Hou et al., 2025). For instance, researchers compare generated survey responses against human data (Argyle et al., 2023), align self-reported beliefs with open-ended outputs (Huang et al., 2024), evaluate personality traits via dialogue cues (Wang et al., 2023b; Bhandari et al., 2025), or examine emergent population dynamics through post-simulation evaluation (Hou et al., 2025). Be-

cause these methods cannot assess belief-behavior consistency, the consistency between an LLM's elicited beliefs and its subsequent actions during role-play is discovered only after considerable resources have been invested in generating synthetic data, and existing work typically considers either population-level or individual-level evaluation, but not both.

Closely tied to the issue of evaluating a model's belief-behavior consistency is the question of what to do when its elicited beliefs diverge from the researcher's expectations-specifically, whether we can control the simulation by imposing a desired belief. For example, if an LLM continues to portray younger individuals as more generous even after being explicitly instructed to simulate a population in which older adults are more generous, this would raise concerns about the controllability of role-playing agents (Shen et al., 2025; Mannekote et al., 2025).

We present a framework that elicits a model's beliefs through targeted prompts to measure belief-behavior consistency in role-play simulations at two levels of analysis. First, at the population level , we quantify consistency by computing the correlation between persona attributes and simulated statistical behaviors aggregated across all simulated participants. Second, at the individual level , we test an LLM's capacity to predict the future actions of a specific simulated member of the population. In both cases, we test whether querying the model's own expectations can flag misaligned beliefs before they lead to errors in large-scale synthetic data. We also examine three design choices: how much background context we give the model when eliciting beliefs, which outcomes we ask it to predict, and how far into the future we ask it to forecast its actions-and how each choice affects belief-behavior consistency.

We illustrate our general framework with a Trust Game case study (Berg et al., 1995), a standard benchmark for LLM role-playing biases (Wei et al., 2024; Xie et al., 2024). The Trust Game offers quantifies interpersonal trust as the amount of money the first player (the Trustor) chooses to send to the second player (the Trustee). We elicit the model's beliefs about how individuals with specific personas or populations with shared characteristics would act, then have the model role-play as the Trustor, allowing direct comparison of stated beliefs with actual behaviors. Our findings suggest systematic belief-behavior inconsistencies: explicit task context during belief elicitation does not appear to improve consistency, selfconditioning enhances alignment in some models while imposed priors tend to undermine it, and individual-level forecasting accuracy tends to degrade over longer horizons.

Our contributions are the following: (1) We introduce an evaluation framework that uses prompt-based belief elicitation to identify issues with the validity of LLM-based role-playing agents prior to large-scale synthetic data generation. (2) At the population level, we analyze belief-behavior consistency in the Trust Game (Section 4) and assess how belief conditioning-through self-conditioning and the imposition of researcher-defined priors-affects model controllability and consistency (Section 4.3). (3) At the individual level, we evaluate whether LLM agents can reliably forecast their own simulated actions across multi-round Trust Game scenarios (Section 5).
