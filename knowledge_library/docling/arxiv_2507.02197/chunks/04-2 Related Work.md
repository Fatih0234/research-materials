## 2 Related Work

LLM-based role-playing agents have gained prominence as tools for generating synthetic behavioral data for a diverse set of applications (Wang et al., 2024a; Mannekote et al., 2025; Shao et al., 2023; Louie et al., 2024; Wang et al., 2024b): for instance developing interactive characters for open-world games (Yan et al., 2023) to predicting vaccine hesitancy in human populations (Hou et al., 2025). Existing approaches to evaluating these agents involves comparing the agent's outputs to human-generated data.

For instance, Argyle et al. (2023) assess whether survey response patterns generated by an LLM match that of real-world surveys. Huang et al. (2024) create TRUSTSIM, which asks the agent to self-report its beliefs and then checks if its free-form responses align with those beliefs. Other works, such as Wang et al. (2023b) and Bhandari et al. (2025), focus on whether an agent's personality cues in dialogue match the attributes as expected. These evaluation methods act as valuable benchmarks, but they share two key limitations: they

Table 1: Comparison of evaluation frameworks for LLM behavioral consistency in simulations of human behavior. Evaluation Regime : Whether consistency is assessed at the population level, individual agent level, or both. Reference : The ground truth standard against which LLM behavior is compared (e.g., human survey data, theoretical models, agent's own stated beliefs). Objective Eval? : Whether evaluation uses quantitative metrics (Objective) versus LLM-based judgment. Effect Size? : Whether the framework quantifies the magnitude of behavioral relationships, not just their direction. Multi Turn? : Whether simulations involve extended interactions across multiple rounds.

| Paper                  | Eval. Granularity   | Reference             | Objective Eval?   | Effect Size?   | Multi Turn?   |
|------------------------|---------------------|-----------------------|-------------------|----------------|---------------|
| Argyle et al. (2023)   | Population-Level    | Human surveys         | ✓                 | ✗              | ✗             |
| Wang et al. (2023b)    | Individual-Level    | Self-elicited beliefs | ✗                 | ✗              | ✗             |
| Huang et al. (2024)    | Individual-Level    | Social norms          | ✗                 | ✗              | ✗             |
| Bhandari et al. (2025) | Individual-Level    | Assigned persona      | ✗                 | ✗              | ✗             |
| Hou et al. (2025)      | Both                | Expert policy         | ✓                 | ✗              | ✓             |
| Our Work               | Both                | Self-elicited beliefs | ✓                 | ✓              | ✓             |

rely heavily on external reference data, which can be limited or difficult to obtain, and they evaluate agent behavior only after simulation, making them fundamentally post-hoc.

In addition to evaluating role-playing agents individually, frameworks such as VACSIM (Hou et al., 2025) evaluate emergent population-level phenomena, like opinion dynamics in social networks. Such ambitious simulations highlight the potential impact of LLMbased role-play can have in influencing real-life policy decisions, but also raise the stakes: undetected errors can propagate, leading to misleading or harmful conclusions. Here, too, evaluation is typically performed after data is generated, reinforcing the post-hoc nature of current practices.

As summarized in Table 1, nearly all existing evaluation methods for role-playing agents are post-hoc. This is problematic because, as Orgad et al. (2024) show, LLMs may encode accurate knowledge internally but often apply it inconsistently across different contexts. Since validation only takes place after simulations are complete, errors in synthetic data can go undetected when outcomes cannot be independently verified. This reactive approach not only increases costs but also allows flawed data to influence downstream analyses.
