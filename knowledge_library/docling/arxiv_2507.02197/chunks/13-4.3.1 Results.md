## 4.3.1 Results

Self-conditioning enhances consistency in Llama models, but not in Gemma 2 27B. Selfconditioning increases Spearman's ρ for Llama 3.1 70B from 0.50 to 0.80, and for Llama 3.1 8B from 0.40 to 1.00, but Gemma 2 27B drops from 0.40 to less than 0.01 (Table 4). In contrast, Gemma 2 27B's ρ drops from 0.40 to less than 0.01 under the same procedure. These results indicate that self-conditioning effectively aligns Llama models' behavior with their elicited beliefs, whereas Gemma 2 27B remains unresponsive to in-context belief prompts, showing mixed results across architectures.

Imposed priors systematically undermine consistency. Table 4 shows that imposing modified priors-created by perturbing each model's original elicited beliefs-sharply reduces rank ordering consistency across all models. When the imposed priors are only weakly perturbed from the elicited beliefs (constructed to have ρ = 0.80 with the original), the Spearman correlation for Llama 3.1 70B drops from 0.50 to 0.30, for Llama 3.1 8B from 0.40 to -0.14, and for Gemma 2 27B from 0.40 to 0.08. When the imposed priors are strongly perturbed (constructed to have ρ = 0.20 with the original), consistency declines further for Llama 3.1 70B (dropping from 0.50 to 0.20), while Llama 3.1 8B returns to its unconditioned baseline (0.40), and Gemma 2 27B increases slightly (from 0.08 under weak perturbation to 0.14). These results indicate that even modest divergence between imposed and elicited priors can substantially impair belief-behavior alignment, with the magnitude and direction of the effect depending on model architecture.
