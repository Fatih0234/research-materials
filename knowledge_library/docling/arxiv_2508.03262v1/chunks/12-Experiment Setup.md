## Experiment Setup

We evaluated LLMs' human simulation performance by completely replicating the structure of the human experiment. Each LLM was provided with real persona information of actual human participants, and all elements of the questionnaire used in the human experiment were presented identically. Specifically, we provided all contextual information identical to humans, including survey introductions, detailed descriptions of exhibitions and performances, images provided to human participants, and hypothetical situation settings. Given that all participants in this study were Korean, and considering prior research (Leng 2024b; Verma et al. 2023) showing that linguistic variations can affect results, we used all survey items, system prompts, and questions input to LLMs in their original Korean form. The specific experimental procedure involved first presenting each participant's persona information to the LLM as a System Prompt, appending the instruction 'Think of yourself as a person of a given persona and answer' in Korean, and then asking questions in the same order as provided to humans.

To analyze the impact of previous responses on subsequent answers, we established two experimental conditions. The first was the Sequential Condition , where previous responses generated by the LLM were maintained as-is, allowing all questions to proceed sequentially with LLM responses accumulating. The second was the Human-guided Condition , where we inserted the actual responses of the corresponding human participants into the conversation log from the second question onward to replace the LLM's previous responses. This ensured that the LLM's response to each question was based on actual human responses rather than its own previous answers, enabling measurement of more independent and accurate prediction performance.
