## Models

In this experiment, we selected three state-of-the-art multimodal LLMs capable of simultaneously processing text and images as evaluation targets: GPT-4o (Hurst et al. 2024), Llama-3.2-90B-Vision-Instruct (Dubey et al. 2024), and Qwen2.5-VL-72B-Instruct (Bai et al. 2025). All models are based on multimodal architectures that can integratively process images and text, and also demonstrate good performance in processing Korean, the experimental language of this study. Particularly, on a popular Korean benchmark, Hae-Rae benchmark (Son et al. 2023), these models achieved good scores: GPT-4o (0.836), Llama-3.2 (0.738), and Qwen2.5 (0.636), respectively. GPT-4o was accessed through the official OpenAI API, while the other two models were accessed through the OpenRouter API. To ensure response consistency, the temperature was fixed at 0 for all.
