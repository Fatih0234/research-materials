## Conclusion

This study presents the first comprehensive evaluation of LLMs' ability to simulate individual human economic decision-making using real 522 human persona data in PayWhat-You-Want (PWYW) pricing experiments. Our find- ings reveal that while LLMs demonstrate reasonable grouplevel tendencies, they struggle significantly with precise individual-level accuracy, achieving overall accuracy rates below 5% across all tested models and conditions. Notably, commonly adopted advanced prompting method such as CoT, RAG, and Few-shot prompting showed no substantial improvements over na¨ ıve baseline methods, and structured survey format consistently outperformed narrative storytelling approaches. The consistently low Jaccard indices across all experimental conditions suggest fundamental limitations in current LLM architectures' ability to identify and weigh decision-making factors that humans actually consider important in their decision making.

These results provide crucial empirical evidence for the current boundaries of LLM-based human behavior simulation and highlight the need for fundamental advances in model architecture and training paradigms rather than incremental improvements in prompt engineering. However, this study has two limitations. First, we did not conduct detailed analysis of which specific persona components among the 65 collected items most significantly influence prediction performance. Second, we did not explore advanced technical modeling approaches such as ensemble methods or fine-tuning that could potentially improve accuracy. Despite these two limitations, this work establishes a rigorous methodological framework for evaluating LLM's ability in persona-based simulation and emphasizes the importance of realistic expectations for current LLM technology in individual human behavior modeling.
