## 3.1 DECOUPLING AGENT FROM ENVIRONMENT

The design of the agent-environment interface is central to Shachi. To ensure agents are portable and experimental results are reproducible, we introduce a principled abstraction layer that decouples an agent's internal cognitive architecture from the external environment it inhabits. This design governs both agent-environment interaction and inter-agent communication.

Our interface takes inspiration from standard reinforcement learning formalisms like OpenAI Gym (Brockman et al., 2016). The simulation proceeds in discrete time steps, guided by the environment's STEP() and RESET() methods (see Appendix D.1). More formally, we model the simulation as a partially observable multi-agent decision process. At each time step t , each agent i possesses an internal state S i t (i.e., its memory). The environment, E , with a global state S E t , emits a tailored observation O i t = f ( S E t , i ) to each agent. This observation contains all information required for decision-making, such as available tools and the expected response format. The agent's cognitive architecture then computes an action A i t according to its policy π , which is conditioned by its intrinsic configuration C i and internal state: A i t ∼ π ( · | O i t , S i t ; C i ) . The environment collects the set of all actions A t = { A 1 t , · · · , A N t } and updates its state via a transition function, S E t +1 = T ( S E t , A t ) . This formal separation of the agent's policy π from the environment's transition function T is what enables agents built in Shachi to be evaluated across diverse environments in a zero-shot manner.

Figure 2: Shachi Methodology Overview. The figure illustrates the core principles of our methodology. Left: Agent instantiation decouples task-specific environment settings (e.g., agent profiles) from task-agnostic agent design. This ensures agent modularity and portability. Middle: The agent's policy π is realized through a cognitive architecture of four components (Configs, Memory, Tools, and LLM). The policy π processes an observation O i t to generate an action A i t . The environment mediates both agent-environment interactions and inter-agent communications via structured messages and facilitates simulation. Agents receive immediate feedback via tool interfaces. Right: The methodology includes a structured three-level benchmark, enabling systematic analysis of agent behavior across contexts of increasing social complexity.

<!-- image -->

The distinction between actions and tool calls is a key methodological contribution. Unlike traditional RL environments with fixed action spaces, Shachi supports expressive, high-dimensional outputs. With this context, we define (1) an action to be an output from the policy π that is passed to the environment's transition function T , thereby advancing the simulation's state to t +1 ; (2) a tool call to be an intra-step cognitive or information-gathering operation, which provides immediate feedback that informs the policy's deliberation before an action is finalized, without advancing the global clock.

Inter-agent communication is also mediated through this robust interface. Instead of allowing direct function calls between agents, which would create complex dependencies, interactions are handled by the environment. For example, an environment may expose a function that allows one agent to send a message to another (this is the case in the OASIS task in our benchmark suite, where an agent talks to others via an environment-specific COMMENT\_TO() function). These interactions are then embedded into the observation space and used to simulate realistic social dynamics such as broadcasting, targeted messaging, or asynchronous communication (see Appendix D.3). To handle the practical challenge of ensuring LLMs produce valid outputs, we leverage modern API features for structured data and employ robust parsing strategies (see Appendix D.4).
