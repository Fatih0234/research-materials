## B.5 EMERGENTANALOGIES (LEVEL I)

Description EmergentAnalogies (Webb et al., 2023) evaluates zero-shot analogical reasoning in LLMs, highlighting analogy's key role in fluid intelligence.

Method The benchmark tests a range of domains for abstract pattern induction and relational reasoning, featuring four core tasks-matrix reasoning, letter-string analogies, four-term verbal analogies, and story analogies. We specifically target free-response accuracy on the matrix reasoning.

Experimental Settings For our reproduction study in Table 1, we use GPT-4 ( gpt-4-0613 ) as the LLM component without a memory component. The task includes neither config nor tool components. We set the LLM temperature to 0 and randomly sampled problems from each category. We use three different seeds for sampling and averaged the results. We compute MAE between the category-wise averages obtained with Shachi (Ours) and those reproduced with their source code. As a naive baseline, we include a model that simply generates a random matrix for each prompt. For our cross-task agent study in Table 2, we utilize GPT-4o ( gpt-4o-2024-08-06 ). We compute an overall average across all categories. It takes approximately one minute to assess matrix reasoning.
