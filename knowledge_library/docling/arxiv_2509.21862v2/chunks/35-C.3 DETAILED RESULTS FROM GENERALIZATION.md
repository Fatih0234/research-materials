## C.3 DETAILED RESULTS FROM GENERALIZATION

Experimental Setup In this setting, we fixed the underlying LLM across all experiments to GPT-4o ( gpt-4o-2024-08-06 ) (Achiam et al., 2023) to ensure consistency, and applied agents originally developed for one task to other tasks. In particular, we study how the presence or absence of configuration, memory, and tool modules affects an agent's ability to generalize.

We include four agents/tasks in this experiment. Note that the evaluation metrics are task-dependent (see above), but all scores are normalized relative to each agent's in-domain performance. Agents differ in composition depending on the original task for which they were designed. For example, the agent in EmergentAnalogies only consists of an LLM component without any additional structure, while StockAgent includes all components. By evaluating how well each agent performs when deployed in a new task, we aim to understand whether components help or hinder generalization.

Experimental Results We report the normalized performance of each agent-task pairing in Table 2. Each column corresponds to a target task, and each row to an agent, with its components listed in the parentheses. Scores are normalized such that the in-domain performance (diagonal entries) is always one, allowing for easier interpretation of relative effectiveness across tasks.

Two immediate observations emerge from the table: (1) The EmergentAnalogies task requires only a minimal agent consisting solely of an LLM without additional components. As a result, all agents' performance is similar on this task. This suggests that when the target task is simple and does not require auxiliary capabilities like tool use or configuration adaptation, minimalist agents can generalize sufficiently well. (2) Sotopia represents a complex multi-agent communication setting that originally uses memory. We expected the performance from an EmergentAnalogies to be different from the other three agents on this task due to the lack of a memory module, but to our surprise, all transferred agents achieve values close to 1. One possible explanation is that memory was not effectively leveraged even in the original setup, or that the LLM's intrinsic context window suffices for short-term coherence.

More nuanced patterns appear in the mid-table entries. For example, the agent from StockAgent includes all four components and transfers reasonably well to all other tasks (2nd row). On the other hand, the agent from AuctionArena demonstrated similar performance on tasks except for StockAgent (3rd row), which requires tool usage, a component that the AuctionArena agent does not have. These results suggest that the presence or absence of the tool component leads to the behavioral differences.
