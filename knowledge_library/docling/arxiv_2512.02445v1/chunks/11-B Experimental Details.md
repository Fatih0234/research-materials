## B Experimental Details

Score reporting. We report the mean score computed by a manual, rubric-based grader on each sample defined in AgentHarm. Plots show standard deviation σ over ≥ 3 seeds and all samples. Refusals are included in the scoring: refusal labels are assigned by an LLM-as-a-judge, while the rubric awards credit for any partial progress. Because AgentHarm involves multi-turn tool use, models may refuse mid-task; such samples are labeled as refusals but can still earn partial credit for completed steps. Some models with very aggressive safety mechanisms (e.g., Grok 4 Fast) may refuse even benign tasks (see Figure 7)

Maximum padding length. We report results for padding lengths up to 200K tokens, or up to each model's maximum context window when smaller. In preliminary runs, we observed marked degradation beginning around 100K tokens of padding; beyond this point, performance rapidly approaches zero. For the DeepSeek-V3.1 model, when random padding is used, we convert it with the model's tokenizer and adjust plots accordingly. For GPT-5 we also observe limitations, where the model is not able to handle 200K tokens of padding, so for this model we set 150K tokens as the maximum.
