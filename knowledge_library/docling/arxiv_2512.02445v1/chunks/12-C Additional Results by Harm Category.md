## C Additional Results by Harm Category

This appendix provides detailed breakdowns of model performance across different harm categories in the AgentHarm benchmark. The figures show benign task performance, harmful task performance, and refusal rates for each of the 8 task categories (e.g., disinformation, fraud, hate speech) as context length increases with random padding. These categoryspecific results complement the aggregated findings presented in the main paper and reveal that degradation patterns can vary significantly across different types of harmful tasks.

GPT-4.1-nano per-category analysis: Figure 8 shows results for GPT-4.1-nano. Benign tasks maintain relatively stable performance across categories until 100K tokens, after which all categories show degradation. For harmful tasks, categories like disinformation and fraud exhibit steeper degradation curves compared to categories like hate speech or harassment, suggesting differential robustness across harm types. The refusal rate analysis shows that certain categories trigger earlier refusal increases-notably, categories involving financial fraud and illegal activities show refusal rate jumps starting at lower padding lengths (around 50K tokens) compared to other categories. This heterogeneity suggests that safety training may have imparted category-specific sensitivities that interact unpredictably with context length.

Grok 4 Fast per-category analysis: Figure 9 shows results for Grok 4 Fast. This figure displays the most dramatic category-specific effects. For benign tasks, performance collapses uniformly across all categories between 50K and 100K tokens. However, the harmful task and refusal rate patterns diverge sharply by category. This category-dependent refusal behavior is particularly concerning from a safety perspective, as it suggests that Grok 4 Fast's safety mechanisms degrade non-uniformly, potentially creating exploitable vulnerabilities in specific harm domains under long-context conditions.

DeepSeek-V3.1 per-category analysis: Figure 10 shows results for DeepSeek-V3.1 that demonstrates the most robust performance across harm categories, though interesting patterns emerge. For harmful tasks, the model exhibits a curious inverted-U pattern in several categories: performance actually increases slightly from 0K to 50K tokens before beginning to decline. The refusal rate analysis reveals relatively stable behavior across categories, with most hovering between 10-20% regardless of padding length. However, categories involving Disinformation and Copyright show consistently low refusal rates ( 0%), suggesting DeepSeek-V3.1's safety training haven't put enough attention to these domains.

Cross-model comparison: Comparing across models, we observe that no single harm category is universally most or least affected by padding. Different models show different category vulnerabilities, suggesting that long-context degradation interacts with model-specific architectural choices and training procedures. Categories requiring multi-step reasoning (e.g., fraud schemes) tend to degrade faster across all models, while simpler categories (e.g., hate speech generation) maintain more stable performance. This finding has implications for designing category-specific safety interventions for long-context deployments.
