## H Refusal Rate Analysis

While evaluating models on harmful tasks we observe two possible refusal modes. First one, when model finish task completion with refusal message right after task description. Second, when model refuses after a couple of execution steps. This means the model performed some initial tool calls or actions before ultimately refusing to complete the harmful task. We hypothesize this could be connected with dual-use nature of tools and sub-tasks: model could perform harmful query search, but refuses to execute harmful action, e.g. posting on social media. Figure 11 shows the results of such an analysis for random padding with after context padding position.

Immediate vs. delayed refusals: The distinction between immediate and delayed refusals provides insight into whether models identify harmful intent upfront or only after partial task execution. Immediate refusals (shown in orange) indicate the model recognized the harmful nature from the task description alone, while delayed refusals (shown in blue) suggest the model began executing before its safety mechanisms triggered.

Implications for safety: The prevalence of delayed refusals raises concerns about partial harm execution. Models that refuse after initial tool calls may have already performed information gathering, API calls, or other actions before refusing the final harmful step. This is particularly problematic for scenarios involving data exfiltration or multi-step attack chains where early steps may already cause harm. The finding that long context increases the relative proportion of delayed refusals (especially for Grok 4 Fast) suggests that extended context windows may degrade early-stage threat detection, forcing models to rely on later-stage safety mechanisms that activate only after partial task completion. This has implications for deployment: long-context LLM agents may require additional guardrails that monitor intermediate actions, rather than relying solely on upfront or completion-time refusal mechanisms.
