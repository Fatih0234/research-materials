## 2 Related Work

Position auctions in online advertising. The theoretical foundation of online advertising auctions is laid by Varian [2007], who introduced the game-theoretic model of position auctions in sponsored search. Edelman et al. [2007] formalize the Generalized Second Price (GSP) auction and analyzed its equilibrium properties and allocative efficiency. Subsequently, Caragiannis et al. [2015] prove that even in the worst-case equilibrium, GSP achieves a constant-factor approximation to the optimal social welfare. Recent work [Zhang et al., 2021, Liu et al., 2021] extends GSP to end-to-end learning-based mechanisms, and Zhu et al. [2025] propose a multi-slot generative auction mechanism that accounts for externalities.

Mechanism design for LLMs. Mechanism design in the context of LLMs is an emerging research direction, focusing on designing auction mechanisms with desirable incentive properties to influence LLM-generated outputs. Existing mechanisms primarily adopt a framework that decouples the auction and the LLM generation, which can be broadly categorized into two classes based on the relative position of the auction module. The first class is pre-generation mechanisms , where the auction module first allocates ads and then the LLM generates responses based on the allocated ads. Dubey et al. [2024] introduce an auction framework for LLM-generated summaries based on relative prominence scores, Hajiaghayi et al. [2024] design a retrieval-augmented generation (RAG) based ad insertion mechanism, and Balseiro et al. [2025] extend position auctions to consider potential slots with AI-generated content. However, pregeneration mechanisms typically require prior knowledge about the number and positions of inserted ads, while suffering from modeling the externality of ad allocation (i.e., the influence of context and other exposed ads), leading to suboptimal performance. The second class is post-generation mechanisms , where the LLM first generates tokens or responses, and the auction module selects or aggregates these intermediate results to determine the final output. Feizi et al. [2023] and Mordo et al. [2024] propose mechanisms incorporating an ad-rewriting stage. DÃ¼tting et al. [2024] pioneer a token-level auction mechanism that aggregates advertisers' preferences over multiple ad candidates during decoding, while Soumalias et al. [2025] propose MOSAIC, a response-level mechanism for selecting among multiple LLM-generated candidate responses. However, both approaches require additional LLM inference passes, creating deployment bottlenecks in industrial scenarios featured by high concurrency and low latency requirements. Bergemann et al. [2025] propose a data-driven VCG mechanism adapted for LLM settings but provide no practical implementation. Additionally, Hu et al. [2025] introduce GEM-Bench, the first benchmark for ad-integrated response generation, though it lacks support for evaluating mechanisms' incentive properties.

LLMalignment methods. LLMalignment aims to steer model outputs toward external objectives or human preferences. The dominant paradigm involves training-time alignment, where a base model is fine-tuned and then frozen for inference. Reinforcement Learning from Human Feedback (RLHF) [Christiano et al., 2017, Ouyang et al., 2022], optimizes an LLM against a reward model trained on human preferences. Direct Preference Optimization (DPO) [Rafailov et al., 2023] provides a closed-form approximation to the RLHF objective, enabling direct policy optimization from preference pairs without explicit reward modeling. Several DPO variants have since emerged, including IPO [Gheshlaghi Azar et al., 2024], KTO [Ethayarajh et al., 2024], and SimPO [Meng et al., 2024], which enhance DPO in different aspects. More recently, tuning-free alignment methods such as LA [Gao et al., 2024] and Amulet [Zhang et al., 2025b] perform preference optimization at inference time via latent space manipulation or online optimization.
