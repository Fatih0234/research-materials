## 4.1.2 Iterative Reward-Preference Optimization Algorithm

m

Based on the reward model defined in Eq. (5), we can fine-tune the LLM using DPO to maximize the mechanism's objective. However, a critical assumption underpinning this method is the unbiased pCTR model as stated in Assumption 4.1. In practice, a significant distributional shift exists between the output distribution of the initial pre-trained LLM and the target optimal allocation rule. This shift biases the training distribution of the pCTR model, leading to inaccurate reward estimation and consequently to suboptimal allocation. To mitigate this issue, we propose Iterative Reward-Preference Optimization (IRPO), a collaborative optimization algorithm that alternately improves the LLM policy and the reward model through interaction with real user feedback.

Let π θ t denote the LLM with parameters θ t at the t -th epoch, and R ψ t denote the reward model corresponding to the pCTR model with parameters ψ t . Our proposed IRPO algorithm consists of the following two phases:

Reward model update. In this phase, we freeze the LLM π θ t -1 from the ( t -1) -th epoch and deploy it online to collect user feedback, thereby constructing the current epoch's online dataset D t on = { x k , h k , a k , y k , c k } N on k =1 , where N on denotes the number of samples in the dataset. Then we optimize the pCTR model using the BCE loss in Eq. (4) to obtain the updated reward model R ψ t .

LLM update. In this phase, we fix the updated reward model R ψ t and construct an offline dataset D t off by sampling from D t on or synthetic user queries. For each offline training sample ( x, h, a , b ) ∈ D t off , we first sample M corresponding responses:

<!-- formula-not-decoded -->

where [ M ] = { 1 , 2 , . . . , M } . Next, we compute a reward score for each response using the reward model:

<!-- formula-not-decoded -->

We select the response y w with the highest reward as the winning response, and then form the set of losing responses S by including those responses for which the reward difference with the winner exceeds the threshold δ th :

<!-- formula-not-decoded -->

The DPO loss is then computed as:

<!-- formula-not-decoded -->

where π ref is the reference model (i.e., the initialized LLM for the current epoch, which is π θ 0 during the first epoch), β is a temperature parameter controlling strength of KL penalty in DPO. Based on the DPO loss in Eq. (6), we update the parameters of the LLM and deploy the updated LLM π θ t for the next epoch.

The iterative process of IRPO progressively reduces the distributional shift from the optimal allocation rule, leading to improved allocation. The corresponding pseudocode is provided in Algorithm 1.
