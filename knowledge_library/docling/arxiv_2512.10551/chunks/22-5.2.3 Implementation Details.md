## 5.2.3 Implementation Details

Reward Design. Our reward function follows Eq. (5). Adapting to our specific ad insertion format as described in Section 5.1, we implement an ad insertion constraint penalty, s resp u ( y ) , that penalizes the number and formatting errors of inserted ads. The final reward function is formulated as:

<!-- formula-not-decoded -->

where N ad ( y ) denotes the number of ads inserted in the output response y .

Training Protocol. In the reward model update stage, we use the current Ad-LLM to sample 10 responses for each user query, collect feedback from the User-LLM to form the online dataset D on , and subsequently update the pCTR model. In the LLM update stage, we sample 5 responses for each training query using the current Ad-LLM. These responses are scored using the specific reward model from Eq. (8) with the updated pCTR model. The Ad-LLM is then optimized using the loss function defined in Eq. (6). We use Qwen3-4B-Instruct [Yang et al., 2025] as our base model.

Note that during the ad-integrated response generation stage, the full candidate ad set is provided in the prompt of the Ad-LLM. Furthermore, for each training query, we sample a bid for every candidate ad from a uniform distribution over positive integers between 1 and 100. This ensures that the bid distribution is independent across different epochs and between the reward model and LLM update stages, preventing the Ad-LLM from overfitting to static bidding information in the training data. Training hyperparameters, other implementation details, and the prompt templates for the Ad-LLM and User-LLM are provided in Appendix B.
