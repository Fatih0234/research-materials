## B.2.1 Configuration of LLM-Native Advertising Simulation Environment

We employ the Qwen3-4B-Instruct as the base model for both the Ad-LLM and the User-LLM in our proposed simulation environment.

Ad-LLM. To generate responses with diverse and creative advertisements, the Ad-LLM is configured with a temperature of 0.7, top-p of 0.8, and top-k of 20, following official recommendations [Yang et al., 2025]. The context window is set to 16384, with a maximum output length of 2048 tokens. Inference is accelerated using the vLLM framework [Kwon et al., 2023].

User-LLM. For simulating user feedback, stability is important. Therefore, we utilize a lower temperature of 0.3 for the User-LLM, while keeping top-p = 0 . 8 and top-k = 20 . We set the context window and the max output length to 8192 and 2048, respectively. Inference is accelerated using the vLLM framework.
