## B.2.3 Iterative Reward-Preference Optimization

For each training epoch, we perform full-parameter fine-tuning of the Ad-LLM using Direct Preference Optimization (DPO) in BFloat16 (BF16) precision. The DPO-specific hyperparameters are set as follows: the regularization coefficient β is 0.1, the learning rate is 1 × 10 -6 with a cosine scheduler and a warmup ratio of 0.1, and the batch size is 64. And we set the threshold δ th for constructing preference pairs to 10. For the reward model, the balancing hyperparameter λ is set to 1. After each training epoch of the LLMs, we update the reference model and randomize the bids of candidate ads in the training samples to mitigate overfitting.
