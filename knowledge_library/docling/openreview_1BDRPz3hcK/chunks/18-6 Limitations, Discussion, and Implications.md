## 6 Limitations, Discussion, and Implications

Reasoning models may improve belief-behavior consistency. The systematic inconsistencies we observe may stem from the rapid, single-pass inference typical of traditional LLMs. Recent reasoning models of the likes of DeepSeek-R1 (Guo et al., 2025) and OpenAI o1 and o3 (Jaech et al., 2024) employ extended reasoning processes that could potentially bridge the gap between belief elicitation and behavioral simulation. These models' ability to engage in multi-step reasoning and self-reflection during inference may enable more coherent application of stated beliefs to subsequent actions.

Limits of in-context conditioning for controllability. While self-conditioning improves consistency in some Llama models, imposed priors tend to undermine it across architectures. This suggests a potential limitation: in-context prompting may struggle to override entrenched model priors, which could limit researchers' ability to test alternative theories or correct biases. Future work might explore knowledge editing (Wang et al., 2023a; Orgad et al., 2024) or inference-time steering (Li et al., 2023; Lamb et al., 2024; Minder et al., 2025) for more robust belief control.

Generalization beyond the Trust Game. While our evaluation framework and beliefbehavior consistency measures are general and domain-agnostic, we demonstrated their utility through the Trust Game, which provides a simple, well-structured environment that served as an ideal testbed for this initial study. However, real-world simulations often involve richer social contexts and more nuanced agent-goals. In future work, we aim to apply our framework to multi-agent environments, open-ended dialogues, or temporally extended tasks to evaluate how these inconsistencies manifest in more complex settings.
