## 7 Conclusion

We investigate belief-behavior consistency in LLM-based role-playing agents using the Trust Game, revealing systematic inconsistencies between models' stated beliefs and simulated behaviors at both population and individual levels. Our evaluation framework identifies these issues before costly deployment by eliciting beliefs as a diagnostic tool. Key findings show that providing task context during belief elicitation does not improve consistency, selfconditioning helps some models while imposed priors generally undermine alignment, and forecasting accuracy degrades over longer horizons. These results highlight fundamental limitations in current LLM role-playing approaches and emphasize the need for robust internal consistency evaluation before using these systems as scientific instruments.
