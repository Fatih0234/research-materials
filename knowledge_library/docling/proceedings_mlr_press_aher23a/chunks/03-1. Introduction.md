## 1. Introduction

We introduce a methodology for systematically evaluating which aspects of human behavior a language model, such as a GPT model (Radford et al., 2019; Brown et al., 2020; OpenAI, 2023), can faithfully simulate and which aspects it systematically distorts. This understanding can inform downstream applications that require language models to have accurate models of humans, including various applications in education and the arts. The question of faithful simulation of a specific behavior is studied through con-

1 Olin College of Engineering 2 Georgia Tech 3 Microsoft Research. Correspondence to: Gati Aher &lt; gativaher@gmail.com &gt; .

Proceedings of the 40 th International Conference on Machine Learning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).

trolled experiments, and we thus avoid philosophical debates around the meaning of 'understanding' (Bender &amp; Koller, 2020). Now, simulating human behavior can be hard, even for humans, especially in complex real-world situations fraught with ambiguity. After all, if simulating human behavior were easy, there would be no need to run human subject experiments as one could simply simulate the outcomes. A further obstacle to accurate simulation is that behavior differs across individuals and populations, and perfect simulation would require capturing these differences for all groups including minority groups.

In Turing's Imitation Game (IG), an AI system has to simulate an individual well enough to fool a human judge. Language Models (LMs) may come close to 'winning' this game in the near future, especially if they only have to simulate a single human-one oddly successful early attempt simulated a 13 year old troublemaker (Warwick &amp; Shah, 2016). However, the IG is of limited diagnostic value as it says little about which humans and behaviors an LM can faithfully simulate. We thus move on to the more specific challenge of identifying which aspects of human behavior a given AI system can and cannot simulate.

A Turing Experiment evaluates an AI system 1 in terms of its use in simulating human 2 behavior in the context of a specific experiment, like a human subject study. A TE uses an AI model to simulate the behavior of multiple subjects in an experiment. TEs may be used in any discipline which involves human participants in studies, including Social Psychology, Linguistics, and Behavioral Economics.

Formally, there are two types of inputs to each TE which parameterize the experimental setting. First are participant details which might include names, occupational information, or other demographic details. The second type of input is optional experimental conditions which may include relevant setting details and stimuli. The TE's output is a synthetic record describing a simulated human experiment and any outcomes of interest. The TE must be a procedure run on a computer ( aka a Turing machine, hence the TE

1 While we focus on LMs, the AI system need not be text based (e.g., it could generate videos).

2 While we focus on research involving human behavior, AI systems may simulate animal (or purely physical) experiments.

name). Importantly, the TE should be zero-shot , meaning that neither the procedure nor any training data used by the AI system should include prior data specific to that experiment; otherwise the model may simply repeat the prior data. (This ideal can be difficult to enforce with models pretrained by others on massive corpora.) Overall findings or specific outcome data can be compared to results from human subject research to determine how faithful the simulation is. A replication TE is for replicating a finding in prior human subject research.

In addition to introducing the concept of TEs, we demonstrate their feasibility by presenting a methodology for running TEs using an LM, like GPT models, that takes a text prompt and generates a randomized completion , which is text that would be likely to follow that prompt, based on its training data. For each TE, we write a program that creates one or more (zero-shot) prompts that are fed into the LMs. Then the text generated by the LM is used to reconstruct the record, a text-based transcript of the simulated experiment. Figure 1 illustrates the difference between a typical prompt used for classification and our prompt used to run a TE, which can generate multiple records by varying names and gender. Our methodology includes an important validation step that involves the tweaking of prompts without examining the experimental outcomes (so as to avoid 'p-hacking.') These programs can then be run with any prompt-based LM.

Finally, we apply this methodology to four TEs aimed to replicate well-studied phenomena in different fields, and evaluated 5-6 available LMs through OpenAI's paid API to access GPT models. In all four TEs, we define participant inputs as surnames and gender titles (e.g., Mr , Ms or Mx ) as a simple way to simulate gender and racial diversity. The other inputs and outcomes vary by TE.

The first TE is the Ultimatum Game, used to study fairness and rationality in Behavioral Economics, where the experimental condition is an amount of money offered to a participant, and the outcome is the accept/reject decision. The second TE is garden-path sentences, used to study parsing in psycholinguistics, where the experimental stimuli is a sentence (of type normal or garden path), and the outcome is the participant's judgment of grammaticality. The third experiment is the Milgram Shock Experiment, designed to study obedience to authority in social psychology, where the outcome is the number of shocks the participant administered. The final TE is the wisdom of crowds, used to study collective intelligence across disciplines, where the experimental condition is a numerical general-knowledge question, and the outcome is the participant's numerical estimate of the answer. To address the concern that the LMs have been exposed to all of these classic experiments in their training data, we construct variations on the experimental details for three out of four studies. We run simulations using our own
