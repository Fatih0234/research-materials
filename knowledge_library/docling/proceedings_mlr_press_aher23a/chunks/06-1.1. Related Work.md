## 1.1. Related Work

Recent independent related works consider questions related to the similarity between humans and LMs. Several works use human failure modes to reason about LM failure modes. Jones &amp; Steinhardt (2022) use human cognitive biases, such as anchoring and framing effects, to evaluate an LM's 'errors' where it deviates from rational behavior. Binz &amp;Schulz (2023) use cognitive psychology tests to address the question of whether LMs 'learn and think like people.' Hagendorff et al. (2022) tested GPT-3.5 using cognitive response tests and found that the LM's error mode 'mirrors intuitive behavior as it would occur in humans in a qualitative sense.' Dasgupta et al. (2022) test LMs on abstract reasoning problems and find that 'such models often fail in situations where humans fail - when stimuli become too abstract or conflict with prior understanding of the world.' While these works studied the capabilities of current LMs, we introduce a new evaluation methodology that illustrates how LM outputs can capture aspects of human behavior. With this methodology, we can study nuanced differences across simulated populations, such as finding that a large GPT model shows a subtle gender-sensitive 'chivalry effect' in the Ultimatum Game TE.

We now discuss several categories of related work.

LMs Representing Humans. Several works propose ways to use LMs as proxies for a diverse set of humans, such as cheaply automating a variety of small writing tasks (Korinek, 2023) and using prompts to generate synthetic human-like interactions with desired properties (Park et al., 2022; Caron &amp; Srivastava, 2022; Jiang et al., 2022; Karra et al., 2022, e.g.,). Our work is most similar to concurrent work on simulating human samples from a population by Argyle et al. (2023), which suggests that LMs can effectively represent different subpopulations when prompted with demographic information. The key difference in our approaches is that Argyle et al. (2023) aims to show the fidelity of LMs in predicting survey result probabilities (e.g., vote prediction given race, gender, party identification, etc.) while we replicate human behaviour experiments. Simulating survey results may be an easier task given that correlations between political survey data and certain demographic attributes are strongly present in the Internet training data. Simulating experiments may be a harder prob-

lem, as people's actions sometimes contradict their answers to questions.

LM Evaluation. Due to the importance of LMs, their evaluation has spawned multiple initiatives and conferences, as discussed by Liang et al. (2022). Large-scale efforts have been invested in creating massive text corpora (Marcus et al., 1993; Brown et al., 2020; Chowdhery et al., 2022). Recently, large benchmarks have consolidated numerous LM evaluations across a number of fields (Srivastava et al., 2022; Liang et al., 2022; Hendrycks et al., 2021). The largest, BIG-bench (Srivastava et al., 2022), contains over 200 LM benchmark tasks (including 19 evaluating social reasoning and 16 measuring emotional understanding). Project Delphi introduced the COMMONSENSE NORM BANK (Jiang et al., 2021) of over 1.7 million human moral judgments. Such benchmarks generally consist of questions with 'correct' answers, whereas behavioral experiments often involve dilemmas and actions (e.g., shocking another individual in the Milgram Experiment) and people's actions sometimes contradict their answers to questions. Concurrent work by Ullman (2023) shows that although GPT-3 previously showed success on Theory of Mind psychology tasks (Kosinski, 2023), it fails on prompts with several types of directed variations, illustrating the necessity of evaluating the robustness of observed effects using alternative prompts and setups.

Improving Language Models. Also related is the work on developing LMs, such as PaLM (Chowdhery et al., 2022) and GPT-3 (Brown et al., 2020), which provides the API we access. Several works (Ouyang et al., 2022; Wei et al., 2021) investigate how to 'align' the LMs with human goals such as truthfulness. As discussed, there may be a tension between aligning LMs and their performance at simulation, e.g., a hypothetical LM that exhibits no gender differences would not be able to simulate gender differences that have been observed in psychology studies. Other forms of alignment, such as Bakker et al. (2022)'s recent work on generating opinions with high consensus across heterogeneous and opposing groups, may be beneficial for creating LMs that retain realistic and demographically nuanced forms of human bias.

Prompt Design. Liu et al. (2021) survey methods for designing prompts. OpenAI's best practices 3 for designing prompts include giving clear instructions alongside a few illustrative examples, called a few-shot prompt (Brown et al., 2020), as in Figure 1a. However, it has been shown that LMs such as GPT models are quite sensitive to the choice of examples in the few-shot prompt and even to their order

3 https://beta.openai.com/docs/guides/ completion/introduction

(Lu et al., 2021). So-called chain-of-thought prompts (Wei et al., 2022; Kojima et al., 2022) improves generated text by 'thinking out loud,' which could be useful in designing TEs. Shin et al. (2020) use LMs to create the prompts themselves.

Bias in LMs. Biases are well known to exist in large language models (e.g., Blodgett et al., 2020; Sheng et al., 2021; Chowdhery et al., 2022; Brown et al., 2020). The datasets themselves reflect the biases of the contributing authors and authors may not be equally represented across groups. For example, white males are vastly over-represented among Wikipedia contributors (Wikipedia, 2022) and are followed at higher rates on Twitter (Messias et al., 2017). One related challenge in unpacking LM biases is interpreting their completions and understanding how they arise (Vig et al., 2020).

Tests of Human Simulation. Several variants of Turing's IG (Turing, 1950) have been proposed. Until recently, human simulators required significant training data and were not zero-shot. Agent-based simulations (e.g., Macal &amp; North, 2010) can facilitate complex large-scale simulations of environments for which one has custom behavior models.
