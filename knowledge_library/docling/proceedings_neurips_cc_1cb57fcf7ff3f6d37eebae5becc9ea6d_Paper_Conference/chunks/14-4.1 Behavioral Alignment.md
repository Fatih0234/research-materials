## 4.1 Behavioral Alignment

Existing alignment definitions predominantly emphasize values that seek to ensure the safety and helpfulness of LLMs (Ji et al., 2023; Shen et al., 2023; Wang et al., 2023c), which cannot fully characterize the landscape of multifaceted alignment between LLMs and humans. Thus, we propose a new concept of behavioral alignment to characterize the LLM-human analogy regarding behavior , which involves both actions and the associated reasoning processes that underlie them. Because actions evolve over time and the reasoning that underlies them involves multiple factors, we define behavioral alignment as the analogy between LLMs and humans concerning factors impacting behavior, namely behavioral factors , and action dynamics, namely behavioral dynamics .

Based on the definition of behavioral alignment, we aim to answer: does agent trust align with human trust? As for behavioral factors , existing human studies have shown that three basic factors impact human trust behavior including reciprocity anticipation (Berg et al., 1995; Cox, 2004), risk perception (Bohnet &amp; Zeckhauser, 2004) and prosocial preference (Al√≥s-Ferrer &amp; Farolfi, 2019). We examine whether agent trust aligns with human trust along these three factors. Although behavioral dynamics vary for different humans and agent personas, we analyze whether agent trust has the same patterns across multiple turns as human trust in the Repeated Trust Game.

Besides analyzing the trust behavior of LLM agents and humans based on quantitative measurements ( e.g. , the amount sent from trustor to trustee), we also explore the use of BDI to interpret the reasoning

process with which LLM agents justify their actions, which can further validate whether LLM agents manifest an underlying reasoning process analogous to human cognition.
