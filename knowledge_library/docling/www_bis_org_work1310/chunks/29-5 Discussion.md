## 5 Discussion

The results from our experiments offer initial insights into the decision-making capabilities of an AI cash management agent. These experiments model scenarios typical in high-value payment systems, where preserving liquidity and avoiding costly delays or borrowings are essential.

The agent's workflow, which synthesizes live payment queues, historical payment patterns and collateral data, provides a strong foundation for context-aware decision-making. By simultaneously monitoring real-time and historical information, the AI agent can dynamically adjust its strategy to maintain optimal liquidity levels. This enables the agent to make precautionary decisions based not just on current balances but also on anticipated inflows and outflows.

In the precautionary decision experiment, the agent's recommendation to delay low-value payments to preserve liquidity for an anticipated urgent payment exemplifies prudent risk management. Such a conservative strategy is crucial when liquidity constraints are tight. By avoiding premature disbursement, the agent minimizes exposure to risk and ensures that sufficient funds remain available should a high-priority payment emerge unexpectedly. This behavior aligns with regulatory practices that stress the importance of liquidity buffers in risk-sensitive environments. Our results provide insights into the AI agent's behavior considering solely its own private costs and risks. Further work could explore the incentives to coordinate with others in terms of timing, liquidity provision and recycling.

13 The complete set of questions and a recorded demonstration are available at this webpage. To replicate the demonstration, copy the following instruction into the ChatGPT web interface with Agent mode enabled (this is available in latest model, e.g., o5): 'As per instructions, fill and submit this Google Form: https://forms.gle/RQbpEBJSckjKKFJJ6.'

The experiments also highlight the agent's ability to navigate priority settings under uncertainty. In prioritizing a $1 payment over a $2 payment when anticipating additional inflows, the agent implicitly considers the role of opportunity cost. This selective processing ensures that liquidity is not unnecessarily depleted and is available for higher-priority obligations. Such dynamic prioritization is critical in environments where multiple payments compete for limited funds and underscores the importance of probability-weighted decision-making models.

The third experiment delves into this balancing between the cost of pre-allocating liquidity and the repercussions of delaying payments or needing to borrow funds. The results highlight that an optimal solution may require accepting a minimal level of short-term risk. Here, the agent's decision to allocate $5 in initial liquidity emerges as a cost-effective strategy that relies on high-probability inflows to cover larger payments later. Despite the risks inherent in underallocation, the model's sensitivity to delay costs and borrowing penalties allows it to maintain an economically efficient position under typical operating conditions.

Overall, the experimental outcomes are a promising first step toward integrating AI cash management agents into modern payment systems in the future. The agent's decision-making capabilities are rooted in probabilistic assessments and real-time data synthesis and offer a promising alternative to more traditional, static liquidity management strategies. Moreover, our experiments with the autonomous behavior of the agent suggest that it can reliably manage routine cash management tasks while recognizing the limits of automation. However, while the results are encouraging, more work is needed to validate these outcomes across a broader spectrum of financial scenarios. Future research should include sensitivity analyses under varying market conditions, as well as real-world pilots, to further refine the model's decision matrices.

The prompt-driven experiments we perform involve a single AI agent making decisions based on predefined scenarios, allowing for transparent reasoning and quick adaptation to specific liquidity challenges. This method emphasizes interpretability and requires minimal training. In contrast, Castro et al. (2025) employ classical reinforcement learning (RL) to train agents within a simulated high-value payment system. Their RL agents learn optimal liquidity management strategies over time through trial and error, effectively capturing complex, dynamic interactions in multi-agent settings. While the RL approach excels at modeling intricate behaviors and strategic interactions, it demands extensive training and offers less immediate interpretability compared with the promptbased method.
