## 3.1 Game Theory for Phenomenological Understanding LLMs

This line of research applies classical game theory concepts to explain observable phenomena in LLMs, including patterns in text generation and the inherent limitations of training within specific frameworks. Such studies are particularly valuable given that LLMs are often treated as 'black boxes' due to their proprietary nature and large-scale complexity.

One approach connects cooperative game theory to LLMs, as these models perform parallel computations on input tokens and are structured around transformer layers. The Shapley Value [Shapley, 1953], a method for attributing contributions to individual players in cooperative games, has been adapted to assess the influence of specific tokens and layers on LLMgenerated outputs. Several studies leverage the Shapley Value to evaluate token significance in prompts [Goldshmidt and Horovicz, 2024; Mohammadi, 2024]. For example, [Mohammadi, 2024] demonstrates that LLMs often assign disproportionately high weights to less informative input components, a behavior strongly correlated with incorrect responses. TokenSHAP [Goldshmidt and Horovicz, 2024] enhances Shapley Value computation using Monte Carlo sampling for efficiency, while TextGenSHAP [Enouen et al. , 2024] extends the approach to longer, structured input-output scenarios. [Liu et al. , 2023] applies the Shapley Value to multi-prompt learning, identifying the most impactful prompts for ensemble generation. Similarly, [Zhang et al. , 2024c] analyzes LLM layer contributions, finding that earlier layers exert a more significant influence on output generation.

Another research direction models LLM alignment with diverse human preferences using social choice theory. This framework helps address challenges aligning LLMs with human values and decision-making processes [Mishra, 2023]. For instance, [Conitzer et al. , 2024] analyzes the role of Reinforcement Learning from Human Feedback (RLHF) in expressing human preferences, identifying fundamental issues arising from preference conflicts, and advocating for social choice principles in LLM alignment. [Ge et al. , 2024] examines RLHF reward modeling as a social choice process, demonstrating that Bradley-Terry-based approaches suffer from intrinsic limitations that violate key axioms. [Qiu, 2024] proposes a representative social choice framework, which extracts a small but representative subset of opinions to manage large-scale preference aggregation effectively.

Additionally, some studies apply game theory to model alignment and decoding strategies. [Zhang et al. , 2024e] examine sociotechnical implications of real-world LLM applications, advocating for incentive compatibility to ensure AI systems align with societal goals while maintaining technical robustness. [Chen et al. , 2025] models the LLM decoding process as a Stackelberg game, where the decoder moves first, and an adversarial entity follows. By analyzing optimal strategies for both players, their study provides a theoretical basis for why heuristic sampling strategies perform well in practice.
