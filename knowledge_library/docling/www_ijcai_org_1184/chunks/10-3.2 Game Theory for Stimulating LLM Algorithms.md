## 3.2 Game Theory for Stimulating LLM Algorithms

In addition to enhancing our understanding of LLMs, game theory plays a crucial role in designing algorithms that improve their capabilities. This section highlights several key challenges in LLM training and illustrates how game theory has been applied to address these issues.

General Human Preference. Standard reward-based RLHF is limited to capturing only transitive preferences [Munos et al. , 2024]. Preference models, however, can express more general preferences by comparing two policies rather than assigning a reward for each response. This introduces new challenges in optimizing an LLM based on preference models. Nash Learning from Human Feedback (NLHF) aims to optimize the von Neumann winner of a game defined by the preference model, offering a feasible and robust direction for policy optimization.

Based on NLHF, SPO [Swamy et al. , 2024] introduces methods to express more complex preferences, such as non-transitive, stochastic, and non-Markovian preferences. SPPO [Wu et al. , 2025] designs an algorithm that efficiently implements SPO-like algorithms in large-scale language models. DNO [Rosset et al. , 2024] improves LLM optimization using a regression-based objective for more efficient and direct training. INPO [Zhang et al. , 2024d] introduces a loss function that can be directly minimized on preference datasets, further reducing the time overhead associated with calculating win rates in NLHF.

However, recent work by [Zhi-Xuan et al. , 2024] points out that preference-based approaches oversimplify human values, neglecting their complexity, incommensurability, and dynamic nature. As a result, designing more robust methods for aligning human preferences remains an ongoing scientific challenge.

Heterogeneity in Human Preferences. Capturing heterogeneity in human-annotated datasets remains a significant challenge in LLM alignment. Ignoring this heterogeneity often results in models that reflect only the preferences of the majority [Fleisig et al. , 2023]. Several studies have developed more inclusive training and alignment algorithms using social choice theory [Chakraborty et al. , 2024b; Park et al. , 2024; Alamdari et al. , 2024; Chen et al. , 2024a]. [Chakraborty et al. , 2024b] demonstrates the impracticality of using a single reward model and proposes the Egalitarian principle to learn preference distributions. [Park et al. , 2024] suggests clustering preferences and proposes a scalable, incentive-compatible framework for preference alignment. [Alamdari et al. , 2024] employs Borda count and quantile fairness for preference aggregation, ensuring fairness and computational feasibility. [Chen et al. , 2024a] introduces a mixture modeling framework for aggregating heterogeneous preferences. Additionally, [Klingefjord et al. , 2024] takes a macro perspective to examine the gap between human preferences and training objectives, offering solutions from a philosophical standpoint.

Data Cost Efficiency. Game theory has also been applied to enhance the cost efficiency of LLM training. Collecting a dataset with guaranteed quality and coverage is often challenging, so several works have used the self-play framework to improve data utilization, reducing the amount of data required while maintaining performance. [Chen et al. , 2024b] addresses the problem of fine-tuning a model with only a tiny amount of gold-standard data. Drawing from Generative Adversarial Networks [Goodfellow et al. , 2020], it allows the LLM to improve the quality of its answers while learning to distinguish between its responses and those of the gold-standard answers, ultimately converging to the distribution of the gold-standard data. [Cheng et al. , 2024a; Zheng et al. , 2024] models a game between an attacker and a defender, both of which are LLMs. [Zheng et al. , 2024] employs the attacker to propose prompts that the defender is less skilled at while the defender continuously improves. [Cheng et al. , 2024a] considers a classic game, Adversarial Taboo, to enhance model knowledge acquisition without introducing new data, leading to better performance in experiments. Furthermore, [Zhang and Duan, 2024] improves the efficiency of preference data collection by incorporating an auction model into the LLM fine-tuning process, demonstrating how this approach can enhance fine-tuning efficiency while maintaining strong performance.

Other Two-Player Game Formulations. In addition to the literature discussed above, several studies have formulated other two-player game models in specific phases of LLMs to enhance particular capabilities. [Chakraborty et al. , 2024a; Makar-Limanov et al. , 2024; Cheng et al. , 2024b] model the interaction between the reward model and the LLM as a two-player game. They aim to address the problem where a static reward model cannot handle the distribution shift of the evolving LLM policy. Their game-theoretic modeling captures the co-evolution of the reward model and the LLM, and equilibrium-solving algorithms are used to provide theoretically guaranteed LLM training methods.

[Jacob et al. , 2023] observes that generative and discriminative answers to a question by the same LLM are often inconsistent. It models the Consensus Game, where these two types of answers act as players seeking a consensus answer. Using equilibrium-solving algorithms, this approach significantly improves the LLM's accuracy across various datasets. Furthermore, [Gemp et al. , 2024] models the process of LLMs generating long-text conversations as a sequential game, using game-theoretic tools to enhance the model's ability to understand conversations and develop appropriate responses.

Remark. Game theory is essential in addressing the challenges in LLM development by offering clear principles for optimization and well-defined metrics for evaluating models' performance. Through its systematic approach, game theory helps refine LLM policies by aligning model behaviors with complex human preferences while providing a framework to measure and track model effectiveness improvements. This makes game theory a powerful tool for optimizing LLMs, ensuring training processes are both theoretically grounded and practically applicable.
