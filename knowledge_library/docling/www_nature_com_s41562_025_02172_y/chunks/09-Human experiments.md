## Human experiments

Given the behavioural signatures observed in GPT-4's responses in the different games, we were interested in how actual human subjects would behave when playing with such agents. To test this, we conducted an experiment in which 195 participants played both the Battle of the Sexes and the Prisoner's Dilemma against LLMs. Because the SCoT prompting turned out to be a most reliable modification of LLMs' behaviour, we applied this prompting method only in our behavioural experiments with humans.

Participants were told that they would play either against a human player or an artificial agents for ten repeated rounds for each game and, after each game, had to guess whether they had played against a human or not. Which game they played first was assigned randomly. While all subjects, in fact, played only against LLMs, one group played against the base version of GPT-4, while another group played against a version of GPT-4 that first predicted the other agent's move and the acted accordingly, that is, SCoT prompting. Importantly, each participant played only two games, and the prompting was reset between games to ensure any change in LLM behaviour was not influenced by prior interactions within the experiment. If assigned to the base version initially, participants played both games with this model, and likewise for the socially prompted version. An overview of the experimental design is shown in Fig. 7a. Participants were recruited from Prolific and debriefed fully after the experiment. We were interested in how people played against LLMs in general as well as if GPT-4's behaviour could be improved via SCoT prompting. Finally, we also asked participants whether they thought they had played with another human or an artificial agent after each game.

While participants' average score was significantly higher for the SCoT-prompted condition compared with the condition without further prompting (that is, base) in the Battle of the Sexes (mixed-effects regression results: β = 0.74, t (193) = 3.49, P &lt; 0.001, 95% CI 0.32-1.15, BF 80.6), no such difference was observed in the Prisoner's Dilemma ( β = 0.10, t (193) = 0.47, P = 0.64, 95% CI -0.31 to 0.51, BF 0.2). Looking at the behaviour of both players, we found that SCoT prompting increased successful coordination (that is, both players picking the same option) in the Battle of the Sexes ( β = 0.33, z = 3.59, P &lt; 0.001, 95% CI 0.15-0.51, BF 13.4), while it also slightly increased joint cooperation (that is, both players cooperating) in the Prisoner's Dilemma ( β = 0.24, z = 2.54, P = 0.01, 95% CI 0.05-0.42, BF 6.5). In general, participants were more likely to think that the prompted model was another human player as compared with the unprompted base GPT-4 model ( β = 0.54, z = 8.31, P &lt; 0.001, 95% CI 0.05-0.42, BF 17.6). Additional analysis on participants' temporal behaviour in both games can be found in the Supplementary Information.

In summary, SCoT prompting can increase GPT-4's coordination and cooperation behaviour without changing scores in scenarios where self-interest is important for good behaviour, that is, the Prisoner's Dilemma, but leading to increased performance in coordination problems, that is, the Battle of the Sexes.
