## Discussion

LLMs are among the most quickly adopted technologies ever, interacting with millions of consumers within weeks 10 . Understanding in a more principled manner how these systems interact with us, and with each other, is thus of urgent concern. Here, our proposal is simple: Just like behavioural game theorists use tightly controlled and theoretically well-understood games to understand human interactions, we use these games to study the interactions of LLMs.

We thereby understand our work as both a proof of concept of the utility of this approach and an examination of the individual failures and successes of socially interacting LLMs. Our large-scale analysis of all 2 × 2 games highlights that the most recent LLMs indeed are able to perform well on a wide range of game-theoretic tasks as measured by their own individual reward, particularly when they do not have to explicitly coordinate with others. This adds to a wide-ranging literature showcasing emergent phenomena in LLMs 4-8 . However, we also show that LLMs' behaviour is suboptimal in coordination games, even when faced with simple strategies.

To tease apart the behavioural signatures of these LLMs, we zoomed in on two of the most canonical games in game theory: the Prisoner's Dilemma and the Battle of the Sexes. In the Prisoner's Dilemma, we show that GPT-4 plays mostly unforgivingly. Starting with full cooperation, it permanently shifts to defection after a single negative interaction with the other agent, even if the other agent later cooperates. While noting that GPT-4's continual defection is indeed the equilibrium policy in this finitely played game, such behaviour comes at the cost of the two agents' joint pay-off. We see a similar tendency in GPT-4's behaviour in the Battle of the Sexes, where it has a strong tendency to stubbornly stick with its own preferred alternative. In contrast to the Prisoner's Dilemma, this behaviour is suboptimal, even on the individual level.

Current generations of LLMs are generally assumed, and trained, to be benevolent assistants to humans 35 . Despite many successes in this

Fig. 7 | Human experiments. a , The design of human experiments ( N = 195, 89 females, mean age 26.72, s.d. 4.19). Each participant gets randomly assigned either the base or the SCoT-prompted version of the LLM at the start and plays both games repeatedly for ten rounds against this agent. b , Results of the Battle of the Sexes game showing participants' average scores by condition (mixedeffects regression results: β = 0.74, t (193) = 3.49, P &lt; 0.001, 95% CI 0.32-1.15, BF 80.6). c , Results of the Prisoner's Dilemma game showing participants' average scores by condition ( β = 0.10, t (193) = 0.47, P = 0.64, 95% CIs -0.31 to 0.51, BF 0.2).

<!-- image -->

direction, the fact that we here show how they play iterated games in such a selfish and uncoordinated manner sheds light on the fact that there is still substantial ground to cover for LLMs to become truly social and well-aligned machines 36 . Their lack of appropriate responses vis-a-vis even simple strategies in coordination games also speaks to the recent debate around ToM in LLMs 37-39 by highlighting a potential failure mode.

Our extensive robustness checks demonstrate how these behavioural signatures are not functions of individual prompts but reflect broader patterns of LLM behaviour. Our intervention pointing out the fallibility of the playing partner-which leads to increased cooperation-adds to a literature that points to the malleability of LLM social behaviour in tasks to prompts 40,41 . This is important as we try to understand what makes LLMs better, and more pleasant, interactive partners. Further experiments on GPT-4's final round behaviour have shown that it did not adjust its behaviour in the last round of games or when faced with varying probabilities of continuation, unlike human players who often increase cooperation when future interactions are likely 42,43 . This suggests that GPT-4 may lack mechanisms for backward induction and long-term strategic planning, primarily focusing on immediate context due to its training on next-token prediction 44 . Consequently, GPT-4 tends to default to defection in uncertain situations, contrasting with human tendencies to anticipate and adjust based on future outcomes 24,45 .

We additionally observed that prompting GPT-4 to make predictions about the other player before making its own decisions can alleviate behavioural flaws and the oversight of even simple strategies. This represents a more explicit way to force an LLM to engage in ToM and shares much overlap with non-SCoT reasoning 34,46 . Just like chain-of-thought prompting is now implemented as a default in some LLMs to improve (non-social) reasoning performance, our work

d , The average proportion of participants guessing that they have played against another human by condition. Error bars represent the 95% CIs of the mean ( β = 0.54, z = 8.31, P &lt; 0.001, 95% CI 0.05-0.42, BF 17.6). e , Participants' successful coordination rates by condition in the Battle of the Sexes game ( β = 0.33, z = 3.59, P &lt; 0.001, 95% CI 0.15-0.51, BF 13.4). f , Participants' mutual cooperation rates by condition in the Prisoner's Dilemma game ( β = 0.24, z = 2.54, P = 0.01, 95% CI 0.05-0.42, BF 6.5).

suggests implementing a similar social cognition prompt to improve human-LLM interaction.

In our exploration of a behavioural game theory of machines, we acknowledge several limitations. First, despite covering many families of games, our investigation is constrained to simple 2 × 2 games. However, we note that our analysis substantially goes beyond current investigations that have often investigated only one game, and done so using single-shot rather than iterated instances of these games. For example, our iterated approach shares more overlap with the more iterated nature of human-LLM conversations. We also note that we mainly study finite games where agents share knowledge about the duration of the interaction. This is in contrast to so-called indefinite games that have either unknown, probabilistic or no endpoints at all. In these games, both optimal prescriptions and empirical behaviour can differ significantly from the finite case, warranting further investigation.

We believe that more complicated games will shed even more light on game-theoretic machine behaviour in the future. For example, games with more continuous choices like the trust game 47 might elucidate how LLMs dynamically develop (mis-)trust. Games with more than two agents, like public goods or tragedy of the commons type games 48 , could probe how 'societies' of LLMs behave, and how LLMs cooperate or exploit each other.

Given the social nature of the tasks studied here, further empirical work is needed to fully understand human-LLM interactions across all paradigms. In our study, we conducted human experiments in two of the games, specifically, the Battle of the Sexes and the Prisoner's Dilemma, and attempted to probe human-like behaviours such as turn-taking in Battle of the Sexes or prompting for forgiveness in the Prisoner's Dilemma. However, these empirical investigations were limited to these two games. By extending human studies to the remaining games, additional dynamics may emerge. Furthermore, asking LLMs

to self-report their strategies in these games and correlating these explanations with their actions could provide valuable insights into their actual decision-making processes.

Our results highlight the broader importance of a behavioural science for machines 49-52 . We believe that these methods will continue to be useful for elucidating the many facets of LLM cognition, particularly as these models become more complex, multimodal and embedded in physical systems.
