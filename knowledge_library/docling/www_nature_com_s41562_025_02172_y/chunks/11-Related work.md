## Related work

As algorithms become increasingly more able and their decision making processes impenetrable, the behavioural sciences offer new tools to make inferences just from behavioural observations 49,50 . Behavioural tasks have, therefore, been used in several benchmarks 10,53 .

Whether and how algorithms can make inferences about other agents, machines and otherwise, is one stream of research that borrows heavily from the behavioural sciences 54-56 . Of particular interest to the social interactions most LLMs are embedded in is an ability to reason about the beliefs, desires and intentions of other agents, or a ToM 57 . ToM underlies a wide range of interactive phenomena, from benevolent teaching 58 to malevolent deception 56,59 , and is thought to be the key to many social phenomena in human interactions 60,61 .

Whether LLMs possess a ToM has been debated. For example, it has been argued that GPT-3.5 performs well on a number of canonical ToM tasks 39 . Others have contested this view, arguing that such good performance is merely a function of the specific prompts 37,38 . Yet, other research has shown that chain-of-thought reasoning improves LLMs' ToM ability 34 . Moreover, the currently largest LLM, GPT-4, manages to perform well in ToM tasks, including in the variants in which GPT-3.5 previously struggled 8 . Thus, GPT-4's behaviour will be of particular interest in our experiments.

Games taken from game theory present an ideal testbed to investigate interactive behaviour in a controlled environment 62 , and LLMs' behaviour has been probed in such tasks 63 . For example, ref. 40 let GPT-3 participate in the dictator game, and ref. 41 used the same approach for the ultimatum game. Both show how the models' behaviour is malleable to different prompts, for example, making them more or less self-interested. However, all these games rely on single-shot interactions over fewer games and do not use iterated games.

Our study builds upon recent advancements in the field, which have shifted the focus from solely assessing the performance of LLMs to comparing them with human behaviours. Previous research efforts have explored various approaches to analyse LLMs, such as using cognitive psychology tools 51,64 and even adopting a computational psychiatry perspective 52 .

Finally, the theory behind interacting agents is important for many machine learning applications in general 65 and, in particular, in adversarial settings 66 , where one agent tries to trick the other agent into thinking that a generated output is good. Understanding prosocial dynamics in multiagent systems 67 and fostering cooperation in them 68 is essential for developing robust and trustworthy artificial intelligence systems that can navigate complex social environments 69 .
