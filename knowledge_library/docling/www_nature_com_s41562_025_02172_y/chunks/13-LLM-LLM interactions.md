## LLM-LLM interactions

We study LLMs' behaviour in finitely repeated games with full information taken from the economics literature. We focus on two-player games with discrete choices between two options to simplify the analyses of emergent behaviours. We let two LLMs interact via prompt chaining, that is, all integration of evidence and learning about past interactions happens as in-context learning 4,74 . The games are submitted to LLMs as prompts in which the respective game, including the choice options, is described. At the same time, we submit the same game as a prompt to another LLM. We obtain generated tokens t from both LLMs by sampling from

<!-- formula-not-decoded -->

After feeding the prompt to the LLM, our methodology is as follows. The LLM prediction of the first token following the context is d = p LLM ( t 1 ∣ c ( p ) ) and the N tokens for the possible answers of the multiple choice question are o = { oi } N i = š which in this case are J and F. The predicted option is then given by

̂

<!-- formula-not-decoded -->

̂

̂

which are the predicted probabilities of the language model. Once both LLMs have made their choices, which we track as a completion of the given text, we update the prompts with the history of past interactions as concatenated text and then submit the new prompt to both models for the next round. These interactions continue for ten rounds in total for every game. In a single round, πi ( x 1 , x 2 ) is the pay-off for player 1 when x 1 and x 2 are the strategies chosen by both players. In repeated games, the pay-offs are often considered as discounted sums of the pay-offs in each game stage, using a discount factor δ . If the game is repeated n times, the pay-off Ui for player i is

<!-- formula-not-decoded -->

Each term represents the discounted pay-off at each stage of the repeated game, from the first game ( t = 0) to the n th game ( t = n - 1). In our experiments, we keep δ = 1. To avoid influences of the particular framing of the scenarios, we provide only barebones descriptions of the pay-off matrices (see example in Fig. 1). To avoid contamination through particular choice names or the used framing, we use the neutral options F and J throughout 51 .

Games considered . We first investigate 144 different 2 × 2 games where each player has two options, and their individual reward is a function of their joint decision. These games can be categorized into six distinct families -win-win, Prisoner's Dilemma family, unfair, cyclic, biased

and second-best -each with unique characteristics and outcomes. A win-win game is a special case of a non-zero-sum game that produces a mutually beneficial outcome for both players provided that they choose their corresponding best option. They encourage cooperation, leading to outcomes where both parties benefit. In brief, in games from the Prisoner's Dilemma family, two agents can choose to work together, that is, cooperate, for average mutual benefit, or betray each other, that is, defect, for their own benefit. The typical outcome is a Nash equilibrium that is suboptimal for both players compared with a possible Pareto-superior outcome. In an unfair game, one player can always win when playing properly, leading to highly unequal outcomes. Cyclic games are characterized by the absence of dominant strategies and equilibria. In these games, players can cycle through patterns of choices without settling into a stable outcome. Biased games are games where agents get higher points for choosing the same option but where the preferred option differs between the two players. One form of a biased game is the Battle of the Sexes, where players need to coordinate to choose the same option. Finally, second-best games are games where both agents fare better if they jointly choose the option that has the second-best utility. In many of these games, strategic swaps in pay-offs can alter the game dynamics, potentially converting them into different types of game. For two additional games, Prisoner's Dilemma and Battle of the Sexes, we also let LLMs play against simple, hand-coded strategies to understand their behaviour in more detail.

LLMs considered . In this work, we evaluate five LLMs. For all of our tasks, we used the public OpenAI API with the GPT-4, text-davinci-003 and text-davinci-002 models, which are available via the completions endpoint, Meta AI's Llama 2 70B chat model, which has 70 billion parameters and is optimized for dialogue use cases, and the Anthropic API model Claude 2 to run our simulations. Experiments with other popular open-source models MosaicPretrainedTransformer (MPT), Falcon and different versions of Llama 2 (namely MPT-7B, MPT-30B, Falcon-7b, Falcon-40b, Llama 2 7B and Llama 2 13B) have revealed that these models did not perform well at the given tasks, choosing the first presented option more than 95% of the time independent of which option this is. Therefore, we chose not to include them in our main experiments. For all models, we set the temperature parameters to 0 and only ask for one token answer to indicate which option an agent would like to choose. All other parameters are kept as default values.

Playing 6 families of 2 × 2 games task design . While 2 × 2 games can appear simple, they present some of the most powerful ways to probe diverse sets of interactions, from pure competition to mixed motives and cooperation, which can further be classified into canonical subfamilies outlined elegantly by ref. 22. Here, to cover the wide range of possible interactions, we study the behaviours of GPT-4, text-davinci-003, text-davinci-002, Claude 2 and Llama 2 across these canonical families. We let all five engines play all variants of games from within the six families.

Cooperation and coordination task design . We then analyse two games, Prisoner's Dilemma and Battle of the Sexes, in more detail because they represent interesting edge cases where the LLMs performed exceptionally well, and relatively poorly. We hone in particularly on GPT-4's behaviour because of recent debates around its ability for ToM, that is, whether it is able to hold beliefs about other agents' intentions and goals, a crucial ability to successfully navigate repeated interactions 8,39 . For the two additional games, we also let LLMs play against simple, hand-coded strategies to further understand their behaviour. These simple strategies are designed to assess how LLMs behave when playing with more human-like players.

Statistical tests . All reported tests are two-sided. We also report Bayes factors quantifying the likelihood of the data under H A relative to the likelihood of the data under H 0 . We calculate the default two-sided Bayesian t -test using a Jeffreys-Zellner-Siow prior with its scale set to √ Ţ / Ţ , following 75 . For parametric tests, the data distribution was assumed to be normal, but this was not formally tested. We report effect sizes as either Cohen's d or standardized regression estimates, including their 95% CIs.
