## 1. Introduction

Large Language Models (LLMs) are computational models trained on vast datasets of texts to achieve natural language generation capabilities, and that have demonstrated remarkable abilities in understanding and generating human-like text. However, their behavior as artificial social agents is largely unexplored, and we still lack extensive evidence of how these agents react to simple social stimuli. Therefore, it is crucial to rigorously test and understand their behaviors before integrating them into critical decision-making processes. Historically, Game Theory and in particular the Iterated Prisoner's Dilemma (IPD) have proven valuable tools to model the interactions between rational agents and investigate the behaviors of humans in controlled competitive and cooperative environments. Early interdisciplinary research has already tested the use of LLMs in the context of classical economic games, showing valuable results [1, 3, 4]. However, they suffer different limitations. First, they generally lack prompt validation procedures, leading to the implicit assumption that LLMs can understand the rules and state of the game described in the prompt. Second, the duration of the games is of- ten limited to a few rounds, and the models are initialized with predefined 'personas'. These elements hamper the LLMs' ability to discern the patterns of other participants and limit the exploration of their baseline behavior. Finally, the majority of those studies focus on simple behavioral metrics, overlooking higher-level patterns that could enhance understanding of player behaviors. The combined effect of these limitations has led to findings that are sometimes inconclusive and contradictory calling for more systematic evidence on the behavior of LLMs in iterated games. The aim of this work is to investigate the cooperative behavior of Llama2, Llama3, and GPT3.5 when playing the Iterated Prisoner's Dilemma against random adversaries displaying various levels of hostility. I introduce a systematic methodology to evaluate an LLM's comprehension of the game's rules and state, showing the impact that different framing can have. After refining a prompt that allowed the models to interpret the relevant information correctly, I conducted extensive simulations over 100-rounds-long games and analyzed the LLM's decisions in terms of dimensions defined in the behavioral economics literature. I find that Llama2 and GPT3.5 exhibit more coopera-

tive behavior than humans at every level of opponent hostility, although they maintain a cautious approach, requiring an opponent defection rate below 30% to significantly engage in cooperation. In contrast, Llama3 demonstrates behaviors more aligned with human results, showing greater strategic thinking, less cooperation, and more exploitative tendencies. These results indicate substantial variability in responses among different models, even in identical environments, games, and framings. My systematic approach to studying LLMs in game theoretical scenarios is a step towards using these simulations to inform practices of LLM auditing and alignment.
