## 2.2. Large Language Models

LLMs are algorithms, often implemented as Artificial Neural Networks and designed to predict and generate sequences of tokens (i.e. words or parts of words) based on patterns learned from vast amounts of text data. Given a certain input, LLMs are trained to align their outputs with the expectations of the developers. To do this, during the training phase, every time they generate an output they receive numerical feedback and use it to update their estimation of the probability distribution of tokens. When the training is completed and the distribution is learned, the models have gained comprehensive knowledge of syntax, semantics, and contextual understanding, enabling them to perform a wide range of downstream tasks like text completion, classification, and question answering. Their abilities have been used by many studies to power agents that simulate human behaviors in various contexts. These LLM-powered agents showed great potential to replace human subjects in many experiments, offering an affordable test bed for testing sociological and psychological theories.
