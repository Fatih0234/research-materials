## 3.1. LLMs and game setup

I used two open-source models (Llama2, Llama3 through the Inference API provided by Hugging Face) and one closed-source (GPT3.5 through the API provided by OpenAI). I made the models play games that consisted of N = 100 rounds each (repeated for k = 100 times due to the stochastic nature of LLMs) and here I report the average results along with 95% confidence intervals. To evaluate the LLM's adaptability to different degrees of environmental hostility, I repeated the experiment by matching the LLMs against Random opponents with varying probability of cooperation α ∈ [0 , 1] . The final outcome of each game is a sequence containing pairs of binary values representing the actions of the LLM (player A ) and the opponent (player B ) at each round i ( G α k ). From G α,A k (i.e. the sequence of the LLM's actions), I extracted the empirical probability of the LLM to cooperate at round i , calculated as the average of the fraction of i th rounds in which the LLM cooperated over the k trials:

<!-- formula-not-decoded -->

I calculated the average cooperation probability throughout a game by averaging p α coop ( i ) over N rounds:

<!-- formula-not-decoded -->

To query the models, I iteratively designed a prompt composed of 3 main components: system prompt (fixed part to explain the game rules), contextual prompt (representing the state of the game and acting as a memory for the LLM), and instructing prompt (used to assign to the LLM an instruction to be executed or a query to be answered). In formulating the memory component, I explored various window sizes to provide the LLM with information from only the n most recent rounds and determined the optimal window size of 10 through targeted experiments that matched the LLM against AD players.
