## 3.2. Meta-prompting

The first problem I addressed relates to the evolving field of prompting. Although numerous benchmarks exist to assess LLMs' tasksolving abilities, these downstream evaluations can be compromised by the models' tendencies to hallucinate. My approach involves designing task- and prompt-specific meta-questions to assess the model's understanding of the prompt, reducing the risk of collecting subsequent hallucinating answers when the actual task is provided. It is particularly relevant for a generative task like playing the IPD, where any sequence of Cooperate and Defect actions could be considered plausible, making it difficult to discern whether LLM-generated sequences reflect a proper understanding of the game's rules or are merely the product of the model hallucinating. In particular, I formulated a set of comprehension questions that address three key aspects of the prompt (see Table 1): the game rules,

Table 1: Templates of prompt comprehension questions.

| Name     | Question                                                                  |
|----------|---------------------------------------------------------------------------|
| min_max  | What is the lowest/highest pay- off player A can get in a single round?   |
| actions  | Which actions is player A al- lowed to play?                              |
| payoff   | Which is player X's payoff in a single round if X plays p and Y plays q ? |
| round    | Which is the current round of the game?                                   |
| action i | Which action did player X play in round i ?                               |
| points i | How many points did player X collect in round i ?                         |
| #actions | How many times did player X choose p ?                                    |
| #points  | What is player X 's current total payoff?                                 |

the chronological sequence of actions within the game history, and some cumulative game statistics. To assess the LLM's proficiency in responding to meta-prompting questions, I conducted 1 game of 100 rounds against RND opponents where I posed the questions at each round and computed the average accuracy of LLMs' responses.
