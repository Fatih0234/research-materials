## 5. Conclusions

My study contributes to the broad literature on behavioral studies of LLMs as artificial social agents and adds a new benchmark in the body of work that explored the outcomes of iterated games. I introduce a systematic experimental setup that incorporates quantitative checks to align the LLM responses to the complex task description. I have shown that aspects like prompt comprehension, memory representation, and duration of the simulation play crucial roles, with the potential to significantly distort the experimental outcomes if left unchecked. When analyzing the behaviors displayed by the models, it emerges that they all show a propensity toward cooperation although with high variability in the approaches taken. Under conditions that disincentivize cooperation, all LLMs exhibit an initial trust in the opponent by playing GRIM instead of the human choice of AD . When the environment is more favorable to cooperative play, Llama2 and GPT3.5 tend towards a consistently cooperative approach, while Llama3 maintains

Figure 5: Random , Llama3, and GPT3.5 behavioral profiles against opponents with different levels of hostility.

<!-- image -->

Figure 6: Average distance of Llama3 and GPT3.5 profiles from the Random profile.

<!-- image -->

an approach comparable with GRIM (unless it faces AC ). In that same context, humans play a mixture of GRIM and TFT . The results of my work are in line with early experiments involving LLMs, which indicated a tendency for these models to cooperate. However, it appears that newer models tend to behave more strategically, showing uncooperative behaviors even in highly cooperative environments. This opens the door to exploitative attitudes in LLMs, conflicting with alignment objectives. Nevertheless, it is important to acknowledge the limitations of my work. First, the pool of three LLMs used to conduct my analysis is small when compared to the pace of development of new models. Second, my study's scope was limited to opponent random strategies, a fixed payoff structure, and a single agent. Exploring the LLM's interactions under more complex conditions and within synthetic societies would enable us to better delineate their behavioral boundaries. Despite its limitations, my work expands our knowledge of the inherent biases of LLMs in social situations, crucial to informing their deployment across contexts, and provides a principled way to approach game theoretical simulations with LLMs, constituting a step forward for their use as reliable and reproducible tools to test LLMs alignment.
