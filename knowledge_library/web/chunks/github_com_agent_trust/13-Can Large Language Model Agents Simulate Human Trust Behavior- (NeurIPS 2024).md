# Can Large Language Model Agents Simulate Human Trust Behavior? (NeurIPS 2024)
[](https://github.com/camel-ai/agent-trust#can-large-language-model-agents-simulate-human-trust-behavior-neurips-2024)
  * **TLDR** : We discover that LLM agents generally exhibit trust behavior in Trust Games and GPT-4 agents manifest high _**behavioral alignment**_ with humans in terms of trust behavior, indicating the potential to simulate human trust behavior with LLM agents.
  * **Authors** : [Chengxing Xie](https://yitianlian.github.io/)*, [Canyu Chen](https://canyuchen.com/)*, [Feiran Jia](https://feiran.io/), [Ziyu Ye](https://ziyu-deep.github.io/), [Shiyang Lai](https://scholar.google.com/citations?user=qALDmfcAAAAJ&hl=en), [Kai Shu](http://www.cs.iit.edu/~kshu/), [Jindong Gu](https://jindonggu.github.io/), [Adel Bibi](https://www.adelbibi.com/), [Ziniu Hu](https://acbull.github.io/), [David Jurgens](http://jurgens.people.si.umich.edu/), [James Evans](https://macss.uchicago.edu/directory/James-Evans), [Philip Torr](https://www.robots.ox.ac.uk/~phst/), [Bernard Ghanem](https://www.bernardghanem.com/), [Guohao Li](https://ghli.org/). (*equal contributions)
  * **Correspondence to** : Chengxing Xie <xiechengxing34@gmail.com>, Canyu Chen <cchen151@hawk.iit.edu>, Guohao Li <guohao.li@eigent.ai>.
  * **Paper** : [Read our paper](https://arxiv.org/abs/2402.04559)
  * **Project Website** : <https://agent-trust.camel-ai.org>
  * **Online Demo** : [Trust Game Demo](https://huggingface.co/spaces/camel-ai/agent-trust-Trust-Game-Demo) & [Repeated Trust Game Demo](https://huggingface.co/spaces/camel-ai/agent-trust-Repeated-trust-game-Demo)


Our research investigates the simulation of human trust behaviors through the use of large language model agents. We leverage the foundational work of the Camel Project, acknowledging its significant contributions to our research. For further information about the Camel Project, please visit [Camel AI](https://github.com/camel-ai/camel).
