# Social science researchers use AI to simulate human subjects
LLMs that emulate human speech are being used to cost-effectively test assumptions and run pilot studies, producing promising early results. But researchers note that human data remains essential.
![Illustration of generic digital faces.](https://news.stanford.edu/__data/assets/image/0019/174124/GettyImages-1867716103.jpg)
Getty Images
By improving our understanding of human behavior, social science research helps businesses design successful marketing programs, ensures governmental policies are responsive to people’s needs, and supports the development of appropriate strategies for fighting disease and maintaining public safety. 
This research spans the fields of economics, psychology, sociology, and political science and uses a variety of approaches, from fieldwork to online polling, randomized controlled trials, focus groups, observation, and more.
But all social science research is complicated by its subject: people. 
“We’re not dealing with petri dishes or plants that sit still and allow us to experiment over long periods of time,” says [Jacy Anthis](https://profiles.stanford.edu/anthis), visiting scholar at the Stanford Institute for Human-Centered AI (HAI) and a PhD candidate at the University of Chicago. “And because we study human subjects, this research can be time-consuming, expensive, and hard to replicate.”
With advances in AI, though, social scientists can now simulate human data. Large language models (LLMs) that emulate human speech can roleplay expert social scientists or diverse human subjects to inexpensively test assumptions, run pilot studies, estimate optimal sample sizes, and leverage the statistical power that a combination of human and LLM subjects provide. 
![](https://news.stanford.edu/__data/assets/image/0016/174130/jacy-anthis_square.jpg)
> These models are remarkably similar to people and give us an opportunity to add them into any part of the social science research pipeline.
> Jacy AnthisStanford HAI Visiting Scholar
Yet there remain some ways in which LLMs aren’t a great stand-in for human subjects, Anthis notes in a [new preprint paper](https://arxiv.org/abs/2504.02234): They often give less varied, biased, or sycophantic answers; and they don’t generalize well to new settings.
Still, Anthis and others are optimistic about using LLMs for social science research since some rough-and-ready methods have already produced promising results. 
If other researchers heed his rallying cry, Anthis says, one more year of work could lead to substantial improvements. “As technology and society rapidly evolve, we need social science tools like simulations that can keep pace.”
