## Evaluating AI as human proxy
While AI has [made major leaps](https://hai.stanford.edu/ai-index/2025-ai-index-report/technical-performance) on popular benchmarks, its ability to [mimic humans](https://hai.stanford.edu/news/ai-agents-simulate-1052-individuals-personalities-with-impressive-accuracy) is a more recent development. To determine how well it predicts human behavior, [Luke Hewitt](https://pacscenter.stanford.edu/person/luke-hewitt/), a senior research fellow at[ Stanford PACS](https://pacscenter.stanford.edu/), and colleagues [Robb Willer](https://www.robbwiller.org/),[ Ashwini Ashokkumar](https://www.ashwinia.com/team), and[ Isaias Ghezae](https://www.pascl.stanford.edu/Team/isaias-ghezae) tested LLMs against previous randomized controlled trials (RCTs): Could the LLMs successfully replicate the results of trials done with human subjects?
Typical RCTs involve a “treatment” – some piece of information or action that scholars expect to impact a person’s attitudes or behavior. So, for example, a researcher might ask participants to read a piece of text, watch a short video, or participate in a game about a topic (climate change or vaccines, for example), then ask them their opinion about that topic and compare their answers to those of a control group that did not undergo the treatment. Did their opinions shift compared to the controls? Are they more likely to change, start, or stop relevant behaviors?
For their [project](https://www.treatmenteffect.app/), Hewitt and his colleagues used the language model GPT-4 to simulate how a representative sample of Americans would respond to 476 different randomized treatments that had been previously studied. They found that in online survey experiments, LLM predictions of simulated responses were as accurate as human experts’ predictions and correlated strongly (0.85) with measured treatment effects.
This accuracy is impressive, Hewitt says. The team was especially encouraged to find the same level of accuracy even when replicating studies that were published after GPT-4 was trained. “Many would have expected to see the LLM succeed at simulating experiments that were part of its training data and fail on new ones it hadn’t seen before,” Hewitt says. “Instead, we found the LLM could make fairly accurate predictions even for entirely novel experiments.”
Unfortunately, he says, newer models are more difficult to vet. That’s not just because their training data includes more-recently conducted studies, but also because LLMs are starting to do their own web searches, giving them access to information they weren’t trained on. To evaluate these models, scholars may need to create an archive of unpublished studies never before on the internet.
![](https://news.stanford.edu/__data/assets/image/0017/174131/luke-hewitt_square.jpg)
> We found the LLM could make fairly accurate predictions even for entirely novel experiments.
> Luke HewittStanford PACS Senior Research Fellow
