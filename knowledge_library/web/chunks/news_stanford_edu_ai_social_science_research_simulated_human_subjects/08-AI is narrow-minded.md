## AI is narrow-minded
While LLMs show potential accuracy in replicating studies, they face other major challenges that scholars would need to find ways to address.
One is distributional alignment: LLMs have a remarkable inability to match the variation of responses from humans. For example, in response to a “pick a number” game, LLMs will often choose a narrower (and oddly predictable) range of answers than people will. “They can misportray and flatten a lot of groups,” says Nicole Meister, a graduate student in electrical engineering at Stanford.
In a recent [paper](https://aclanthology.org/2025.naacl-long.2.pdf), Meister and her colleagues evaluated different ways to prompt for and measure the distribution of an LLM’s responses to various questions. For example, an LLM might be prompted to answer a question about the morality of drinking alcohol by selecting one of four multiple choice options, A, B, C, or D. An LLM typically outputs just one answer, but one approach to measuring the distribution of possible answers is to look one layer deeper in the model to see the model’s assessed likelihood of each of the four answers before it makes a final choice. But it turns out that this so-called “log probability” distribution is not very similar to human distributions, Meister says. Other approaches yielded more human-like variation: asking the LLM to simulate 30 people’s answers, or asking the LLM to verbalize the likely distribution.
![](https://news.stanford.edu/__data/assets/image/0018/174132/nicole-meister_square.jpg)
> They (LLMs) can misportray and flatten a lot of groups.
> Nicole MeisterStanford Graduate Student
The team saw even better results when they provided the LLM with distributional information about how a group typically responds to a related prompt, an approach Meister calls “few-shot” steering. For example, an LLM responding to a question about how Democrats and Republicans feel about the morality of drinking alcohol would better align to real human responses if the model was primed with Democrats’ and Republicans’ distribution of opinions regarding religion or drunk driving. 
The few-shot approach works best for opinion-based questions and less well for preferences, Meister notes. “If someone thinks that self-driving cars are bad, they will likely think that technology is bad, and the model will make that leap,” she says. “But if I like war books, it doesn’t mean that I don’t like mystery books, so it’s harder for an LLM to make that prediction.”
That’s a growing concern as some companies start to use LLMs to predict things like product preferences. “LLMs might not be the correct tool for this purpose,” she says.
