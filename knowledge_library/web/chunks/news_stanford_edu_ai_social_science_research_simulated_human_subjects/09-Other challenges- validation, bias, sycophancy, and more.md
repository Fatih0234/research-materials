## Other challenges: validation, bias, sycophancy, and more
As with most AI technologies, the use of LLMs in the social sciences could be harmful if people use LLM simulations to replace human experiments, or if they use them in ways that are not well validated, Hewitt says. When using a model, people need to have some sense of whether they should trust it: Is their use case close enough to other uses the model has been validated on? “We’re making progress, but in most instances I don’t think we have that level of confidence quite yet,” Hewitt says.
It will also be important, Hewitt says, to better quantify the uncertainty of model predictions. “Without uncertainty quantification,” he says, “people might trust a model’s predictions insufficiently in some cases and too much in others.” 
According to Anthis, other key challenges to using LLMs for social science research include:
  * Bias: Models systematically present particular social groups inaccurately, often relying on racial, ethnic, and gender stereotypes. 
  * Sycophancy: Models designed as “assistants” tend to offer answers that may seem helpful to people, regardless of whether they are accurate. 
  * Alienness: Models’ answers may resemble what a human might say, but on a deeper level are utterly alien. For example, an LLM might say 3.11 is greater than 3.9, or it might solve a simple mathematical problem using a bizarrely complex method.
  * Generalization: LLMs don’t accurately generalize beyond the data at hand, so social scientists may struggle using them to study new populations or large group behavior.


These challenges are tractable, Anthis says. Researchers can already apply certain tricks to alleviate bias and sycophancy; for example, [interview-based simulation](https://hai.stanford.edu/news/ai-agents-simulate-1052-individuals-personalities-with-impressive-accuracy), asking the LLM to roleplay an expert, or fine-tuning a model to optimize for social simulation. Addressing the alienness and generalization issues is more challenging and may require a general theory of how LLMs work, which is currently lacking, he says.
