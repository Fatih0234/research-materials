[Skip to content](https://news.stanford.edu/stories/2025/07/ai-social-science-research-simulated-human-subjects#main-content)
[![Stanford University](https://news.stanford.edu/__data/assets/file/0023/176063/stanford-university-white.svg)](https://www.stanford.edu/)
Menu
Search queryClearSearch
* * *
Submit search
* * *
  * [University News](https://news.stanford.edu/university-news)
  * [Research & Scholarship](https://news.stanford.edu/research-and-scholarship)
  * [On Campus](https://news.stanford.edu/on-campus)
  * [Student Experience](https://news.stanford.edu/student-experience)


* * *
  * [Videos](https://news.stanford.edu/video)
  * [Community Profiles](https://news.stanford.edu/profile)
  * [In the News](https://news.stanford.edu/in-the-news)
  * [University Updates](https://news.stanford.edu/university-updates)
  * [Announcements](https://news.stanford.edu/announcement)
  * [Recent Stories](https://news.stanford.edu/news-archive)


* * *
  * [Contact](https://news.stanford.edu/contact)
  * [Subscribe](https://news.stanford.edu/subscribe)
  * [Media Contacts](https://news.stanford.edu/media-contacts)
  * [Press Center](https://news.stanford.edu/press-center)


  * [Contact](https://news.stanford.edu/contact)
  * [Press Center](https://news.stanford.edu/press-center)


  * [Contact](https://news.stanford.edu/contact)
  * [Press Center](https://news.stanford.edu/press-center)


* * *
  * [Community Engagement](https://community.stanford.edu/)
  * [Admissions](https://admission.stanford.edu/)
  * [Giving](https://giving.stanford.edu/)
  * [Events](https://events.stanford.edu/)


  * [VPSA](https://studentaffairs.stanford.edu/)
  * [VPUE](https://undergrad.stanford.edu/about-vpue)
  * [VPGE](https://vpge.stanford.edu/)
  * [R&DE](https://rde.stanford.edu/)


  * [Cardinal at Work](https://cardinalatwork.stanford.edu/)
  * [University IT](https://uit.stanford.edu/)
  * [Department of Public Safety](https://police.stanford.edu/)
  * [Stanford Transportation](https://transportation.stanford.edu/)

Close menuClose
Toggle SearchSearchClose
Search query
ClearSearch
* * *
Submit search
[![Stanford Report](https://news.stanford.edu/__data/assets/file/0034/128995/sr-logo-color.svg)](https://news.stanford.edu)[![Stanford Report](https://news.stanford.edu/__data/assets/file/0035/128996/srlogo_light.svg)](https://news.stanford.edu)
# Social science researchers use AI to simulate human subjects
## Read next:
[](https://news.stanford.edu/stories/2025/07/ai-social-science-research-simulated-human-subjects)
PreferencesShow me...Faculty/StaffStudent
## Along with Stanford news and stories, show me:
  * Student information
  * Faculty/Staff information


We want to provide announcements, events, leadership messages and resources that are relevant to you. Your selection is stored in a browser cookie which you can remove at any time using “Clear all personalization” below.
Clear all personalization
* * *
Appearance preference: LightDark
Close preferencesClose
  * [University News](https://news.stanford.edu/university-news)
  * [Research & Scholarship](https://news.stanford.edu/research-and-scholarship)
  * [On Campus](https://news.stanford.edu/on-campus)
  * [Student Experience](https://news.stanford.edu/student-experience)


## Stanford Report cookie usage information
We want to provide stories, announcements, events, leadership messages and resources that are relevant to you. Your selection is stored in a browser cookie which you can remove at any time by visiting the "Show me..." menu at the top right of the page. For more, read our [cookie policy](https://www.stanford.edu/cookie-policy).
AcceptDecline
July 29th, 20259 min read[Science & Engineering](https://news.stanford.edu/research-and-scholarship/topic/science-and-engineering)
# Social science researchers use AI to simulate human subjects
LLMs that emulate human speech are being used to cost-effectively test assumptions and run pilot studies, producing promising early results. But researchers note that human data remains essential.
![Illustration of generic digital faces.](https://news.stanford.edu/__data/assets/image/0019/174124/GettyImages-1867716103.jpg)
Getty Images
By improving our understanding of human behavior, social science research helps businesses design successful marketing programs, ensures governmental policies are responsive to people’s needs, and supports the development of appropriate strategies for fighting disease and maintaining public safety. 
This research spans the fields of economics, psychology, sociology, and political science and uses a variety of approaches, from fieldwork to online polling, randomized controlled trials, focus groups, observation, and more.
But all social science research is complicated by its subject: people. 
“We’re not dealing with petri dishes or plants that sit still and allow us to experiment over long periods of time,” says [Jacy Anthis](https://profiles.stanford.edu/anthis), visiting scholar at the Stanford Institute for Human-Centered AI (HAI) and a PhD candidate at the University of Chicago. “And because we study human subjects, this research can be time-consuming, expensive, and hard to replicate.”
With advances in AI, though, social scientists can now simulate human data. Large language models (LLMs) that emulate human speech can roleplay expert social scientists or diverse human subjects to inexpensively test assumptions, run pilot studies, estimate optimal sample sizes, and leverage the statistical power that a combination of human and LLM subjects provide. 
![](https://news.stanford.edu/__data/assets/image/0016/174130/jacy-anthis_square.jpg)
> These models are remarkably similar to people and give us an opportunity to add them into any part of the social science research pipeline.
> Jacy AnthisStanford HAI Visiting Scholar
Yet there remain some ways in which LLMs aren’t a great stand-in for human subjects, Anthis notes in a [new preprint paper](https://arxiv.org/abs/2504.02234): They often give less varied, biased, or sycophantic answers; and they don’t generalize well to new settings.
Still, Anthis and others are optimistic about using LLMs for social science research since some rough-and-ready methods have already produced promising results. 
If other researchers heed his rallying cry, Anthis says, one more year of work could lead to substantial improvements. “As technology and society rapidly evolve, we need social science tools like simulations that can keep pace.”
## Evaluating AI as human proxy
While AI has [made major leaps](https://hai.stanford.edu/ai-index/2025-ai-index-report/technical-performance) on popular benchmarks, its ability to [mimic humans](https://hai.stanford.edu/news/ai-agents-simulate-1052-individuals-personalities-with-impressive-accuracy) is a more recent development. To determine how well it predicts human behavior, [Luke Hewitt](https://pacscenter.stanford.edu/person/luke-hewitt/), a senior research fellow at[ Stanford PACS](https://pacscenter.stanford.edu/), and colleagues [Robb Willer](https://www.robbwiller.org/),[ Ashwini Ashokkumar](https://www.ashwinia.com/team), and[ Isaias Ghezae](https://www.pascl.stanford.edu/Team/isaias-ghezae) tested LLMs against previous randomized controlled trials (RCTs): Could the LLMs successfully replicate the results of trials done with human subjects?
Typical RCTs involve a “treatment” – some piece of information or action that scholars expect to impact a person’s attitudes or behavior. So, for example, a researcher might ask participants to read a piece of text, watch a short video, or participate in a game about a topic (climate change or vaccines, for example), then ask them their opinion about that topic and compare their answers to those of a control group that did not undergo the treatment. Did their opinions shift compared to the controls? Are they more likely to change, start, or stop relevant behaviors?
For their [project](https://www.treatmenteffect.app/), Hewitt and his colleagues used the language model GPT-4 to simulate how a representative sample of Americans would respond to 476 different randomized treatments that had been previously studied. They found that in online survey experiments, LLM predictions of simulated responses were as accurate as human experts’ predictions and correlated strongly (0.85) with measured treatment effects.
This accuracy is impressive, Hewitt says. The team was especially encouraged to find the same level of accuracy even when replicating studies that were published after GPT-4 was trained. “Many would have expected to see the LLM succeed at simulating experiments that were part of its training data and fail on new ones it hadn’t seen before,” Hewitt says. “Instead, we found the LLM could make fairly accurate predictions even for entirely novel experiments.”
Unfortunately, he says, newer models are more difficult to vet. That’s not just because their training data includes more-recently conducted studies, but also because LLMs are starting to do their own web searches, giving them access to information they weren’t trained on. To evaluate these models, scholars may need to create an archive of unpublished studies never before on the internet.
![](https://news.stanford.edu/__data/assets/image/0017/174131/luke-hewitt_square.jpg)
> We found the LLM could make fairly accurate predictions even for entirely novel experiments.
> Luke HewittStanford PACS Senior Research Fellow
## AI is narrow-minded
While LLMs show potential accuracy in replicating studies, they face other major challenges that scholars would need to find ways to address.
One is distributional alignment: LLMs have a remarkable inability to match the variation of responses from humans. For example, in response to a “pick a number” game, LLMs will often choose a narrower (and oddly predictable) range of answers than people will. “They can misportray and flatten a lot of groups,” says Nicole Meister, a graduate student in electrical engineering at Stanford.
In a recent [paper](https://aclanthology.org/2025.naacl-long.2.pdf), Meister and her colleagues evaluated different ways to prompt for and measure the distribution of an LLM’s responses to various questions. For example, an LLM might be prompted to answer a question about the morality of drinking alcohol by selecting one of four multiple choice options, A, B, C, or D. An LLM typically outputs just one answer, but one approach to measuring the distribution of possible answers is to look one layer deeper in the model to see the model’s assessed likelihood of each of the four answers before it makes a final choice. But it turns out that this so-called “log probability” distribution is not very similar to human distributions, Meister says. Other approaches yielded more human-like variation: asking the LLM to simulate 30 people’s answers, or asking the LLM to verbalize the likely distribution.
![](https://news.stanford.edu/__data/assets/image/0018/174132/nicole-meister_square.jpg)
> They (LLMs) can misportray and flatten a lot of groups.
> Nicole MeisterStanford Graduate Student
The team saw even better results when they provided the LLM with distributional information about how a group typically responds to a related prompt, an approach Meister calls “few-shot” steering. For example, an LLM responding to a question about how Democrats and Republicans feel about the morality of drinking alcohol would better align to real human responses if the model was primed with Democrats’ and Republicans’ distribution of opinions regarding religion or drunk driving. 
The few-shot approach works best for opinion-based questions and less well for preferences, Meister notes. “If someone thinks that self-driving cars are bad, they will likely think that technology is bad, and the model will make that leap,” she says. “But if I like war books, it doesn’t mean that I don’t like mystery books, so it’s harder for an LLM to make that prediction.”
That’s a growing concern as some companies start to use LLMs to predict things like product preferences. “LLMs might not be the correct tool for this purpose,” she says.
## Other challenges: validation, bias, sycophancy, and more
As with most AI technologies, the use of LLMs in the social sciences could be harmful if people use LLM simulations to replace human experiments, or if they use them in ways that are not well validated, Hewitt says. When using a model, people need to have some sense of whether they should trust it: Is their use case close enough to other uses the model has been validated on? “We’re making progress, but in most instances I don’t think we have that level of confidence quite yet,” Hewitt says.
It will also be important, Hewitt says, to better quantify the uncertainty of model predictions. “Without uncertainty quantification,” he says, “people might trust a model’s predictions insufficiently in some cases and too much in others.” 
According to Anthis, other key challenges to using LLMs for social science research include:
  * Bias: Models systematically present particular social groups inaccurately, often relying on racial, ethnic, and gender stereotypes. 
  * Sycophancy: Models designed as “assistants” tend to offer answers that may seem helpful to people, regardless of whether they are accurate. 
  * Alienness: Models’ answers may resemble what a human might say, but on a deeper level are utterly alien. For example, an LLM might say 3.11 is greater than 3.9, or it might solve a simple mathematical problem using a bizarrely complex method.
  * Generalization: LLMs don’t accurately generalize beyond the data at hand, so social scientists may struggle using them to study new populations or large group behavior.


These challenges are tractable, Anthis says. Researchers can already apply certain tricks to alleviate bias and sycophancy; for example, [interview-based simulation](https://hai.stanford.edu/news/ai-agents-simulate-1052-individuals-personalities-with-impressive-accuracy), asking the LLM to roleplay an expert, or fine-tuning a model to optimize for social simulation. Addressing the alienness and generalization issues is more challenging and may require a general theory of how LLMs work, which is currently lacking, he says.
## Current best practice? A hybrid approach
Despite the challenges, today’s LLMs can still play a role in social science research. [David Broska](https://sociology.stanford.edu/people/david-sebastian-broska), a sociology graduate student at Stanford, has developed a[ general methodology](https://journals.sagepub.com/doi/abs/10.1177/00491241251326865) for using LLMs responsibly that combines human subjects and LLM predictions in a mixed subjects design.
“We now have two data types,” he says. “One is human responses, which are very informative but expensive, and the other, LLM predictions, is not so informative but cheap.”
![Image of graduate student David Broska sharing his research during an HAI event.](https://news.stanford.edu/__data/assets/image/0019/174133/hai_digitalistpapersevent_9-24-24_patrickbeaudouin_img_63274.jpg)
At a recent HAI event, Stanford sociology graduate student David Broska shared his research outlining a hybrid approach to AI and traditional social science research. | Courtesy Stanford HAI
The idea is to first run a small pilot study with humans and also run the same experiment with an LLM to see how interchangeable the results are. The approach, called prediction-powered inference, combines the two data resources effectively while preventing the LLM from introducing bias. “We want to keep what the human subjects tell us and increase our confidence in the overall treatment effect while also statistically preventing the LLM from diminishing the credibility of our results,” he says.
An initial hybrid pilot study can also provide a power analysis – a concrete estimate of the proportion of human and LLM subjects that will be most likely to generate a statistically meaningful result, Broska says. This sets researchers up for success in a hybrid study that could potentially be less expensive.
> At the end of the day, if you’re studying human behavior, your experiment needs to ground out in human data.
> David BroskaStanford Sociology Graduate Student
More broadly, Hewitt sees cases where LLM simulations are already useful. “If I was designing a study right now to test an intervention for shifting people’s attitudes about climate in relation to a news event or new policy, or to increase public trust in vaccines, I would definitely first simulate that experiment in an LLM and use the results to augment my intuition.”
Trust in the model is less important if the LLM is only helping with selecting experimental conditions or the wording of a survey question, Hewitt says. Human subjects are still paramount.
“At the end of the day, if you’re studying human behavior, your experiment needs to ground out in human data.”
* * *
## For more information
This story was [originally published](https://hai.stanford.edu/news/social-science-moves-in-silico) by Stanford HAI.
###  Writer 
Katharine Miller 
###  Campus unit 
[ Stanford Institute for Human-Centered Artificial Intelligence ](https://news.stanford.edu/featured-unit/stanford-institute-for-human-centered-artificial-intelligence)
###  Related topics 
[ Science & Engineering ](https://news.stanford.edu/research-and-scholarship/topic/science-and-engineering)
[ Biology ](https://news.stanford.edu/research-and-scholarship/topic/science-and-engineering/biology)
[ Artificial Intelligence ](https://news.stanford.edu/artificial-intelligence)
###  Share this story 
Copy link
## Subscribe to Stanford Report
News, insights and events delivered to your inbox each weekday morning.
Sign up
![](https://news.stanford.edu/__data/assets/image/0035/172898/20250409_Eric_Appel-2-2.jpg)
## Research Matters
Groundbreaking innovations that begin in Stanford labs flow freely into private industry to improve human well-being, fuel the economy, and strengthen American competitiveness.
[Learn more about research at Stanford](https://news.stanford.edu/features/research-at-stanford)
## Stories for you
## Popular stories
  1. [BTS to perform at Stanford Stadium](https://news.stanford.edu/stories/2026/01/bts-performance-stanford-stadium)
  2. [Stanford’s farm goes fully electric](https://news.stanford.edu/stories/2026/01/odonohue-family-educational-farm-renewable-energy-microgrid)
  3. [Volleyball standout Ipar Kurt rises to challenges on and off the court](https://news.stanford.edu/stories/2025/11/ipar-kurt-womens-volleyball-olympics)
  4. [Stanford initiative champions nature as a vital asset](https://news.stanford.edu/stories/2025/11/natural-capital-project-people-planet-prosperity)
  5. [A day in the life of offensive lineman Fisher Anderson](https://news.stanford.edu/stories/2025/11/offensive-lineman-fisher-anderson-routine-day-life)


## Read next
* * *
[View allRead next](https://news.stanford.edu/research-and-scholarship/topic/science-and-engineering)
![Image of an artist’s rendering of an atomically thin coating of silver and some silver atoms.](https://news.stanford.edu/__data/assets/image/0031/181777/SilverDoping.jpg)
##  [ New protective layer boosts lithium metal battery performance ](https://news.stanford.edu/stories/2026/01/solid-electrolyte-advance-lithium-metal-batteries-research)
[ Science & Engineering ](https://news.stanford.edu/research-and-scholarship/topic/science-and-engineering)
Research
![Open video Image of a person using a flight simulator. in a modal](https://news.stanford.edu/__data/assets/image/0033/181599/20251029_aviation_still_1.jpg)
##  [ Stanford researchers and Air Force partner to test AI copilots ](https://news.stanford.edu/stories/2026/01/researchers-air-force-test-pilots-ai-copilots-safety)
[ Science & Engineering ](https://news.stanford.edu/research-and-scholarship/topic/science-and-engineering)
Video
![Illustration depicting multiple laptops](https://news.stanford.edu/__data/assets/image/0024/181536/GettyImages-2188320883.jpg)
##  [ Empowering users to discern fact from fiction in the age of AI ](https://news.stanford.edu/stories/2026/01/ai-digital-literacy-interventions-misinformation-scams-research)
[ Science & Engineering ](https://news.stanford.edu/research-and-scholarship/topic/science-and-engineering)
Research
![Image of a time-lapse of the evolution of color patterns in a soft photonic skin sample.](https://news.stanford.edu/__data/assets/image/0019/181090/Timelapse.png)
##  [ New material changes color and texture like an octopus ](https://news.stanford.edu/stories/2026/01/flexible-material-changes-color-texture-camouflage-robotics-research)
[ Science & Engineering ](https://news.stanford.edu/research-and-scholarship/topic/science-and-engineering)
Research
![Image of asteroids floating in space.](https://news.stanford.edu/__data/assets/image/0022/181480/Asteroid-MN45.jpg)
##  [ Rubin Observatory spots fastest-spinning asteroid of its size ](https://news.stanford.edu/stories/2026/01/rubin-observatory-lsst-research-record-breaking-asteroid)
[ Science & Engineering ](https://news.stanford.edu/research-and-scholarship/topic/science-and-engineering)
Research
![Concept illustration of an artificial metabolism turning waste CO2 into useful chemicals.](https://news.stanford.edu/__data/assets/image/0025/181249/Bioreactor_V7_1500X1000.jpg)
##  [ Synthetic biologists transform waste CO2 into useful chemicals ](https://news.stanford.edu/stories/2026/01/artificial-metabolism-waste-co2-chemicals-synthetic-biology-carbon-recycling)
[ Science & Engineering ](https://news.stanford.edu/research-and-scholarship/topic/science-and-engineering)
Research
Slide 1Slide 2Slide 3Slide 4Slide 5Slide 6
Previous Next
Close
[Stanford Report Home](https://news.stanford.edu)Stories to keep you informed and inspired, from one of the world's leading research and teaching institutions.
## [University News](https://news.stanford.edu/university-news)
  * [Constructive Dialogue](https://news.stanford.edu/university-news/topic/constructive-dialogue)
  * [Simplifying Work at Stanford](https://news.stanford.edu/university-news/topic/simplifying-work-at-stanford)
  * [University Statement](https://news.stanford.edu/university-news/topic/university-statement)
  * [Community Message](https://news.stanford.edu/university-news/topic/community-message)
  * [Awards, Honors & Appointments](https://news.stanford.edu/university-news/topic/awards-honors-and-appointments)
  * [Campus & Facilities](https://news.stanford.edu/university-news/topic/campus-and-facilities)
  * [Distinguished Visitors](https://news.stanford.edu/university-news/topic/distinguished-visitors)
  * [Institutional News](https://news.stanford.edu/university-news/topic/institutional-news)
  * [Leadership & Governance](https://news.stanford.edu/university-news/topic/leadership-and-governance)


## [Research & Scholarship](https://news.stanford.edu/research-and-scholarship)
  * [Research Matters](https://news.stanford.edu/research-and-scholarship/topic/research-matters)
  * [Arts & Humanities](https://news.stanford.edu/research-and-scholarship/topic/arts-and-humanities)
  * [Business](https://news.stanford.edu/research-and-scholarship/topic/business)
  * [Earth & Climate](https://news.stanford.edu/research-and-scholarship/topic/earth-and-climate)
  * [Economics](https://news.stanford.edu/research-and-scholarship/topic/economics)
  * [Education](https://news.stanford.edu/research-and-scholarship/topic/education)
  * [Health & Medicine](https://news.stanford.edu/research-and-scholarship/topic/health-and-medicine)
  * [Law](https://news.stanford.edu/research-and-scholarship/topic/law)
  * [Science & Engineering](https://news.stanford.edu/research-and-scholarship/topic/science-and-engineering)
  * [Social Sciences](https://news.stanford.edu/research-and-scholarship/topic/social-sciences)


## [On Campus](https://news.stanford.edu/on-campus)
  * [Community Engagement](https://news.stanford.edu/on-campus/topic/community-engagement)
  * [Arts](https://news.stanford.edu/on-campus/topic/arts)
  * [Athletics](https://news.stanford.edu/on-campus/topic/athletics)
  * [Campus resources](https://news.stanford.edu/on-campus/topic/campus-resources)
  * [Campus Safety](https://news.stanford.edu/on-campus/topic/campus-safety)
  * [Community & Culture](https://news.stanford.edu/on-campus/topic/community-and-culture)
  * [Events](https://news.stanford.edu/on-campus/topic/events)


## [Student Experience](https://news.stanford.edu/student-experience)
  * [Academics](https://news.stanford.edu/student-experience/topic/academics)
  * [Mental Health](https://news.stanford.edu/student-experience/topic/mental-health)
  * [Volunteer & Service](https://news.stanford.edu/student-experience/topic/volunteer-and-service)


* * *
  * [Videos](https://news.stanford.edu/video)
  * [Community Profiles](https://news.stanford.edu/profile)
  * [In the News](https://news.stanford.edu/in-the-news)
  * [University Updates](https://news.stanford.edu/university-updates)
  * [Announcements](https://news.stanford.edu/announcement)
  * [Recent Stories](https://news.stanford.edu/news-archive)


  * [Contact](https://news.stanford.edu/contact)
  * [Subscribe](https://news.stanford.edu/subscribe)
  * [Media Contacts](https://news.stanford.edu/media-contacts)
  * [Press Center](https://news.stanford.edu/press-center)


  * [Contact](https://news.stanford.edu/contact)
  * [Press Center](https://news.stanford.edu/press-center)


  * [Contact](https://news.stanford.edu/contact)
  * [Press Center](https://news.stanford.edu/press-center)


  * [Community Engagement](https://community.stanford.edu/)
  * [Admissions](https://admission.stanford.edu/)
  * [Giving](https://giving.stanford.edu/)
  * [Events](https://events.stanford.edu/)


  * [VPSA](https://studentaffairs.stanford.edu/)
  * [VPUE](https://undergrad.stanford.edu/about-vpue)
  * [VPGE](https://vpge.stanford.edu/)
  * [R&DE](https://rde.stanford.edu/)


  * [Cardinal at Work](https://cardinalatwork.stanford.edu/)
  * [University IT](https://uit.stanford.edu/)
  * [Department of Public Safety](https://police.stanford.edu/)
  * [Stanford Transportation](https://transportation.stanford.edu/)


[![Stanford University](https://news.stanford.edu/__data/assets/file/0029/175835/stanford-university-stacked-white.svg)](https://www.stanford.edu/)
  * [Stanford Home](https://www.stanford.edu/)
  * [Maps & Directions](https://visit.stanford.edu/plan/)
  * [Search Stanford](https://www.stanford.edu/search/)
  * [Emergency Info](https://emergency.stanford.edu/)


  * [Terms of Use](https://www.stanford.edu/site/terms/)
  * [Privacy](https://www.stanford.edu/site/privacy)
  * [Copyright](https://uit.stanford.edu/security/copyright-infringement)
  * [Trademarks](https://adminguide.stanford.edu/chapters/guiding-policies-and-principles/conflict-interest/ownership-and-use-stanford-trademarks)
  * [Non-Discrimination](https://bulletin.stanford.edu/academic-polices/student-conduct-rights/nondiscrimination)
  * [Accessibility](https://www.stanford.edu/site/accessibility/)


© CopyrightStanford University. Stanford, California 94305.
